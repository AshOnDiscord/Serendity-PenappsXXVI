ID,URL,Summary,Similar_1_Title,Similar_1_Score,Similar_1_URL,Similar_1_Summary,Similar_2_Title,Similar_2_Score,Similar_2_URL,Similar_2_Summary,Similar_3_Title,Similar_3_Score,Similar_3_URL,Similar_3_Summary
1,https://arxiv.org/pdf/2509.07604,"K2-Think is a parameter-efficient reasoning system that achieves state-of-the-art performance with a 32B parameter model, surpassing or matching larger models like GPT-OSS 120B and DeepSeek v3.1. Built on the Qwen2.5 base model, K2-Think combines advanced post-training and test-time computation techniques, including Long Chain-of-thought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards, agentic planning, test-time scaling, speculative decoding, and inference-optimized hardware. The system excels in mathematical reasoning, achieving top scores on public benchmarks for open-source models, and maintains strong performance in code and science domains. K2-Think's developers have made the model freely available, demonstrating best-in-class inference speeds of over 2,000 tokens per second on Cerebras Wafer-Scale Engine, and providing a public website and API endpoint for interactive use. The system's performance is competitive with larger models, and its parameter efficiency makes it a significant advancement in open-source language modeling.",rStar2-Agent: Agentic Reasoning Technical Report,0.7748672962188721,https://www.arxiv.org/abs/2508.20722,"rStar2-Agent is a 14B math reasoning model that achieves high performance through agentic reinforcement learning. Key advancements include: (i) an efficient RL infrastructure with a reliable Python code environment; (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy; and (iii) an efficient agent training recipe. The model demonstrates advanced cognitive behaviors like careful Python coding and reflection on code execution. It achieves state-of-the-art results, surpassing DeepSeek-R1 on AIME24 and AIME25, and generalizes well to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at a provided GitHub URL.
",OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling,0.7619909048080444,https://arxiv.org/abs/2506.20512,"OctoThinker introduces a two-stage mid-training strategy, Stable-then-Decay, to improve reinforcement learning (RL) scaling in language models. Key findings include: (1) high-quality mathematical corpora significantly boost base model and RL performance; (2) adding QA-style data, especially long chain-of-thought (CoT) examples, enhances RL outcomes; (3) scaling mid-training consistently improves downstream RL performance. The authors release open-source models and a curated math reasoning-intensive corpus (MegaMath-Web-Pro-Max).
",Reverse-Engineered Reasoning for Open-Ended Generation,0.7591944336891174,https://arxiv.org/pdf/2509.06160,"This paper introduces REverse-Engineered Reasoning (REER), a new paradigm for instilling deep reasoning in open-ended, creative generation tasks. Unlike existing methods like reinforcement learning (RL) and instruction distillation, which struggle in the absence of clear reward signals, REER works ""backwards"" from known good solutions to discover the underlying reasoning process. The authors created DeepWriting-20K, a dataset of 20,000 deep reasoning trajectories, and trained DeepWriter-8B, which outperforms open-source baselines and rivals leading proprietary models like GPT-4o and Claude 3.5. The project page is https://m-a-p.ai/REER_DeepWriter.
"
2,https://arxiv.org/pdf/2508.05004,"R-Zero is a novel framework for training large language models (LLMs) to self-evolve from zero external data, eliminating the need for human-curated tasks and labels. The framework consists of a Challenger and a Solver, both initialized from the same base LLM, which co-evolve through a reinforcement learning process. The Challenger generates synthetic questions targeted at the edge of the Solver's capabilities, while the Solver is fine-tuned on a filtered set of these challenging questions. This process yields a targeted, self-improving curriculum, resulting in substantial improvements in reasoning capabilities across different backbone LLMs. For instance, R-Zero boosts the Qwen3-4B-Base model by +6.49 on math reasoning benchmarks and +7.54 on general-domain reasoning benchmarks. The framework is model-agnostic and can act as a mid-training method, and it has been shown to have synergy with supervised fine-tuning. However, the framework's performance can degrade over multiple iterations, with larger models being more resilient to this collapse. Analysis suggests that the degradation of pseudo-label accuracy is not the sole driver of this instability, and further investigation is needed to understand the underlying causes.",Absolute Zero: Reinforced Self-play Reasoning with Zero Data,0.8170470595359802,https://arxiv.org/pdf/2505.03335,"Absolute Zero introduces a new Reinforcement Learning with Verifiable Rewards (RLVR) paradigm for training reasoning models without any human-curated data. The Absolute Zero Reasoner (AZR) is a system that self-evolves its training curriculum and reasoning ability by using a code executor to validate proposed code reasoning tasks and verify answers. AZR achieves state-of-the-art performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. AZR is compatible with various model scales and classes.
",Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers,0.7981447577476501,https://arxiv.org/html/2408.06195v1,"This paper introduces rStar, a self-play mutual reasoning approach that significantly improves the reasoning capabilities of small language models (SLMs) without fine-tuning or relying on superior models. rStar uses a self-play mutual generation-discrimination process. One SLM constructs reasoning trajectories, and another SLM verifies them. Experiments show rStar boosts GSM8K accuracy significantly for LLaMA2-7B, Mistral-7B, and LLaMA3-8B-Instruct. The code will be available on Github.
",LLMs Could Autonomously Learn Without External Supervision,0.7858173251152039,https://arxiv.org/html/2406.00606v1,"This paper introduces ""Autonomous Learning"" for Large Language Models (LLMs), a self-sufficient learning approach that allows LLMs to learn without human supervision. This method enables LLMs to self-educate by interacting directly with text, similar to how humans read and comprehend literature. Empirical results show that Autonomous Learning outperforms Pre-training, Supervised Fine-Tuning (SFT), and retrieval-augmented methods. This approach aims to improve the efficiency and effectiveness of LLM training and lead to more advanced, self-reliant AI systems.
"
3,https://interactivetextbooks.tudelft.nl/showthephysics/Introduction/About.html,"This online book, ""Show the Physics,"" presents a collection of 99 physics demonstrations, carefully developed, described, and tested by physics teachers and educators in the Netherlands. The demonstrations are categorized into four sections: nature of science, scientific inquiry, conceptual development, and special occasions, and cover various topics in physics, including waves, optics, electricity, magnetism, and thermodynamics. The book aims to provide teachers with a valuable resource to make physics lessons engaging and educational, and includes features such as Python coding cells that can be run in a browser, allowing for live coding and simulations. The book is published under a Creative Commons Attribution-NonCommercial license, allowing for non-commercial use and adaptation of the materials, and encourages contributions from readers through its online platform. Overall, the book seeks to inspire physics teachers to incorporate demonstrations into their lessons, making physics more accessible and enjoyable for students.",1. Show the Physics — ShowingPhysics,0.9993361234664917,https://interactivetextbooks.tudelft.nl/showthephysics/Introduction/About.html,"This book, ""Show the Physics,"" is an open-access resource presenting 99 physics demonstrations selected from the Dutch book series ""Show _de_ Fysica."" It's designed for physics teachers, offering strategies to make demonstrations both engaging and educational, including videos, live code, and Python simulations. The demonstrations are categorized into nature of science, scientific inquiry, conceptual development, and special occasions. The book originated from a collaboration between physics teachers and educators in the Dutch Association for Science Education (NVON), with each demonstration tested for effectiveness and accessibility.
",GitHub - TUDelft-CITG/learn-python: Python for Engineers,0.74759441614151,https://github.com/TUDelft-CITG/learn-python,"This is a GitHub repository for ""Python for Engineers"" created by TUDelft-CITG. It contains files and folders including a book, .gitignore, README.md, environment.yml, and more. The repository has 2 stars and 3 forks.
",GitHub - wavefrontshaping/introduction_to_python_for_physics,0.7348854541778564,https://github.com/wavefrontshaping/introduction_to_python_for_physics,"This is a GitHub repository titled ""introduction_to_python_for_physics"" by wavefrontshaping. It appears to be an introductory resource for scientists with some programming experience, focusing on Python. The repository includes ""Exercises.ipynb"", ""README.md"", and ""Slides.ipynb"" files. The user is advised to install the Anaconda package before using the resource.
"
4,https://arxiv.org/pdf/2005.08100,"The Conformer model, proposed by Anmol Gulati and colleagues, combines the strengths of convolutional neural networks (CNNs) and Transformers to improve speech recognition. By integrating CNNs, which effectively capture local features, with Transformers, which excel at modeling global interactions, the Conformer model achieves state-of-the-art results on the LibriSpeech benchmark, outperforming previous models with a word error rate of 2.1%/4.3% without a language model and 1.9%/3.9% with an external language model. The model consists of Conformer blocks, which comprise a feed-forward module, a self-attention module, a convolution module, and a second feed-forward module, and ablation studies demonstrate the importance of each component, including the convolution module and the Macaron-style feed-forward modules. The Conformer model exhibits better accuracy with fewer parameters than previous work, making it a promising approach for end-to-end speech recognition.",Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss,0.8802714347839355,https://arxiv.org/abs/2002.02562,"This paper introduces a streaming speech recognition model called Transformer Transducer, which uses Transformer encoders and RNN-T loss. The model encodes audio and label sequences independently using self-attention. It's trained with RNN-T loss, suitable for streaming decoding. Results on the LibriSpeech dataset show that limiting left context in Transformer layers enables streaming with minimal accuracy loss. The full attention version achieves state-of-the-art accuracy. The paper also explores bridging the gap between full and limited attention versions by attending to a limited number of future frames.
",State-of-the-Art Speech Recognition Using Multi-Stream Self-Attention With Dilated 1D Convolutions,0.8539789319038391,https://arxiv.org/abs/1910.00716,"This paper introduces a novel neural network model architecture called multi-stream self-attention for speech recognition. The model uses parallel streams of self-attention encoders, each with 1D convolutions and dilated kernels, followed by a self-attention layer. This allows the model to handle highly correlated speech frames more effectively. The model achieves a word error rate of 2.2% on the test-clean dataset of the LibriSpeech corpus, which was the best result reported at the time of publication. The paper was submitted on October 1, 2019, and accepted to ASRU 2019.
",On the Comparison of Popular End-to-End Models for Large Scale Speech Recognition,0.8516812920570374,https://arxiv.org/abs/2005.14327,"This paper compares three end-to-end (E2E) models for automatic speech recognition: recurrent neural network transducer (RNN-T), RNN attention-based encoder-decoder (AED), and Transformer-AED. The study uses 65,000 hours of Microsoft anonymized training data. Key findings include: AED models are stronger than RNN-T in non-streaming mode, RNN-T is competitive in streaming mode with proper encoder initialization, and Transformer-AED achieved the best accuracy in both streaming and non-streaming modes. Both streaming RNN-T and Transformer-AED models outperformed a highly-optimized hybrid model.
"
5,https://inference-docs.cerebras.ai/quickstart,"The QuickStart guide for Cerebras Inference helps users get started with the Cerebras API by walking them through setting up their developer environment, installing the Cerebras Inference library, and making their first API request. To begin, users need a Cerebras account, an Inference API key, and Python 3.7+ or TypeScript 4.5+. The guide recommends setting the API key as an environment variable for security and convenience. The Cerebras Inference library can be installed through the Python Package Index (PyPI) or npm package manager. Once set up, users can make an API request using code snippets provided, which demonstrate how to perform a chat completion. Additional resources, such as API reference documentation and the developer playground, are also available for users who want to dive deeper or interact with models before making an API call.",Quickstart,0.7806888818740845,https://docs.dev.runwayml.com/guides/quickstart,"To start using Runway's generative video models, first sign up for an account in the developer portal and create an organization. Then, create an API key in the API Keys tab, and store it securely. Add credits to your organization in the billing tab. When making API requests, include your API key in the headers, or set the `RUNWAYML_API_SECRET` environment variable. You can also upload base64 encoded images as data URIs in the `promptImage` field.
",Introduction - Daily Bots - The Open Source Cloud for Voice Agents,0.69794100522995,https://docs.dailybots.ai/introduction,"Daily Bots allows developers to quickly integrate voice and video agents into their applications. Key features include: a modular architecture for easy switching between LLMs and voice models, multi-turn context management, voice-to-voice response times as low as 500ms, interruption handling, phrase endpointing, echo cancellation, background noise reduction, and detailed metrics. It's built on the Pipecat server framework and implements the RTVI standard for real-time inference.
",Quickstart Guide,0.688892662525177,https://docs.stack-ai.com/stack-ai/welcome-to-stack-ai/quickstart-guide,"This guide details how to build a simple application using Stack AI to answer questions about a website. Key steps include: creating a project, using input and output nodes, adding a Large Language Model (LLM), loading data from a website using a URL node, storing website text in a vector database, and connecting these components. The LLM is configured with a system message to act as a web assistant and uses the user's input as a question, along with context from the vector database.
"
6,https://docs.exa.ai/reference/quickstart,"The Exa platform provides a range of tools and APIs for developers to get started with search and content retrieval. To begin, users can create and set up an API key, and then make requests to Exa's API endpoints using Python or JavaScript SDKs, or directly with cURL. A `.env` file can be used to store the API key, and the `dotenv` library can be installed for added security. Once set up, developers can use Exa's APIs to search and crawl content, generate chat completions, find similar links, and retrieve full text content. For example, the `exa.search_and_contents` function can be used to search for articles and retrieve their full text content, as demonstrated in the provided code snippet.",Welcome to Exa - Exa,0.8225468993186951,https://docs.exa.ai/reference/getting-started,"Exa offers five core functionalities: search, content retrieval, finding similar links, answering questions, and automated web research.  You can get started with the API playground, quickstart guides, tool calling integrations, and examples.
",OpenAI Responses API - Exa,0.7858679294586182,https://docs.exa.ai/reference/openai-responses-api-with-exa,"This page is about the OpenAI Responses API from Exa, a search engine built for AI. Exa provides links and content from web pages, using neural search for semantic understanding. To get started, you'll need API keys from both OpenAI and Exa. The page also includes links to documentation, examples, SDKs, and integrations, including Anthropic and OpenAI tool calling.
",Exa Researcher - Python - Exa,0.7745680212974548,https://docs.exa.ai/examples/exa-researcher-python,"This page details how to build an Exa Researcher application in Python. It covers using Exa's auto search to find relevant sources and synthesize information into a research report. The tutorial requires an Exa API key and an OpenAI API key. The setup involves importing the Exa and OpenAI SDKs and setting up API keys.
"
7,https://energy-based-transformers.github.io/static/pdfs/paper.pdf,"Researchers have proposed a new class of models called Energy-Based Transformers (EBTs) that can learn to think and reason in a more human-like way. Unlike existing approaches, EBTs can generalize System 2 Thinking, a type of slow and deliberate thinking, to any problem and modality without requiring external supervision or rewards. EBTs learn to verify the compatibility between inputs and candidate predictions, reframing prediction problems as optimization with respect to this verifier. This allows for dynamic allocation of computation during inference, uncertainty estimation, and explicit verification of predictions. In experiments, EBTs outperformed existing models, such as Transformer++ and Diffusion Transformers, in both discrete and continuous modalities, achieving up to 35% higher scaling rates during training and 29% better performance with System 2 Thinking. EBTs also generalized better to out-of-distribution data, demonstrating their potential as a promising new paradigm for scaling both learning and thinking capabilities of models.",Paper page - Energy-Based Transformers are Scalable Learners and Thinkers,0.890087902545929,https://huggingface.co/papers/2507.02092,"Energy-Based Transformers (EBTs) introduce a new approach to model performance and scalability across different data types by learning to verify predictions through unsupervised learning and energy minimization. EBTs assign an energy value to each input and candidate-prediction pair, enabling predictions via gradient descent-based energy minimization. Key advancements include improved scaling rates during training (up to 35% higher than Transformer++) and enhanced performance during inference, outperforming Transformer++ on language tasks and Diffusion Transformers on image denoising. EBTs also show better results on downstream tasks compared to existing models, suggesting improved generalization capabilities.
",Cognitively Inspired Energy-Based World Models,0.8345615863800049,https://arxiv.org/html/2406.08862v1,"This paper introduces Energy-Based World Models (EBWM) as an alternative to traditional autoregressive models for world modeling. EBWMs are designed to mimic human cognition by predicting the compatibility of a context and a future state using an Energy-Based Model (EBM). This approach enables models to influence internal cognitive processes, evaluate prediction plausibility, and dynamically allocate time for predictions, similar to System 2 thinking. The authors also developed an Energy-Based Transformer (EBT) tailored for EBWMs. Results show that EBWM scales better with data and GPU hours than traditional autoregressive transformers in Computer Vision (CV) and shows promising early scaling in Natural Language Processing (NLP).
",Energy-Based Transformers are Scalable Learners and Thinkers,0.8012092113494873,https://arxiv.org/html/2507.02092v1,"This paper introduces Energy-Based Transformers (EBTs), a new class of Energy-Based Models (EBMs) designed to enhance both the learning and thinking capabilities of models. EBTs utilize a System 2 Thinking approach, enabling them to explicitly verify the compatibility between inputs and candidate predictions through energy minimization. This allows for modality and problem-agnostic application. Key advancements include faster scaling during training (up to 35% higher scaling rate) compared to Transformer++ models across various factors like data, batch size, and parameters. During inference, EBTs show improved performance with System 2 Thinking, outperforming Transformer++ on language tasks by 29% and excelling in image denoising compared to Diffusion Transformers while using fewer forward passes. EBTs demonstrate better generalization, especially on out-of-distribution data, and achieve superior results on downstream tasks.
"
8,https://arxiv.org/pdf/1706.03762,"The Transformer, a new simple network architecture based solely on attention mechanisms, has been proposed as a replacement for complex recurrent or convolutional neural networks in sequence transduction models. The Transformer model achieves state-of-the-art results in machine translation tasks, with a BLEU score of 28.4 on the WMT 2014 English-to-German translation task and 41.8 on the WMT 2014 English-to-French translation task, while being more parallelizable and requiring significantly less training time. The model consists of an encoder and decoder stack, with self-attention mechanisms and point-wise, fully connected layers, and uses positional encoding to inject information about the relative position of tokens in the sequence. The Transformer also achieves good results on English constituency parsing, outperforming previously reported models in some cases, and its code is available for future research and application to other tasks.",Redirect Notice,0.6730402112007141,https://arxiv.org/pdf/google.com/url?q=https%3A%2F%2Farxiv.org%2Fpdf%2F1111.1567.pdf,"This page is a redirect notice. It indicates that the previous page is sending the user to the PDF at https://arxiv.org/pdf/1111.1567.pdf. Users can choose to return to the previous page if they do not wish to visit the linked PDF.
",Cryptographic Challenges: Masking Sensitive Data in Cyber Crimes through ASCII Art,0.6614448428153992,https://arxiv.org/abs/2509.00059,"This paper proposes the use of ASCII art as a novel method for masking sensitive information in cybercrime. It examines the advantages and limitations of this technique, focusing on its role in protecting personal data during delivery and beyond. The study also provides recommendations for enhancing data security and promoting privacy awareness.
",Die Verarbeitung medizinischer Forschungsdaten ohne datenschutzrechtliche Einwilligung: Der Korridor zwischen Anonymisierung und der Forschungsausnahme in Österreich,0.6522507071495056,https://arxiv.org/abs/2509.08841,"This paper, ""Die Verarbeitung medizinischer Forschungsdaten ohne datenschutzrechtliche Einwilligung: Der Korridor zwischen Anonymisierung und der Forschungsausnahme in Österreich,"" analyzes the processing of medical research data without consent, focusing on the legal framework in Austria. It examines anonymization under GDPR and the national research exemption, and their interaction. The paper is 28 pages long, written in German, and addresses data protection concerns in modern, data-driven medical research.
"
