[
  {
    "id":0,
    "url":"https:\/\/arxiv.org\/pdf\/2509.07604",
    "content":"K2-Think: A Parameter-Efficient Reasoning System\nZhoujun Cheng*, Richard Fan*, Shibo Hao*, Taylor W. Killian*,O, Haonan Li*, Suqi Sun*\nHector Ren, Alexander Moreno, Daqian Zhang, Tianjun Zhong, Yuxin Xiong, Yuanzhe Hu, Yutao Xie\nXudong Han, Yuqi Wang, Varad Pimpalkhute, Yonghao Zhuang, Aaryamonvikram Singh, Xuezhi Liang\nAnze Xie, Jianshu She, Desai Fan, Chengqian Gao, Liqun Ma, Mikhail Yurochkin, John Maggs\nXuezhe Ma, Guowei He, Zhiting Hu, Zhengzhong Liu*,O, Eric P. XingO\nInstitute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence\n*Core Contributors (listed alphabetically), OCorresponding Authors\nWe introduce K2-Think, a reasoning system that achieves frontier performance with just a 32B\nparameter model \u2014 surpassing or matching much larger models such as GPT-OSS 120B and\nDeepSeek v3.1. Built on the Qwen2.5 base model, our system demonstrates that smaller models can\ncompete at the highest levels through synergistic combination of advanced post-training and test-time\ncomputation techniques. Our approach is built on top of six key technical pillars: Long Chain-of-\nthought Supervised Finetuning, Reinforcement Learning with Verifiable Rewards (RLVR), Agentic\nplanning prior to reasoning, Test-time Scaling, Speculative Decoding, and Inference-optimized\nHardware, using only publicly available open-source datasets. K2-Think prioritizes mathematical\nreasoning, achieving state-of-the-art scores on public benchmarks for open source models, while also\nmaintaining strong performance on other domains such as Code and Science. Our results validate\nthat a more parameter-efficient model like K2-Think 32B can rival state-of-the-art systems through\nan integrative post-train recipe including long chain-of-thought training and strategic inference-time\nenhancements, paving the way for more accessible and affordable open-source reasoning systems.\nWe have made K2-Think freely available at k2think.ai demonstrating best-in-class inference\nspeeds, through the Cerebras Wafer-Scale Engine, delivering upwards of 2,000 tokens per second\nK2-Think (Model)\nhuggingface.co\/LLM360\/K2-Think\nK2-Think (Code)\ngithub.com\/MBZUAI-IFM\/K2-Think-SFT\ngithub.com\/MBZUAI-IFM\/K2-Think-Inference\n0Correspondence to: {Eric.Xing,Hector.Liu,Taylor.Killian}@mbzuai.ac.ae\nFigure 1: K2-Think exhibits remarkable parameter efficiency, providing comparable or superior performance to\nfrontier reasoning models in complex math domains with an order of magnitude smaller model. The composite\nscore here is the micro-average for each model over four complex math benchmarks, weighted by the number of\nquestions in each benchmark (AIME 2024, AIME 2025, HMMT 2025, and Omni-MATH-HARD; see Section 3 for\nthe benchmark details). Note: parameter counts for proprietary models are speculative.\nRecent advances in frontier reasoning models have highlighted the effectiveness of long chain-of-thought\nreasoning, enabled by large-scale supervised fine-tuning and reinforcement learning. Systems like OpenAI-\nO3 (OpenAI, 2025) and Gemini 2.5 (Google DeepMind, 2025) have achieved strong results on competition-\nlevel math benchmarks, complex coding tasks, and advanced scientific reasoning datasets, setting new\nmilestones for reasoning-centered language models. These developments have also stimulated further\nexploration in the open-source community, where researchers have trained competitive reasoning systems\nwith reinforcement learning (Yu et al., 2025; Hu et al., 2025; Wang et al., 2025c) and investigated mechanisms\nby which RL improves reasoning (Zeng et al., 2025; Yue et al., 2025; Shao et al., 2025; Agarwal et al.,\n2025b; Wang et al., 2025b).\nIn this report we introduce K2-Think: a competitive reasoning system built from the open-weight Qwen2.5-\n32B base model (Yang et al., 2024a). We break down our system into stages, including post-training and\ntest-time, where an integrative recipe involving 6 major technical innovations was introduced over all stages\nspanning finetuning, reinforcement learning, planning, and hardware optimization to boost the base model\nreasoning capability, and we evaluated how each stage affects performance. These components combine\nto enable a model of merely 32 billion parameters, with modest test-time compute, to match the\nmathematical reasoning performance of proprietary frontier models. In fact, K2-Think emerges\nas the top open-source model for complex math benchmarks matching or exceeding previously leading\nmodels that are orders of magnitude greater in size. Figure 1 presents a plot of the global micro-average of\nperformance (essentially dividing the total number of correct answers by the total number of questions\nacross all test sets) of each model over four challenging math competition tasks with respect to the total\nnumber of parameters for each model. The prominent positioning of K2-Think in the top-left visually\ndepicts its superior parameter efficiency, demonstrating that it achieves State-of-the-Art performance among\nopen-source models with a significantly smaller total parameter count. Detailed results and discussion for\nthe benchmarks are presented in Section 3.\nMore specifically, K2-Think incorporates six key innovations to deliver a strong reasoning system. We first\nextend the base model with chain-of-thought capabilities through Supervised Fine-tuning (SFT), followed\nby Reinforcement Learning with Verifiable Rewards (RLVR) to strengthen reasoning performance. We then\nenhance the model with inference-time techniques: agentic planning and test-time scaling using Best-of-N\nsampling. Finally, we deploy K2-Think with two speed optimizations: speculative decoding and Cerebras\u2019\nWafer-Scale Engine, an inference-optimized hardware system. This final stage enables the model to deliver\nits powerful chain-of-thought reasoning capabilities with near-instantaneous response times, deployed at\nspeeds upwards of 2000 tokens per second per user request.\nWith the release of K2-Think, we share our experience and make available an important advancement\nin open-source language modeling, that aggressive post-training engineering and test-time computation,\neven with a modest commodity pretrained base model, can significantly boost reasoning capabilities in a\ncost-effective manner. Prior studies have reported that, in certain regimes, allocating more computation\nduring inference can be more cost-effective than scaling model size (Snell et al., 2025); for recent frontier\nsystems\u2014including OpenAI\u2019s o1\/o3 (OpenAI, 2024, 2025), DeepSeek-R1 (Guo et al., 2025), Google\u2019s\nGemini 2.5 (Google DeepMind, 2025), and xAI\u2019s Grok4 (xAI, 2025) \u2014 model capabilities have been claimed\nto improve with increased test-time budgets (Ji et al., 2025a; Yang et al., 2025b).\nIn addition to releasing code and model weights, we offer K2-Think through a public website and as a\nproduction-ready API endpoint.1 This allows the community to engage directly with a living system,\nshifting the emphasis from static artifacts to a deployable, studyable service that can be stress-tested and\niterated on in the open. As dynamic inference-time reasoning becomes more complex, our API demonstrates\nthe requirements of sophisticated systems for top performance, and provides an operational deployment\ndelivering robustness, safety, and efficiency under real-world constraints.\nIn Section 2 we describe the development process and deployment of K2-Think, using the Cerebras Wafer-\nScale Engine. Section 3 presents a thorough set of evaluations and ablations that attribute gains across\npost-training and test-time computation. Section 4 situates our contributions within the literature. We\nconclude in Section 5 with a summary overview, discuss our motivations for deploying this model, and chart\nfuture directions for extending reasoning performance with openly released models and deployment-ready\n2 K2-Think Development\nWe initiate K2-Think\u2019s development to study a complete post-training recipe for enhanced reasoning and\nestablish best practices for extending our in-house foundation models. Throughout this study, we seek to\nvalidate published best practices as well as test original test-time computation ideas.\nWe choose to fine-tune a 32B-scale base model for K2-Think, as:\n(1) it allows for fast iteration while providing strong base capabilities and\n(2) its size suits both research and consumer computation frameworks.\n1available upon request\nFigure 2: Pass@1 performance over training. Pass@1\nof K2-Think-SFT across five benchmarks; the x-axis is\ntraining progress (epochs), the y-axis is pass@1 score.\nFigure 3: Pass@k on AIME2024. Pass@k of K2-Think-\nSFT and the Qwen-2.5 32B base model; the x-axis is the\nnumber of rollouts per question, the y-axis is pass rate.\nSpecifically, we selected Qwen2.5-32B as it is not tuned for reasoning, allowing us to fully validate our\nrecipe\u2019s effectiveness.\n2.1 Phase 1: Supervised Fine Tuning\nThe initial stage of K2-Think development constitutes supervised fine-tuning (SFT) of the base model\nusing curated long chain-of-thoughts (CoT), establishing the first pillar of our complete reasoning system.\nThis follows the paradigm introduced by DeepSeek in the development of their R1 model (Guo et al., 2025).\nThis phase of training serves to provide guidance to the pre-trained base language model for generating\nstructured responses to complex queries. Additionally, the model is trained to adopt an expected output\nformat in which the model\u2019s reasoning process is made clear prior to producing an answer. By providing a\ntoken-by-token supervisory signal through extended CoT, the base model\u2019s intrinsic computation capabilities\nare expanded substantially (Wei et al., 2022; Schuurmans et al., 2024).\nOur SFT phase2 uses the existing AM-Thinking-v1-Distilled dataset,3 composed of CoT reasoning traces\nand instruction\/response pairs, with prompts drawn from tasks spanning mathematical reasoning, code\ngeneration, scientific reasoning, instruction following, and general chat (Ji et al., 2025b; Tian et al., 2025).\nIn what follows, we will refer to this supervised fine-tuned model as K2-Think-SFT.\n2.1.1 Observations\nOur SFT experiments on Qwen2.5-32B yield several practical insights. Of particular note, we conduct\na step-wise evaluation of K2-Think-SFT across five representative benchmarks. As shown in Figure 2,\nperformance improves rapidly within the first third of training (roughly 0.5 epoch), particularly on\nmathematics benchmarks (AIME 2024 and AIME 2025). After this sharp initial gain, most benchmarks\nplateau, with AIME 2024 stabilizing around 79.3% pass@1 and AIME 2025 around 72.1%. GPQA\nand IFEval continue to exhibit modest upward trends, while LiveCodeBench shows a slower but steady\nimprovement up to 56.4%. We observe that our SFT phase has reached convergence, with the model\nexhibiting diminishing returns to continued training on the dataset.\nApart from pass@1 scores, we also use pass@k to quantify reasoning performance under a fixed sampling\nbudget k. Interpreting the pass@k curve as a capability boundary, we evaluate K2-Think-SFT. In Figure 3,\nassessing performance on AIME2024, our SFT model dominates the base model across sampling budgets.\nThe SFT curves saturate near 93.3% by k \u2248 128, whereas the base model continues to improve but remains\n2Code, based on LLaMA-Factory, for SFT can be found at https:\/\/github.com\/MBZUAI-IFM\/K2-Think-SFT\n3https:\/\/huggingface.co\/datasets\/a-m-team\/AM-Thinking-v1-Distilled\nFigure 4: Ablation Studies on Multi-stage Training and RL from Base Models. (top): RL from base models\nachieves much faster performance gains compared to RL from SFT models. However, a substantial performance gap\nremains, suggesting that SFT enhances the model\u2019s score at the cost of slower subsequent improvement and increased\nsusceptibility to collapse during RL. (bottom): reducing K2-Think-SFT maximum response length significantly\nimpacts performance. Multi-stage training (16,000 to 32,000) struggles to recover original performance, even with\nprolonged training.\nwell below that plateau. The growth in K2-Think-SFT performance as the sampling budget grows suggests\nthere remains an opportunity for improvement during the following RL stage.\n2.2 Phase 2: Reinforcement Learning with Verifiable Rewards\nFollowing the SFT stage, we perform Reinforcement Learning with Verifiable Rewards (RLVR) to train\nK2-Think to excel in domains with verifiable outcomes, which constitutes the second pillar of our full\nreasoning system. RLVR reduces the complexity and cost of preference-based alignment via RLHF (Casper\net al., 2023) by directly optimizing for correctness of model generations.\nFor K2-Think\u2019s RLVR, we use the Guru dataset (Cheng et al., 2025), which was curated to extend\nopen-source reasoning models to verifiable domains beyond Math and Code. We leverage all six domains\nfrom the Guru dataset, comprising nearly 92,000 verifiable prompts that cover Math, Code, Science, Logic,\nSimulation, and Tabular tasks. We refer interested readers to the Guru paper for the detailed dataset\ncuration, including de-duplication, reward designs, and filtering. Our RLVR implementation is built on the\nverl library (Sheng et al., 2025) with the GRPO algorithm (Shao et al., 2024).\n2.2.1 Observations\nIn this subsection we provide a retrospective set of observations that serve as motivation for future\nStarting from a strong SFT checkpoint yields better performance but limits RL gains. While RL\nconsistently improves K2-Think-SFT performance across internal evaluations and public benchmarks, the\nabsolute improvements were modest. As a comparative experiment, we also train a model with the same RL\nrecipe and Guru data directly from the Qwen2.5-32B base. Figure 4 (top) demonstrates that RL training\nfrom the base model achieves nearly 40% improvement on AIME 2024 over the training course, while RL\nfrom K2-Think-SFT yields only 5% improvement. This validates that stronger SFT checkpoints leave less\nroom for RL refinement, consistent with findings from Liu et al. (2025b) regarding the relationship between\nSFT scope and subsequent RL effectiveness. Also, we notice RL training from the SFT checkpoint exhibits\nearly plateauing and even degradation. We suspect that heavily \u201cSFTed\u201d models become constrained in\ntheir ability to explore alternative reasoning strategies during RL training, limiting the policy\u2019s capacity\nfor meaningful adaptation.\nMulti-stage RL training with reduced initial context length degrades performance. Many concurrent\nresearch efforts employ multi-stage training as implicit curriculum learning (An et al., 2025; Liu et al.,\n2025a; Rastogi et al., 2025), incrementally increasing context length. This accelerates early training while\nthe model develops competency and then allows the model in later stages to handle more difficult questions\nwith the extended context. We test this approach by first constraining model output to 16,000 tokens\nduring initial RL training from K2-Think-SFT, then expanding to 32,000 tokens (this is the maximum\nlength seen during the SFT stage) for continued training. As shown in Figure 4 (bottom), this multi-stage\napproach failed to match even the baseline SFT model performance. Cutting the maximum length below\nthe SFT training configuration yields substantially lower performance. This negative result undermines the\noriginal motivation for multi-stage training to achieve on-par or better performance with shorter responses\nto save inference tokens. We suspect that reducing context length below the SFT training regime (32k \u2192\n16k \u2192 32k) disrupts the model\u2019s established reasoning patterns as we do not perform any additional data\nfiltering to correspond to this multi-stage training. However, we do not evaluate expanding beyond the\nSFT context length (e.g., 32k \u2192 48k), as implemented in Polaris (An et al., 2025), which may still provide\n2.3 Phase 3: Test-time Improvement\nTo further enhance K2-Think performance, we develop a test-time scaffolding that implements existing\nmethods as well as integrates an original approach to provide structured input to our post-trained reasoning\nmodel. This subsection details two specific aspects of this scaffolding: agentic planning before reasoning,\nnamely \u201cPlan-Before-You-Think\u201d, and test-time scaling using Best-of-N sampling. These two techniques\nare pillars three and four of the complete K2-Think system.\nA diagram mapping the flow of information from the input provided, down to the final response, is\nillustrated in Figure 5. First, the prompt is restructured to outline a high-level plan, highlighting relevant\nconcepts. This augmented prompt is then passed through the K2-Think model, generating multiple\nresponses. Finally, a pairwise comparison between candidate responses surfaces the best generation as the\nfinal output of our reasoning system. The remainder of this section provides details of how we set-up and\nimplement each of these components.4\n\u201cPlan-Before-You-Think\u201d. The first procedure of K2-Think\u2019s test-time computation is the introduction\nof a planning agent. In our current system implementation, we simply ask the agent to extract key concepts\nfrom the query, and create a high-level plan from them. The generated plan is appended alongside the\noriginal query, and provided to the K2-Think model. K2-Think\u2019s planning agent is simply implemented\nvia prompting an instruction-tuned Language Model. We restrict this \u201cPlan-Before-You-Think\u201d procedure\n4Code for K2-Think test-time improvements is at: https:\/\/github.com\/MBZUAI-IFM\/K2-Think-Inference\nFigure 5: Schematic overview of how K2-Think generates responses via our test-time computation scaffold. A user\nquery is first input to an external model which generates a high-level plan to provide a structured prompt to our\nK2-Think model. We then sample 3 responses, using an external model to select the best which is then provided as\nfrom providing direct answers or any reasoning trace. This deliberation phase, prior to any \u201cthinking\u201d by\nour reasoning model, has some basis in psychology and cognitive science. Planning and reasoning can\nbe considered dual processes of human cognition and decision making (Evans, 2010) where planning is\nconsidered a meta-thinking process developing some structure to help guide one\u2019s thoughts.\nBest-of-N (BoN) sampling. Best-of-N sampling, sometimes called repeated sampling, is a method\nwhere an LLM generates N independent outputs for a given prompt, and a reward model (or verifier)\nchooses the best one according to some metric\u2014such as accuracy, coherence, or alignment with human\npreference (Stiennon et al., 2020; Nakano et al., 2021). This strategy effectively explores multiple possibilities\nand picks the most promising completion.\nImplementation-wise, we pick the answer by comparing the answer candidates pairwise, discarding the one\nthat an independent LLM judges to be worse. In K2-Think, we finally adopt N = 3, which provides a\nreasonable improvement with low cost.\n2.3.1 Observations\nAt inference time, we explore several approaches to enhance the K2-Think model\u2019s performance. We begin\nwith simple engineering adjustments but soon discover that minor changes to our test-time computation\nprocedures significantly impact overall performance.\nWe experiment with temperature tuning, iterating through a list of temperatures from 0.1 to 1.0, but find\nthe overall improvement to be insignificant, leading us to use a temperature of 1.0 for all future runs. We\nalso conduct extensive prompt engineering, trying over 30 different system prompts that utilized techniques\nlike few-shot learning (Brown et al., 2020), role-playing (Kong et al., 2023), and situational prompting.\nHowever, we observe negligible gains.\nMore sophisticated test-time scaling methods are also tried following Sharma (2024). We test several\nstandard approaches including re2 (ReRead) (Xu et al., 2024), self-consistency (Wang et al., 2023b), CoT\nwith reflection (Shinn et al., 2023), and Mixture of Agents (MoA) (Wang et al., 2024b). Among these,\nBest-of-N (BoN) and MoA yield the most notable improvements. While MoA delivers marginally better\nperformance, its significantly higher computational cost lead to the selection of BoN for the final K2-Think\nsystem. A different, more experimental approach involving Reinforcement Learning with rewards drawn\nfrom self-certainty signals (Zhao et al., 2025) is also tested, but it does not lead to any improvement of the\npost-trained model\u2019s performance.\n2.4 Deploying K2-Think\nWe deploy K2-Think on Cerebras Wafer-Scale Engine (WSE) systems, leveraging the world\u2019s largest\nprocessor and speculative decoding (Leviathan et al., 2023) to achieve unprecedented inference speeds for\nthe reasoning system, making up the final two pillars of K2-Think. The WSE delivers approximately 2,000\ntokens per second, representing a 10 times improvement over the nominal 200 tokens per second observed\non typical deployment environments on a regular cloud provider. This dramatic speed-up fundamentally\ntransforms the practical usability of long chain-of-thought reasoning.\nConsider a typical complex reasoning task that generates a 32,000 token response, which is common for\nchallenging mathematical proofs or multi-step coding problems. On the WSE, 32,000 token generation is\ncompleted in just 16 seconds, maintaining user engagement, enabling true back-and-forth problem solving,\nand providing a near real-time chat experience.\nThe performance advantage comes from the unique architecture of WSE. WSE keeps all model weights\nin massive on-chip memory, leveraging 25 Petabytes per second of on-chip memory bandwidth. Since\nauto-regressive models generate tokens serially, memory bandwidth can be a significant bottleneck during\ninference. By integrating greater compute, memory, and memory bandwidth in a single device, wafer-scale\ntechnology enables industry-leading inference speed for generative models.\nThis efficiency proves especially critical for our test-time computation approach and agent-based reasoning\nworkflows. When performing best-of-3 sampling, the system must wait for all three responses to complete\nbefore LLM evaluation can select the optimal solution. Further, multi-step reasoning pipelines that require\nsequential calls for planning and generation suffer from cumulative delays. The WSE\u2019s low-latency inference\nkeeps these workflows interactive, preventing the cascade of delays that would otherwise render complex\nreasoning tasks impractical.\nThe difference between waiting minutes versus seconds for each interaction fundamentally transforms the\nuser experience from batch processing to interactive reasoning. This deployment ensures that K2-Think\nprovides not just frontier reasoning capabilities but also the responsiveness required for practical, real-world\napplications, making sophisticated AI reasoning truly accessible for interactive use cases. We invite everyone\nto experience our K2-Think system, powered by Cerebras\u2019 WSE, via API and through k2think.ai.\n3 K2-Think Evaluation\nWe evaluate K2-Think in comparison with frontier models, both open-weight and proprietary, among a class\nof challenging reasoning benchmarks focused on Math, Code and Science. We design these evaluations to\ndemonstrate that K2-Think, despite only having 32B parameters and fairly modest test-time computation,\npushes the frontier of open-source reasoning models. In particular we find that K2-Think is highly capable\nfor complex Math tasks, as shown in Table 1. In total we evaluate K2-Think on the following benchmarks:\n\u2013 AIME 2024 (MAA, 2024), AIME 2025 (Ye et al., 2025b): The 2024 and 2025 editions of the\nAmerican Invitation Mathematics Examination (AIME), with each year featuring 30 questions\nthat have integer answers.\n\u2013 HMMT25 (Balunovi\u0107 et al., 2025): This dataset, used as part of the MathArena benchmarking\nsuite, is drawn from the Harvard-MIT Mathematics Tournament February 2025 competition,\nfeaturing 30 questions drawn from the subject areas of Algebra+Number Theory, Combinatorics,\n\u2013 Omni-MATH-HARD (Omni-HARD, Gao et al. (2024)): We use the most difficult subset of the\nOmni-MATH dataset, featuring questions sampled from competitive mathematics competitions\nat the Olympiad level from several countries, retaining only those problems that are rated as\nthe top 2 difficulty levels (9.0 and 10.0). This set has 173 questions, a much larger competition\nmath benchmark, and perhaps the most compelling one.\n\u2013 A global micro-average (Micro-Avg.) is obtained by dividing the total number of correct answers\nby the total number of questions across all datasets.\n\u2013 LiveCodeBench (LCBv5, Jain et al. (2024)): A collection of 599 programming challenge\nproblems aggregated from online platforms. We use queries aggregated between July 1, 2024\nand February 1, 2025 (v5).\n\u2013 SciCode (Tian et al., 2024): SciCode evaluates a model\u2019s ability to generate code for solving\n65 realistic scientific research questions, covering 16 subdomains from Physics, Math, Material\nScience, Biology, and Chemistry. We report scores from the version of the benchmark where\nbackground knowledge is included within the prompt. Since SciCode already performs a complex,\nmulti-step planning phase in collating this information we do not run our \u201cPlan-Before-You-\nThink\u201d step during our evaluation of K2-Think on this baseline. All evaluation results include\nsub-problem and full-problem accuracies in Table 1.\n\u2013 GPQA-Diamond (GPQA-D, Rein et al. (2023)): This benchmark is comprised of 198 \u201cGoogle-\nproof\u201d advanced multiple-choice questions written by experts from biology, physics, and chem-\n\u2013 Humanity\u2019s Last Exam (HLE, Phan et al. (2025)): Humanity\u2019s Last Exam was developed by\nsubject-matter experts and consists of 2158 multiple-choice and short-answer questions with\nsolutions that are unambiguous and easily verifiable, but cannot be quickly answered via internet\nWe measure the performance of K2-Think in comparison to frontier reasoning models, both open-source\n{Qwen3-30B-A3B (Yang et al., 2025a), GPT-OSS 20B (Agarwal et al., 2025a), QwQ-32B (Team, 2025),\nOpenReasoning-Nemotron-32B (NVIDIA, 2025), DeepSeek R1 (Guo et al., 2025), DeepSeek-v3.1 (Think-\ning) (DeepSeek, 2025), GPT-OSS 120B (Agarwal et al., 2025a), Qwen3-235B-A22B (Thinking) (Yang et al.,\n2025a)} and proprietary {GPT-5 (High) (OpenAI, 2025), Gemini-2.5 (Pro) (Google DeepMind, 2025), o3\n(High) (OpenAI, 2025)} to adequately assess the advancements made by our post-training and test-time\ncomputation scaffold. We use a standardized evaluation methodology across all benchmarks and models.\nThe maximum generation length is set to 64,000 tokens, sampling temperature is fixed at 1.0, top-p is\n0.95 and the stop token is <\/answer>. Each benchmark result reported in Table 1 is the average of 16\nindependent pass@1 evaluations.\nK2-Think excels in competition math questions. The evaluation results are summarized in Table 1.\nK2-Think, a 32B-parameter model, exhibits a micro-average score of 67.99 across all math questions.\nThis result is particularly noteworthy when compared to other models of similar or slightly larger size, such\nas GPT-OSS 20B (m-avg. 52.50), Qwen3-30B-A3B (m-avg. 33.08), and OpenReasoning-Nemotron-32B\n(m-avg. 65.78). The results clearly show that K2-Think surpasses these models by a significant\nmargin. Furthermore, K2-Think\u2019s performance is not only dominant within its size class but also highly\ncompetitive with models that are orders of magnitude larger. Its math score also surpasses the larger\nmodels, including the two state-of-the-art open source models: DeepSeek V3.1 671B (m-avg. 64.43) and\nGPT-OSS 120B (m-avg. 67.20). Notably, K2-Think performs well on Omni-MATH-HARD (60.73), which\ncontains the most difficult questions across competitions. Such performance places K2-Think at the\ntop of all open source models on math reasoning, and is close to strong proprietary models such as o3\nHigh, showing that K2-Think excels in the most challenging questions.\nQwen3 235B-A22B\nAIME 2024 AIME 2025 HMMT25 Omni-HARD Micro-Avg. LCBv5 SciCode (sub\/main) GPQA-D HLE\nTable 1: Benchmark performance comparison of K2-Think against open-source (top) and proprietary (bottom)\nfrontier models. All metrics are reported as percentages. We find that K2-Think is especially strong on\nchallenging Math benchmarks while also maintaining respectable performance on Code and Science. Values\nmarked with * are directly taken from published results. All other reported values are avg@16 accuracy of generated\nanswers, evaluated locally or through paid API access. From these results, we see that our K2-Think system with\nonly 32B parameters approaches or exceeds the performance of the frontier models that are orders of magnitude\nlarger. \u2020 showing results for the original DeepSeek R1 and V3.1. Since the performance of DeepSeek R1-0528 is\nsimilar to V3.1, we do not report it separately.\nK2-Think is versatile in Science and Coding domains. The evaluation results also show that K2-\nThink demonstrates a robust and competitive capability in both coding and scientific domains, solidifying\nits position as a versatile model. On coding benchmarks, K2-Think achieves a score of 63.97 on\nLiveCodeBench, significantly outperforming its similarly sized peers, including GPT-OSS 20B (42.20) and\nQwen3-30B-A3B (36.9). This performance also surpasses the larger Qwen3 235B-A22B (56.64). When\ncompared to larger models, K2-Think shows parity and even superiority in certain metrics: it achieves\n39.2 on the SciCode benchmark (sub-problems), making it a close second compared with Qwen3 235B-\nA22B (39.3). On scientific reasoning, our system\u2019s performance on the GPQA-Diamond benchmark is\n71.08, superior to most open-source models except OpenReasoning-Nemotron-32B (74.98), and GPT-OSS\n120B (77.04). While its HLE score of 9.95 is not the highest, it remains respectable and indicative of\na broad knowledge base. This combination of strong performance across diverse domains argues that\nK2-Think is not merely a specialist but a versatile model capable of tackling a wide range of analytical\nand knowledge-intensive tasks with high efficacy.\nBeyond the preliminary conclusions shared in this report, our team is continuing to perform additional\nanalyses and comparisons between K2-Think and a more complete set of competing models and reasoning\nIt is however clear that K2-Think presents an advancement in open-source reasoning\nsystems. With a 32B parameter model, and a moderate amount of test-time compute, our system provides\ncomparative performance to models significantly larger (see Figure 1 for a visual depiction). This level\nof parameter efficiency, in terms of benchmark performance is a notable achievement, specifically among\ncomplex mathematics reasoning tasks.\nComponent Analysis of K2-Think Test-Time Computation In order to analyze the individual contri-\nbution of each element of the test-time computation procedure to the final performance of K2-Think, we\nconduct an analysis where we implement each procedure in isolation on top of the post-trained checkpoint.\nThat is, after performing both SFT and RL, we apply only the prompt restructuring via high-level planning\nor best-of-3 re-sampling and verification during evaluation. To simplify the discussion, we present this\ncomponent analysis only using the four Math benchmarks however the overall insights are consistent across\nall other benchmarks.\nAIME 2024 AIME 2025 HMMT25 Omni-MATH-HARD\nSFT+RL Checkpoint\n+ Plan + Bo3 (K2-Think)\nTable 2: Component analysis of the test-time computation procedures used to improve from our post-training\ncheckpoint in the development of our final K2-Think system. The greatest gains come from Best-of-3 sampling,\nfurther improvement is seen after combining with high-level planning.\nSFT+RL Checkpoint\nQwen3-235B-A22B\nTable 3: An analytical comparison of the average number of tokens used between the full K2-Think system and the\npost-training checkpoint. After implementing our test-time computation scaffold, our response length decreases on\naverage, with the percentage of reduction included in the shaded cells. We also compare to the average number of\ntokens generated by top-performing open-weight models, showing better efficiency than Qwen3-235B-A22B and\nsimilar to GPT-OSS 120B.\nThe component analyses presented here are executed in the same fashion as the comparative baselines\ncontained in Table 1. All results presented in Table 2 are averaged over 16 independent runs with the same\nsettings as presented previously. We see in this analysis that the majority of the improvement over the\npost-trained checkpoint is afforded via Best-of-N scaling, using only 3 sampled generations per prompt. In\nisolation, the performance benefit of re-structuring the input prompt with a high-level plan also contributes\nan improvement to performance but with lesser effect. However, in combination with Best-of-N scaling the\noverall test-time procedure offers significant gains, offering 4-6 percentage points of improvement across all\n\u201cPlan-Before-You-Think\u201d Reduces Response Lengths With the complete K2-Think system, we\nrequire a model to create a plan before thinking. While we have shown this procedure to positively affect\nreasoning performance, the expansion of the prompt might cause more tokens used when formulating an\nanswer. However, we found the opposite to be true. We report the average number of tokens K2-Think\ngenerated in the final response before and after implementing test-time computation in Table 3 for select\nbenchmarks evaluated across Math, Code and Science domains, comparing the K2-Think post-training\ncheckpoint and the full system. The inclusion of this plan achieves two benefits: response quality improves\nand there is a reduction in the number of tokens used, by up to nearly 12% in comparison to the post-training\ncheckpoint. Thus, by conducting planning before reasoning, K2-Think provides more concise answers.\nWe also compare the average number of tokens used among the best performing open-weight models. We\nsee that the K2-Think responses are much shorter than Qwen3-235-A22B and are in a range similar to the\nresponses from GPT-OSS 120B in mathematical reasoning. When comparing response lengths for the code\nand science domains, K2-Think are still shorter than Qwen3-235-A22B, but longer than GPT-OSS 120B.\n3.1 Red-teaming K2-Think\nEnsuring the safe operation of a model is essential for its open release. To this end, we systematically\nevaluate K2-Think against adversarial prompts, harmful content, and robustness stress tests using\nestablished public safety benchmarks (Lin et al., 2024). For each benchmark, we sample 100 test cases and\nreport a safe score, where higher values indicate stronger safety performance. To provide a clear picture of\nreal-world risks, we consolidate results into four key aspects that capture the practical safety surfaces most\nrelevant in deployment:\n1. High-Risk Content Refusal \u2014 ability to reject direct requests for unsafe or harmful outputs.\n2. Conversational Robustness \u2014 maintaining safe behavior consistently across multi-turn dialogues.\n3. Cybersecurity & Data Protection \u2014 resilience against information leakage, prompt extraction, and\ncyberattack assistance.\n4. Jailbreak Resistance \u2014 robustness to adversarial attacks designed to bypass safeguards.\nThis framework provides a clearer operational safety profile and guides targeted mitigations.\nHigh-Risk Content Refusal We first check the\nmodel\u2019s reliability in rejecting unsafe requests. Evalu-\nation spans complementary datasets covering harmful\ninstructions (Do-Not-Answer (Wang et al., 2023c),\nHarmBench (Mazeika et al., 2024)), physical harm\nscenarios (PhysicalSafety (Bianchi et al., 2023)), ba-\nsic safety checks (SimpleSafetyTests (Vidgen et al.,\n2023)), toxic content generation (ToxiGen (Hartvigsen\net al., 2022; Hosseini et al., 2023)), commonsense safety\n(CoNA (Bianchi et al., 2023)), and harmful Q&A\n(HarmfulQ (Shaikh et al., 2023)).\nSimpleSafetyTests\nThe results of this analysis are featured in Table 4.\nK2-Think demonstrates extensive ability to avoid gen-\nerating high-risk content as measured by near-perfect\nscores in 4 out of 7 benchmarks. Of the remaining 3\nbenchmarks in this aspect of safety evaluation, HarmBench and PhysicalSafety reveal a weakness in our\nsystem toward recognizing cyber or physical risks. We are actively working to improve our system along\nthese dimensions of risk in its public facing deployment.\nTable 4: High-risk content refusal results across\nsafety datasets. The model achieves near-perfect\nperformance on four of seven tasks, with clear im-\nprovement opportunities on HarmBench and Physi-\nConversational Robustness Next, we assess refusal consistency across multi-turn adversarial dialogues\nusing DialogueSafety (Dinan et al., 2019), HH-RLHF (Bai et al., 2022), and DICES350 (Aroyo et al.,\n2023) for dynamic dialogue manipulations.\nWe see in Table 5 that K2-Think is especially robust to sustained adversarial dialogues and repeated efforts\nto ellicit harmful behaviors from our reasoning system. Here, K2-Think is near perfect at maintaining\nrefusal consistency on both the DialogueSafety and HH-RLHF benchmarks.\nCybersecurity & Data Protection & Prompt Extraction We evaluate resilience against data leakage\nand misuse with PersonalInfoLeak (Li et al., 2023) (privacy leakage), CyberattackAssistance (Bhatt\net al., 2023) (hacking assistance), and PromptExtractionRobustness (Toyer et al., 2023) (system prompt\nWe see in Table 6 that K2-Think is able to resist attempts to extract personally identifying information\nwhile unfortunately exhibiting some susceptibility to revealing the system prompt and aiding in devising\ncyberattacks. This indicates an opportunity to further tune our reasoning system for improved resilience.\nPersonalInfoLeak (few-shot)\nCyberattackAssistance\nPromptExtractionRobustness\nTable 5: Conversational robustness results across dia-\nlogue safety datasets. The model exhibits notable ro-\nbustness to multi-turn adversarial attempts to produce\nharmful outputs, with particular strength on Dialogue-\nSafety and room for improvement on DICES350.\nTable 6: Cybersecurity, data protection, and prompt\nextraction results. The model demonstrates robustness\nagainst leaking personal information, with significant\nroom for improvement on cyberattack assistance preven-\ntion and prompt extraction robustness.\nJailbreak Resistance Finally, we evaluate var-\nious adversarial attack strategies: hidden triggers\n(LatentJailbreak (Qiu et al., 2023)), prompt redi-\nrection (PromptInjection (Liu et al., 2023b)),\ninstruction overrides (Gandalf Ignore (Schul-\nhoff et al., 2023)), role-play attacks (DAN (Shen\net al., 2023)), cross-lingual exploits (Multilingual\n(Wang et al., 2023a)), grammatical perturbations\n(Tense Change Lin et al. (2024)), adversarial\ndemonstrations (Few-Shot Attack (Wei et al.,\n2023b)), bias-driven attacks (One-Sided State-\nment (Liu et al., 2023a)), identity manipulation\n(Persona Modulation (Shah et al., 2023)), and\ndirect refusal bypasses (Refusal Suppression\n(Wei et al., 2023a)).\nFew-Shot Attack\nPromptInjection\nOne-Sided Statement\nRefusal Suppression\nPersona Modulation\nDo-Anything-Now\nLatentJailbreak\nTable 7: Jailbreak resistance results across adversarial\nprompt techniques. The model demonstrates mixed re-\nsilience, with strong performance against direct attacks\nand vulnerabilities to indirect methods.\nK2-Think\u2019s jailbreak resistance results (shown\nin Table 7) demonstrate a mixture of resilience\nand susceptibility to various adversarial prompt strategies. K2-Think exhibits strong performance when\nattacks are immediately apparent but shows an apparent weakness to indirect attacks. This lack of\ngeneralized robustness to adversarial jailbreaking attempts illustrates a need to thoroughly improve our\npublicly deployed reasoning system.\nOverall Results Across all four dimensions, results are aggregated into a single Safety-4 macro score,\ncomputing the average from the four analyses performed as part of our safety testing of K2-Think. The\nmacro average of each of the four analyses are included in Table 8.\nOverall, K2-Think achieves a Safety-4 macro score of 0.75, indicating a solid baseline of safety with\nstrong performance in refusing harmful content and maintaining consistent behavior in conversations.\nAt the same time, we recognize that further work is required to strengthen cybersecurity defenses,\njailbreak robustness, and refusal calibration. While establishing a solid baseline, we acknowledge clear\nopportunities to improve the safety of our reasoning system. Addressing these areas is an active priority in\nour roadmap to further improve K2-Think under adversarial conditions.\nMacro-Avg Score\nHigh-Risk Content Refusal\nConversational Robustness\nCybersecurity & Data Protection\nJailbreak Resistance\nSafety-4 Macro (avg)\nTable 8: Overall Safety-4 results which is a composite score of the four safety surfaces evaluated in this broad\nanalysis. The macro score of 0.75 indicates that K2-Think establishes a solid safety profile with specific strengths\nin harmful content refusal and maintaining consistent behavior in conversations.\nExtending base language model capabilities via SFT Supervised fine-tuning (SFT) has become a\nwidely used post-training method to extend the capability boundary of Large Language Models (Ouyang\net al., 2022; Dubey et al., 2024; Guo et al., 2025; Bercovich et al., 2025). Early SFT work primarily focused\non task specialization, adapting foundational models to specific NLP benchmarks like text classification\nor translation on narrowly-defined datasets (Liu et al., 2019; Raffel et al., 2020). This paradigm shifted\nsignificantly with the rise of large-scale instruction tuning; the goal evolved from single-task mastery to\ncreating general-purpose assistants capable of following diverse human commands (Wei et al., 2021; Ouyang\net al., 2022; Taori et al., 2023). More recently, SFT has pivoted towards enhancing complex reasoning on\ndiverse downstream tasks like math, code, and science (Hui et al., 2024; Yang et al., 2024b; Abdin et al.,\n2025; Liu et al.). Some approaches focus on scale, constructing massive datasets of reasoning traces to\ninstill robust, long-chain-of-thought capabilities in models (Guha et al., 2025; Tian et al., 2025; Liu et al.,\n2025b). In contrast, other methods demonstrate that meticulously curated, high-quality data can also\nendow LLMs with expert-level reasoning in domains like math (Ye et al., 2025a; Muennighoff et al., 2025).\nBuilding on the above, our work conducts analysis and provides practical insights on SFT.\nImproving LLM Reasoning with RL Reinforcement Learning from Verifiable Rewards (RLVR) has\nemerged as a powerful paradigm for enhancing the reasoning capabilities of Large Language Models (Guo\net al., 2025; OpenAI, 2024). Following initial successes, a significant body of open work has explored\nRLVR, primarily concentrating on specializing models for highly challenging single domains. Efforts such as\nOpen-Reasoner-Zero (Hu et al., 2025), Skywork-OR1 (He et al., 2025), DeepScaler (Luo et al., 2025b), and\nSimpleRL (Zeng et al., 2025) have notably leveraged extensive mathematical data to achieve state-of-the-art\nperformance on complex math benchmarks. Similarly, DeepCoder (Luo et al., 2025a) focused on RL for code\ngeneration tasks. While powerful within their specific areas, this domain-specific focus inherently limits the\ngeneralizability of the resulting models across the broader landscape of reasoning tasks. Concurrent works\nto our K2-Think development like General-Reasoner (Ma et al., 2025) and Nemotron-CrossThinker (Akter\net al., 2025) have begun to explore broader domains for RL training. However, none of these works explore\nthe added utility of test-time computation for improving the general reasoning capabilties of post-trained\nTest Time Scaling Test-time scaling has been a major component of proprietary models released in\nrecent years; such as o1 (OpenAI, 2024), Grok Heavy (xAI, 2025), Gemini 2.5 (Google DeepMind, 2025),\nand GPT-5 (OpenAI, 2025). However, with fairly little transparency about specific components and their\noverall effect. The closest work to ours is PlanGEN (Parmar et al., 2025), a multi-model framework for\nplanning and reasoning combining a constraint model, a verification model, and a selection model to\nguide inference-time algorithms including Best-of-N. By using constraint-guided iterative verification and\na modified UCB-based selection policy, PlanGEN chooses the most suitable algorithm for each problem\ninstance. Importantly, they use Best-of-N with verifiers on the plans: we use it for the generated responses.\nAlso related are general LLM-based hierarchical reasoning approaches, particularly those that operate\nwith at least one level of hierarchy doing planning. Wang et al. (2024a) has a planning model provide\nhigh-level strategy while a solver model performs detailed reasoning. HyperTree Planning (Gui et al., 2025)\nmodels planning with a hypertree-structure, allowing LLMs to decompose planning queries into structured\nsub-tasks. Wang et al. (2025a) demonstrates a brain-inspired architecture with separate recurrent modules\nfor high-level planning and low-level reasoning, showing that explicit separation of timescales improves\nperformance on algorithmic reasoning tasks. Our novelty is to combine our \u201cPlan-Before-You-Think\u201d\napproach, a type of multi-LLM-hierarchical reasoning, with Best-of-N with verifiers (Cobbe et al., 2021) in\norder to return the best responses.\n5.1 Primary technical insights\nMultiple domains are important for post-training. Following the findings from Guru (Cheng et al.,\n2025), there is a need to expand post-training to include more domains for general reasoning models. The\neffect of post-training, and the domains utilized, is nuanced. Domains commonly included in pre-training\n(Math, Code, and Science) broadly benefit from a variety of post-training data as the refinement of the\nmodel\u2019s chains of thought is supported by the knowledge it already has. However those domains with\nlimited pre-training exposure\u2013like Logic and Simulation tasks\u2013only improve when they are included in the\nRL training pipeline. This indicates that using diverse, multi-domain datasets is critical for developing\ntruly versatile reasoning models.\nTest-time computation performance gains can be additive with the right combination. We find that\ntwo simple test-time computation procedures work well together: our \u201cPlan-Before-You-Think\u201d prompt\nrestructuring in conjunction with Best-of-N scaling. Each individual method does improve over the K2-\nThink model but the largest gains in performance are seen when these components are combined. To our\nsurprise, simply extracting a high level plan focused on the core concepts associated with the input and\nonly sampling 3 candidate responses are sufficient to provide significant improvement.\n\u201cPlan-Before-You-Think\u201d improves model performance while reducing token expenditure. By\nrequiring the model to create a plan before initiating its reasoning process, we achieve two benefits: planning\nitself improves response quality, and response lengths are reduced by nearly 12%.\n5.2 Looking forward\nEmpowering small models to \u201cpunch above their weight\u201d. With the complete K2-Think system, we\ndemonstrate that a 32B-scale model, post-trained to produce long reasoning chains of thought, paired with\nrelatively little test-time computation can endow the small model with capabilities that are competitive\nwith models with orders of magnitude more parameters. Altogether our end-to-end reasoning system\nunlocks performance at the frontier of current open-source capabilities.\nBeyond Open Source. We are extending the limit of our open-source activities beyond data, models and\ntraining artifacts. This expansion of our open-source efforts will now include deploying our full reasoning\nsystem for public use. We are publishing our test-time computation implementation as well. K2-Think is\nbroadly available via API and an online web portal. In this we are opening avenues to explore how to best\n\u201cbattle-test\u201d public facing LLM infrastructure. Details about how to use and interact with K2-Think can\nbe found at k2think.ai, we proudly invite all to try it out!\nK2-Think is a compelling stepping stone for our ongoing efforts to broaden access to foundation model\nresearch and development through open-science. Our motivation to deploy K2-Think for public use is\ngrounded in curiosity about how to best engineer inference systems for large-scale foundation models.\nSecondarily, as we continue scaling our own open-source models, there will be a time when simply making\nthe weights and training artifacts public is no longer useful as fewer institutions and organizations will\nbe able to host or interact with the models themselves. This by-product of our work, investigating and\nbuilding ever more capable open models, is antithetical to our founding ethos as a research institute. We\nare committed to making publicly available as much of our model development and deployment as possible\nin order to enable all who are interested to build on or contribute to our work. The lessons we learn through\ndeployment with K2-Think will be critical to our ongoing development of larger and more capable models.\nThe authors hereby acknowledge and thank the strong support and collaboration of G42 for their contribu-\ntions throughout the project, including the essential computational infrastructure as well as significant\nexpertise in evaluation methodology and safety protocols. This partnership proved instrumental in advancing\nour research objectives.\nMarah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo\nde Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi-4-reasoning technical report. arXiv preprint\narXiv:2504.21318, 2025.\nSandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K Arora, Yu Bai,\nBowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925,\nShivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy\nminimization in llm reasoning, 2025b. URL https:\/\/arxiv.org\/abs\/2505.15134.\nSyeda Nahida Akter, Shrimai Prabhumoye, Matvei Novikov, Seungju Han, Ying Lin, Evelina Bakhturi, Eric Nyberg,\nYejin Choi, Mostofa Patwary, Mohammad Shoeybi, et al. Nemotron-crossthink: Scaling self-learning beyond math\nreasoning. arXiv preprint arXiv:2504.13941, 2025.\nChenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu,\nMingxuan Wang, and Lingpeng Kong. Polaris: A post-training recipe for scaling reinforcement learning on\nadvanced reasoning models, 2025. URL https:\/\/hkunlp.github.io\/blog\/2025\/Polaris.\nLora Aroyo, Alex S. Taylor, Mark D\u00edaz, Christopher Homan, Alicia Parrish, Gregory Serapio-Garc\u00eda, Vinodkumar\nPrabhakaran, and Ding Wang. DICES dataset: Diversity in conversational AI evaluation for safety. In Advances in\nNeural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023,\nNeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http:\/\/papers.nips.cc\/paper\\_fi\nles\/paper\/2023\/hash\/a74b697bce4cac6c91896372abaa8863-Abstract-Datasets\\_and\\_Benchmarks.html.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav\nFort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning\nfrom human feedback, 2022. URL https:\/\/doi.org\/10.48550\/arXiv.2204.05862.\nMislav Balunovi\u0107, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovi\u0107, and Martin Vechev. Matharena: Evaluating llms\non uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe,\nTomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal,\nAlexander Bukharin, Yian Zhang, Tugrul Konuk, Gerald Shen, Ameya Sunil Mahabaleshwarkar, Bilal Kartal, Yoshi\nSuhara, Olivier Delalleau, Zijia Chen, Zhilin Wang, David Mosallanezhad, Adi Renduchintala, Haifeng Qian, Dima\nRekesh, Fei Jia, Somshubra Majumdar, Vahid Noroozi, Wasi Uddin Ahmad, Sean Narenthiran, Aleksander Ficek,\nMehrzad Samadi, Jocelyn Huang, Siddhartha Jain, Igor Gitman, Ivan Moshkov, Wei Du, Shubham Toshniwal,\nGeorge Armstrong, Branislav Kisacanin, Matvei Novikov, Daria Gitman, Evelina Bakhturina, Jane Polak Scowcroft,\nJohn Kamalu, Dan Su, Kezhi Kong, Markus Kliegl, Rabeeh Karimi, Ying Lin, Sanjeev Satheesh, Jupinder Parmar,\nPritam Gundecha, Brandon Norick, Joseph Jennings, Shrimai Prabhumoye, Syeda Nahida Akter, Mostofa Patwary,\nAbhinav Khattar, Deepak Narayanan, Roger Waleffe, Jimmy Zhang, Bor-Yiing Su, Guyue Huang, Terry Kong,\nParth Chadha, Sahil Jain, Christine Harvey, Elad Segal, Jining Huang, Sergey Kashirsky, Robert McQueen, Izzy\nPutterman, George Lam, Arun Venkatesan, Sherry Wu, Vinh Nguyen, Manoj Kilaru, Andrew Wang, Anna Warno,\nAbhilash Somasamudramath, Sandip Bhaskar, Maka Dong, Nave Assaf, Shahar Mor, Omer Ullman Argov, Scot\nJunkin, Oleksandr Romanenko, Pedro Larroy, Monika Katariya, Marco Rovinelli, Viji Balas, Nicholas Edelman,\nAnahita Bhiwandiwalla, Muthu Subramaniam, Smita Ithape, Karthik Ramamoorthy, Yuting Wu, Suguna Varshini\nVelury, Omri Almog, Joyjit Daw, Denys Fridman, Erick Galinkin, Michael Evans, Shaona Ghosh, Katherine\nLuna, Leon Derczynski, Nikki Pope, Eileen Long, Seth Schneider, Guillermo Siman, Tomasz Grzegorzek, Pablo\nRibalta, Monika Katariya, Chris Alexiuk, Joey Conway, Trisha Saar, Ann Guan, Krzysztof Pawelec, Shyamala\nPrayaga, Oleksii Kuchaiev, Boris Ginsburg, Oluwatobi Olabiyi, Kari Briski, Jonathan Cohen, Bryan Catanzaro,\nJonah Alben, Yonatan Geifman, and Eric Chung. Llama-nemotron: Efficient reasoning models, 2025. URL\nhttps:\/\/arxiv.org\/abs\/2505.00949.\nManish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song,\nFaizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis\nKozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer\nWhitman, and Joshua Saxe. Purple llama cyberseceval: A secure coding benchmark for language models, 2023.\nURL https:\/\/arxiv.org\/abs\/2312.04724.\nFederico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R\u00f6ttger, Dan Jurafsky, Tatsunori Hashimoto, and James\nZou. Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.\nCoRR, abs\/2309.07875, 2023. URL https:\/\/doi.org\/10.48550\/arXiv.2309.07875.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\nStephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel\nFreedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of\nreinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.\nZhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo\nDey, Yuheng Zha, et al. Revisiting reinforcement learning for llm reasoning from a cross-domain perspective.\narXiv preprint arXiv:2506.14965, 2025.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,\nJerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv\npreprint arXiv:2110.14168, 2021.\nDeepSeek. Deepseek-v3.1 release, August 2025. URL https:\/\/api-docs.deepseek.com\/news\/news250821.\nEmily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. Build it break it fix it for dialogue\nsafety: Robustness from adversarial human attack. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan,\neditors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4537\u20134546, Hong Kong,\nChina, November 2019. Association for Computational Linguistics. URL https:\/\/aclanthology.org\/D19-1461\/.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages\narXiv\u20132407, 2024.\nJonathan St BT Evans. Intuition and reasoning: A dual-process perspective. Psychological Inquiry, 21(4):313\u2013326,\nBofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin\nXu, et al. Omni-math: A universal olympiad level mathematic benchmark for large language models. arXiv\npreprint arXiv:2410.07985, 2024.\nGoogle DeepMind. Gemini 2.5: Our newest Gemini model with thinking - The Keyword. https:\/\/blog.google\/te\nchnology\/google-deepmind\/gemini-model-thinking-updates-march-2025\/#gemini-2-5-thinking, March\nEtash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean\nMercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel,\nSachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar,\nKartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li,\nAchal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal,\nSaadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori\nHashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and\nLudwig Schmidt. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. URL\nhttps:\/\/arxiv.org\/abs\/2506.04178.\nRunquan Gui, Zhihai Wang, Jie Wang, Chi Ma, Huiling Zhen, Mingxuan Yuan, Jianye HAO, Defu Lian, Enhong\nChen, and Feng Wu. Hypertree planning: Enhancing llm reasoning via hierarchical thinking. In Forty-second\nInternational Conference on Machine Learning, 2025.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi\nWang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv\npreprint arXiv:2501.12948, 2025.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: A\nlarge-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3309\u20133326,\nJujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng\nXu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork\nopen reasoner series. https:\/\/capricious-hydrogen-41c.notion.site\/Skywork-Open-Reaonser-Series-1\nd0bc9ae823a80459b46c149e4f51680, 2025. Notion Blog.\nSaghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah. An empirical study of metrics to measure\nrepresentational harms in pre-trained language models. CoRR, abs\/2301.09211, 2023. URL https:\/\/doi.org\/10\n.48550\/arXiv.2301.09211.\nJingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero:\nAn open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290,\nBinyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu,\nKeming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024.\nNaman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama,\nKoushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models\nfor code. arXiv preprint arXiv:2403.07974, 2024.\nYixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian Mo, and Min Zhang. Test-time compute: from\nsystem-1 thinking to system-2 thinking. arXiv preprint arXiv:2501.02497, 2025a.\nYunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, and Xiangang Li.\nAm-thinking-v1: Advancing the frontier of reasoning at 32b scale. arXiv preprint arXiv:2505.08311, 2025b.\nAobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, and Xiaohang\nDong. Better zero-shot reasoning with role-play prompting. arXiv preprint arXiv:2308.07702, 2023.\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In\nInternational Conference on Machine Learning, pages 19274\u201319286. PMLR, 2023.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-step jailbreaking pri-\nvacy attacks on chatgpt. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore,\nDecember 6-10, 2023, pages 4138\u20134153, 2023. URL https:\/\/aclanthology.org\/2023.findings-emnlp.272.\nLizhi Lin, Honglin Mu, Zenan Zhai, Minghan Wang, Yuxia Wang, Renxi Wang, Junjie Gao, Yixuan Zhang, Wanxiang\nChe, Timothy Baldwin, Xudong Han, and Haonan Li. Against the achilles\u2019 heel: A survey on red teaming for\ngenerative models, 2024. URL https:\/\/arxiv.org\/abs\/2404.00629.\nChengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, and Fei Wu. Goal-oriented\nprompt attack and safety evaluation for llms, 2023a. URL https:\/\/arxiv.org\/abs\/2309.11830.\nMingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged\nreinforcement learning expands reasoning boundaries in large language models, 2025a. URL https:\/\/arxiv.or\ng\/abs\/2505.24864.\nYi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang\nLiu. Prompt injection attack against llm-integrated applications, 2023b. URL https:\/\/doi.org\/10.48550\/arX\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\nZihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-\nnemotron 1.1: Advancing math and code reasoning through sft and rl synergy. arXiv preprint arXiv:2506.13284,\nZihang Liu, Tianyu Pang, Oleg Balabanov, Chaoqun Yang, Tianjin Huang, Lu Yin, Yaoqing Yang, and Shiwei\nLiu. Lift the veil for the truth: Principal weights emerge after rank reduction for reasoning-focused supervised\nfine-tuning. In Forty-second International Conference on Machine Learning.\nMichael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin\nCai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: A fully open-source\n14b coder at o3-mini level, 2025a. URL https:\/\/pretty-radio-b75.notion.site\/DeepCoder-A-Fully-Ope\nn-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51. Notion Blog.\nMichael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo,\nLi Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl,\n2025b. URL https:\/\/pretty-radio-b75.notion.site\/DeepScaleR-Surpassing-O1-Preview-with-a-1-5\nB-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2. Notion Blog.\nXueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner: Advancing llm\nreasoning across all domains. https:\/\/github.com\/TIGER-AI-Lab\/General-Reasoner\/blob\/main\/General_\nReasoner.pdf, 2025.\nMAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination\n- AIME 2024, February 2024. URL https:\/\/maa.org\/math-competitions\/american-invitational-mathema\ntics-examination-aime.\nMantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li,\nSteven Basart, Bo Li, David A. Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluation framework\nfor automated red teaming and robust refusal. CoRR, abs\/2402.04249, 2024. URL https:\/\/doi.org\/10.48550\n\/arXiv.2402.04249.\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy\nLiang, Emmanuel Cand\u00e8s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393,\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu\nJain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin\nButton, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering\nwith human feedback. arXiv preprint arXiv:2112.09332, 2021.\nNVIDIA. Openreasoning-nemotron-32b, july 2025. URL https:\/\/huggingface.co\/nvidia\/OpenReasoning-Nemot\nron-32B. Large language model for mathematical, coding, and scientific reasoning. Based on Qwen2.5-32B-Instruct\nwith 32B parameters. Released July 16, 2025.\nOpenAI. OpenAI o1 System Card. https:\/\/openai.com\/index\/openai-o1-system-card\/, 2024.\nOpenAI. Introducing GPT-5. https:\/\/openai.com\/index\/introducing-gpt-5\/, 2025. Accessed: 2025-09-04.\nOpenAI. Introducing openai o3 and o4-mini, 2025. URL https:\/\/openai.com\/index\/introducing-o3-and-o4-m\nini\/. Accessed: 2025-06-12.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in neural information processing systems, 35:27730\u201327744, 2022.\nMihir Parmar, Xin Liu, Palash Goyal, Yanfei Chen, Long Le, Swaroop Mishra, Hossein Mobahi, Jindong Gu,\nZifeng Wang, Hootan Nakhost, et al. Plangen: A multi-agent framework for generating planning and reasoning\ntrajectories for complex problem solving. arXiv preprint arXiv:2502.16111, 2025.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed\nShaaban, John Ling, Sean Shi, et al. Humanity\u2019s last exam. arXiv preprint arXiv:2501.14249, 2025.\nHuachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. Latent jailbreak: A benchmark for\nevaluating text safety and output robustness of large language models. CoRR, abs\/2307.08487, 2023. URL\nhttps:\/\/doi.org\/10.48550\/arXiv.2307.08487.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of\nmachine learning research, 21(140):1\u201367, 2020.\nAbhinav Rastogi, Albert Q Jiang, Andy Lo, Gabrielle Berrada, Guillaume Lample, Jason Rute, Joep Barmentlo,\nKarmesh Yadav, Kartik Khandelwal, Khyathi Raghavi Chandu, et al. Magistral. arXiv preprint arXiv:2506.10910,\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian\nMichael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark, 2023. URL https:\n\/\/arxiv.org\/abs\/2311.12022.\nSander V Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Fran\u00c3ois Bouchard, Chenglei Si, Jordan Lee Boyd-Graber,\nSvetlina Anati, Valen Tagliabue, Anson Liu Kost, and Christopher R Carnahan. Ignore this title and hackaprompt:\nExposing systemic vulnerabilities of llms through a global prompt hacking competition. In Empirical Methods in\nNatural Language Processing, 2023.\nDale Schuurmans, Hanjun Dai, and Francesco Zanini. Autoregressive large language models are computationally\nuniversal. arXiv preprint arXiv:2410.03170, 2024.\nRusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush Tagade, Stephen Casper, and Javier Rando. Scalable\nand transferable black-box jailbreaks for language models via persona modulation. CoRR, abs\/2311.03348, 2023.\nURL https:\/\/doi.org\/10.48550\/arXiv.2311.03348.\nOmar Shaikh, Hongxin Zhang, William Held, Michael S. Bernstein, and Diyi Yang. On second thought, let\u2019s not\nthink step by step! bias and toxicity in zero-shot reasoning. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14,\n2023, pages 4454\u20134470, 2023. URL https:\/\/doi.org\/10.18653\/v1\/2023.acl-long.244.\nRulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert,\nSewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious\nrewards: Rethinking training signals in rlvr. https:\/\/rethink-rlvr.notion.site\/Spurious-Rewards-Rethi\nnking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f, 2025. Notion Blog.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nYK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv\npreprint arXiv:2402.03300, 2024.\nAsankhaya Sharma. Optillm: Optimizing inference proxy for llms, 2024. URL https:\/\/github.com\/codelion\/op\nXinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. \"do anything now\": Characterizing\nand evaluating in-the-wild jailbreak prompts on large language models. CoRR, abs\/2308.03825, 2023. URL\nhttps:\/\/doi.org\/10.48550\/arXiv.2308.03825.\nGuangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin,\nand Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. In Proceedings of the Twentieth European\nConference on Computer Systems, pages 1279\u20131297, 2025.\nNoah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language\nagents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:8634\u20138652,\nCharlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be\nmore effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning\nRepresentations, 2025. URL https:\/\/openreview.net\/forum?id=4FWAwZtd2n.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,\nand Paul F Christiano. Learning to summarize with human feedback. Advances in neural information processing\nsystems, 33:3008\u20133021, 2020.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.\nQwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025.\nMinyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji,\nKittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang,\nBohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua\nTao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. Scicode: A research coding benchmark curated by\nscientists, 2024.\nXiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, and Xiangang Li.\nNot all correct answers are equal: Why your distillation source matters. arXiv preprint arXiv:2505.14464, 2025.\nURL https:\/\/arxiv.org\/abs\/2505.14464.\nSam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim\nElmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. Tensor trust: Interpretable prompt\ninjection attacks from an online game. CoRR, abs\/2311.01011, 2023. URL https:\/\/doi.org\/10.48550\/arXiv\nBertie Vidgen, Hannah Rose Kirk, Rebecca Qian, Nino Scherrer, Anand Kannappan, Scott A. Hale, and Paul\nR\u00f6ttger. Simplesafetytests: a test suite for identifying critical safety risks in large language models. CoRR,\nabs\/2311.08370, 2023. URL https:\/\/doi.org\/10.48550\/arXiv.2311.08370.\nDanqing Wang, Zhuorui Ye, Fei Fang, and Lei Li. Cooperative strategic planning enhances reasoning capabilities in\nlarge language models. arXiv preprint arXiv:2410.20007, 2024a.\nGuan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori.\nHierarchical reasoning model. arXiv preprint arXiv:2506.21734, 2025a.\nJunlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. Mixture-of-agents enhances large language\nmodel capabilities. arXiv preprint arXiv:2406.04692, 2024b.\nWenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, and Michael R. Lyu.\nAll languages matter: On the multilingual safety of large language models. CoRR, abs\/2310.00905, 2023a. URL\nhttps:\/\/doi.org\/10.48550\/arXiv.2310.00905.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh\nInternational Conference on Learning Representations, 2023b. URL https:\/\/openreview.net\/forum?id=1PL1\nYiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang,\nJianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for\nreasoning in large language models with one training example, 2025b. URL https:\/\/arxiv.org\/abs\/2504.20571.\nYuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. Do-not-answer: A dataset for evaluating\nsafeguards in llms. CoRR, abs\/2308.13387, 2023c. URL https:\/\/doi.org\/10.48550\/arXiv.2308.13387.\nZengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Revisiting mid-training in the era of rl scaling.\nhttps:\/\/tinyurl.com\/OctoThinker, 2025c. Notion Blog.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does LLM safety training fail? In Advances\nin Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems\n2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. URL http:\/\/papers.nips.cc\/p\naper\\_files\/paper\/2023\/hash\/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai,\nand Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing\nsystems, 35:24824\u201324837, 2022.\nZeming Wei, Yifei Wang, and Yisen Wang. Jailbreak and guard aligned language models with only few in-context\ndemonstrations. CoRR, abs\/2310.06387, 2023b. URL https:\/\/doi.org\/10.48550\/arXiv.2310.06387.\nxAI. Grok 4. https:\/\/x.ai\/news\/grok-4, July 2025.\nGuowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason\nstep-by-step. arXiv preprint arXiv:2411.10440, 2024.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei\nHuang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a.\nAn Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren\nZhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement.\narXiv preprint arXiv:2409.12122, 2024b.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen\nHuang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a.\nWenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for\nllm reasoning. arXiv preprint arXiv:2502.18080, 2025b.\nYixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning.\narXiv preprint arXiv:2502.03387, 2025a.\nYixin Ye, Yang Xiao, Tiantian Mi, and Pengfei Liu. Aime-preview: A rigorous and immediate evaluation framework\nfor advanced mathematical reasoning, 2025b.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu,\nXin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476,\nYang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement\nlearning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837,\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating\nand taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025.\nXuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external\nrewards. arXiv preprint arXiv:2505.19590, 2025.",
    "embedding":[
      0.0232333839,
      -0.0794232413,
      -0.0335880555,
      -0.0384862758,
      0.0176848546,
      -0.0472636968,
      -0.0058281762,
      0.0567992553,
      0.0212515295,
      0.0957205817,
      -0.067117773,
      -0.0166348703,
      0.0707603395,
      0.0131213143,
      0.0132372715,
      0.0160932429,
      0.079805389,
      -0.0368966945,
      -0.1130525768,
      -0.0845343992,
      -0.0152978702,
      -0.0082478123,
      0.0060818275,
      -0.0579037964,
      -0.0524486192,
      -0.0207233112,
      0.0206263214,
      0.0242460053,
      0.0123514393,
      -0.0271730199,
      -0.0195249896,
      0.0479635224,
      -0.017101001,
      -0.0306763537,
      0.010803598,
      -0.0014736749,
      -0.0722004995,
      -0.0246268045,
      -0.0635080859,
      0.0276111048,
      0.0003207655,
      -0.0282409396,
      -0.0391079523,
      0.0267827269,
      0.1160700917,
      0.0541412272,
      0.0032042316,
      0.0090732155,
      -0.0012146087,
      -0.0721547008,
      -0.0525846332,
      -0.0466696955,
      -0.0454584211,
      0.0064775045,
      0.0343570895,
      0.0320252515,
      -0.0423326604,
      0.0381112434,
      -0.0056986641,
      -0.0496506281,
      -0.0320527069,
      -0.0430028774,
      0.0148704601,
      -0.0577519797,
      -0.0081279119,
      -0.0430394188,
      -0.0033080759,
      0.056822136,
      -0.0168546475,
      0.0356347896,
      0.0303793848,
      0.0353731923,
      -0.0340190902,
      -0.0271128844,
      0.0370090716,
      0.001784832,
      0.0626780763,
      0.032434158,
      0.0268415939,
      -0.11016725,
      0.0446389876,
      -0.0250128526,
      -0.0758694857,
      -0.0563630089,
      -0.0002059066,
      -0.0460295789,
      -0.0396212973,
      -0.0442574583,
      0.0896414965,
      -0.0318692699,
      0.0872191414,
      -0.033962559,
      0.0535587184,
      -0.0076552746,
      0.0763277188,
      0.0599246398,
      0.0429773219,
      0.0068554739,
      -0.0993624628,
      0.0712685138,
      -0.0221293475,
      0.0487730354,
      -0.035734497,
      -0.0367241837,
      0.074144803,
      -0.0935862213,
      0.0415173993,
      0.04451821,
      0.0401403308,
      -0.0594143122,
      0.0076078069,
      0.0572369844,
      -0.0521260649,
      0.0192879755,
      0.0114983441,
      -0.0108937221,
      0.0501855128,
      -0.0045982478,
      0.0050823917,
      -0.0518648848,
      -0.0154007189,
      0.0249481369,
      -0.0555083491,
      0.0230361465,
      -0.0304614473,
      -0.0091600129,
      -0.0469772965,
      8.216255886e-33,
      0.015014777,
      0.0542981327,
      0.0768820196,
      -0.0503452756,
      -0.0274349004,
      -0.0823202357,
      -0.0622050278,
      -0.0471822321,
      -0.033483129,
      0.0155867422,
      -0.1041467115,
      -0.0212780554,
      -0.0070137647,
      0.0293046515,
      0.1391321421,
      -0.0088481186,
      -0.0604610406,
      0.030467093,
      -0.0259916857,
      -0.0514117442,
      0.1492894739,
      0.0242834054,
      -0.0017546507,
      -0.0553854629,
      0.0576522537,
      -0.0167997237,
      0.0557136089,
      -0.0649820939,
      -0.0444850288,
      0.0406399034,
      -0.1348734498,
      0.0001849331,
      -0.08380045,
      0.1163631454,
      -0.0695965216,
      -0.0967697203,
      -0.0787634328,
      -0.0227975771,
      0.0302993879,
      -0.065618135,
      -0.057937067,
      0.0463356152,
      0.0261950158,
      -0.0206504166,
      0.002051265,
      -0.0370509177,
      0.0899153426,
      0.0389915332,
      -0.0129559692,
      0.0511146784,
      0.0322310738,
      -0.0078566605,
      0.0448756181,
      0.0124629578,
      0.0087423744,
      0.0049603139,
      0.1298351288,
      0.0201761331,
      0.0288590137,
      0.0834701136,
      0.0291694105,
      -0.0466602668,
      -0.0189948082,
      0.0617484413,
      0.0138709666,
      0.0935350955,
      -0.0775434151,
      0.0914307386,
      0.0158997681,
      0.0399198048,
      -0.03594644,
      0.028662635,
      0.0630522668,
      -0.0399601161,
      0.087523073,
      -0.1059257537,
      0.0237125531,
      -0.0450959802,
      -0.0340753086,
      -0.0263186619,
      -0.0181582253,
      0.0278354548,
      -0.0969037488,
      -0.0009232075,
      0.0323903039,
      -0.075132899,
      -0.0010504447,
      -0.0397177935,
      0.0105581023,
      -0.0341355391,
      -0.0618738458,
      -0.0015834526,
      0.0156821609,
      -0.0061409133,
      -0.0573588833,
      -8.322781388e-33,
      0.0073768925,
      0.0091830827,
      -0.0181610752,
      0.0858583674,
      0.0327699445,
      -0.07137613,
      0.0098106563,
      -0.0262707863,
      -0.0653418079,
      -0.0447092615,
      0.0087026423,
      0.0511778034,
      0.1107999012,
      0.0621194765,
      0.0690688267,
      -0.0942227915,
      -0.0535341538,
      0.0108024096,
      -0.018473044,
      -0.0060782731,
      0.0240460318,
      -0.0377684683,
      -0.0960636139,
      -0.0433791876,
      0.0060535944,
      0.0539110899,
      -0.0268240087,
      0.0460708551,
      -0.041867353,
      0.024275288,
      -0.0425530523,
      -0.0405451208,
      -0.0693837777,
      -0.0102882702,
      -0.0235640183,
      0.0047302889,
      0.0651581064,
      -0.0424518399,
      -0.101431936,
      0.084709987,
      -0.0048310529,
      0.014552868,
      -0.0130282808,
      0.046604339,
      -0.0626600161,
      0.0509881638,
      -0.0064170272,
      0.0290001053,
      0.0455304794,
      -0.0997936651,
      0.064495571,
      0.0408498086,
      -0.0380234867,
      0.1109727174,
      -0.0070046149,
      0.0089741368,
      -0.0684442073,
      0.0382226333,
      0.0347040966,
      0.0145594943,
      -0.04373293,
      -0.0540525317,
      0.0284744482,
      0.0294104833,
      0.0285059921,
      -0.0755558833,
      0.0336308442,
      0.0714606643,
      -0.029380966,
      0.0416350327,
      -0.0703128725,
      -0.0188066103,
      -0.046395272,
      0.0091748126,
      0.0009561706,
      0.0525451377,
      0.0197196659,
      -0.0014057615,
      0.0330426842,
      -0.0216446929,
      0.0356447846,
      -0.0532404445,
      0.0835870355,
      0.0641414672,
      0.0071253264,
      -0.0341104232,
      0.0193147119,
      0.1008127928,
      0.0261690915,
      -0.0226911604,
      -0.0199366584,
      -0.0124947773,
      0.0273118205,
      0.1626036912,
      -0.0711153746,
      -0.0000000653,
      -0.0648428649,
      0.0420838818,
      0.0083532277,
      0.0654505119,
      0.0364388414,
      -0.0529322475,
      -0.0787867606,
      0.0740693212,
      -0.0009247648,
      0.0247269683,
      0.0512075014,
      0.0133442627,
      -0.0399016477,
      0.0019628904,
      0.0009631079,
      0.0355442278,
      -0.0091241719,
      -0.0141249588,
      -0.0145002212,
      -0.0005978737,
      0.0459350273,
      0.0147037189,
      -0.0551145263,
      -0.0243821088,
      0.0939239785,
      -0.0349630713,
      -0.0975597873,
      0.0810726508,
      0.0291058645,
      0.0450301729,
      0.0023603141,
      -0.0100095952,
      0.0146841016,
      0.0211155079,
      0.0558286123,
      -0.0194286183,
      -0.0729891881,
      0.0505461544,
      -0.0131167667,
      -0.0205420852,
      0.0072541679,
      0.059816204,
      -0.0964105949,
      0.0661034137,
      0.1044919714,
      0.0288225915,
      -0.0337356403,
      -0.0544446744,
      -0.0083564892,
      -0.0472672209,
      -0.042203363,
      0.0175255854,
      -0.0092312414,
      0.0117243277,
      0.0263051018,
      0.0560560562,
      -0.0982488319,
      -0.0212228298,
      -0.0083340211,
      -0.0518872216,
      0.1013280377,
      -0.0309285335,
      -0.0568841919,
      0.0965516791
    ],
    "cluster":6,
    "time":"00:01:40"
  },
  {
    "id":1,
    "url":"https:\/\/arxiv.org\/pdf\/2508.05004",
    "content":"R-Zero: Self-Evolving Reasoning LLM from Zero Data\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nChengsong Huang1,2\u2020,Wenhao Yu1\u2020, Xiaoyang Wang1,Hongming Zhang1, Zongxia Li1,3,\nRuosen Li1,4, Jiaxin Huang2, Haitao Mi1, Dong Yu1\n1Tencent AI Seattle Lab, 2Washington University in St. Louis,\n3University of Maryland, College Park, 4The University of Texas at Dallas\n\u2020 Core contributors\nchengsong@wustl.edu; wenhaowyu@global.tencent.com\nSelf-evolving Large Language Models (LLMs) offer a scalable path toward superintelli-\ngence by autonomously generating, refining, and learning from their own experiences.\nHowever, existing methods for training such models still rely heavily on vast human\ncurated tasks and labels, typically via fine-tuning or reinforcement learning, which poses\na fundamental bottleneck to advancing AI systems toward capabilities beyond human\nintelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous\nframework that generates its own training data from scratch. Starting from a single\nbase LLM, R-Zero initializes two independent models with distinct roles \u2013 a Challenger\nand a Solver. These models are optimized separately and co-evolve through inter-\naction: the Challenger is rewarded for proposing tasks near the edge of the Solver\u2019s\ncapability, and the Solver is rewarded for solving increasingly challenging tasks posed\nby the Challenger. This process yields a targeted, self-improving curriculum without\nany pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning\ncapability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49\non math reasoning benchmarks, and +7.54 on general-domain reasoning benchmarks.\nCode: https:\/\/github.com\/Chengsong-Huang\/R-Zero.\nFigure 1: (Left): R-Zero employs a co-evolutionary loop between Challenger and Solver. (Right):\nR-Zero achieves strong benchmark gains without any pre-existing tasks or human labels.\nSelf-evolving Large Language Models (LLMs) represent a promising frontier for advancing language\nintelligence. By autonomously generating, refining, and learning from their own experiences, these\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nmodels provide a scalable pathway toward artificial superintelligence (Tao et al., 2024; Tan et al.,\n2024a). A critical requirement for training such self-evolving LLMs is access to large volumes of\nexpertly curated tasks and labels, which serve as supervision signals for fine-tuning or reinforcement\nlearning with verifiable rewards (RLVR) (Shao et al., 2024; DeepSeek-AI et al., 2025). However,\nrelying on human annotators to create these tasks and labels is not only costly, labor-intensive,\nand difficult to scale, but also presents a fundamental bottleneck to advancing AI systems toward\ncapabilities that could eventually surpass human intelligence (Su et al., 2025; Zhao et al., 2025a).\nTo reduce dependence on human-curated data, self-generated and label-free methods have been\nproposed to eliminate the need for explicit supervision. In particular, label-free RL derives reward\nsignals directly from the model\u2019s own outputs, such as sequence-level confidence scores (Li et al.,\n2025a; Prabhudesai et al., 2025; Huang et al., 2025) and output entropy (Agarwal et al., 2025; Cheng\net al., 2025). However, despite removing the need for explicit labels, label-free methods still relies on\na pre-existing corpus of tasks, which limits its scalability in truly self-evolving settings. On the other\nside, self-challenging approaches train LLMs on tasks generated by the models themselves (Zhou\net al., 2025a; Wang et al., 2025a; Zhao et al., 2025a), While promising, many of these methods rely on\nexternal code executors to ensure that the synthesized tasks are both feasible and verifiable. However,\nin domains that lack an explicit verification oracle, such as open-ended reasoning, ensuring the\nquality and correctness of self-generated data remains a significant challenge.\nIn this paper, we propose R-Zero, a framework for training reasoning LLMs that can self-evolve\nfrom zero external data. In R-Zero, a single base model is initialized with two roles \u2013 a Challenger\nand a Solver that are independently optimized but co-evolve throughout the RL process. During\nco-evolving, the Challenger is rewarded for generating tasks targeted to be at the edge of Solver\u2019s\ncurrent abilities, while the Solver is rewarded for solving increasingly challenging tasks posed by\nthe Challenger. Framework details are provided in Section 3, but briefly, in the Challenger training\nphase, the Challenger is trained via Group Relative Policy Optimization (GRPO) (Shao et al., 2024)\nto generate difficult questions. The reward signal is derived from the uncertainty for the frozen\nSolver, which is measured by the self-consistency of its multiple generated answers. In the Solver\ntraining phase, the Solver is fine-tuned with GRPO on a filtered set of these challenging questions\ngenerated by the now-frozen Challenger, using the pseudo-labels voted by itself. This entire process\nrepeats, creating a self-evolving cycle that operates without any human intervention.\nOur experiments demonstrate that R-Zero is a model-agnostic framework, consistently and iteratively\nimproving the reasoning abilities of different backbone LLMs. For example, Qwen3-4B-Base model\u2019s\naverage score on math benchmarks increased by a significant +6.49 points after three iterations\nof self-evolution. Moreover, the reasoning skills learned through our math-focused questions can\ngeneralize to complex general-domain tasks, with models trained using R-Zero showing significant\nimprovements on general domain reasoning benchmarks like MMLU-Pro (Wang et al., 2024) and\nSuperGPQA (Du et al., 2025). Our further analysis finds that R-Zero can act as a mid-training method,\nas models first improved by our method achieve higher performance after fine-tuned on labeled data.\nIn addition, we provide an in-depth analysis that validates our framework\u2019s components, demon-\nstrates its synergy with supervised fine-tuning, and characterizes the co-evolutionary dynamics to\nidentify both strengths and limitations, offering insights for future research.\n2 Preliminaries\nOur work builds upon recent advancements in reinforcement learning for fine-tuning large language\nmodels. We briefly review two key methodologies that are relevant to our framework.\n2.1 Group Relative Policy Optimization\nGroup Relative Policy Optimization (GRPO) (Shao et al., 2024) is a reinforcement learning algorithm\nthat fine-tunes a policy LLM \u03c0\u03b8 without a separate, learned value function. Its core idea is to\nnormalize rewards based on the performance of other responses generated from the same prompt.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nFor a given prompt p, a policy LLM \u03c0\u03b8old generates a group of G complete responses {x1, . . . , xG}.\nEach response xi is evaluated to receive a single scalar reward ri. The rewards across the group are\nthen normalized using a z-score to compute a response-level advantage:\nwhere \u03b5norm is a small constant added for numerical stability.\nri \u2212 mean(r1, . . . , rG)\nstd(r1, . . . , rG) + \u03b5norm\nPolicy Update. The policy is updated using a clipped surrogate objective, similar to PPO, to ensure\nstable training. The objective, regularized by a KL-divergence penalty to constrain policy drift, is:\n(cid:16) \u03c0\u03b8 (xi)\n\u02c6Ai, clip(cid:0) \u03c0\u03b8 (xi)\n(xi) , 1 \u2212 \u03f5, 1 + \u03f5(cid:1) \u02c6Ai\n+ \u03b2 KL(cid:0)\u03c0\u03b8\u2225 \u03c0\u03b8old\nMaximizing the negative of this loss encourages the policy to increase the probability of generating\nresponses with positive relative advantages, while the KL term, controlled by \u03b2, limits divergence\nfrom the previous policy.\n2.2 Reinforcement Learning with Verifiable Rewards\nReinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024) is a paradigm for fine-\ntuning models in domains where response quality can be deterministically verified. This approach\nrelies on a rule-based verifier v : X \u2192 {0, 1} that assigns a binary reward to each generation xi:\nif xi satisfies a task-specific correctness check,\nThis reward structure is especially effective for tasks like math, code generation with clear correctness\ncriteria, and serves as the foundation for the reward mechanism in our Solver training.\nWe propose R-Zero, a fully automated framework featuring a Challenger and a Solver, both\ninitialized from the same base LLM. The framework operates in an iterative loop. We illustrate\nthe main framework in Figure 2. First, the Challenger (Q\u03b8) is trained with Group Relative Policy\nOptimization (GRPO) to generate synthetic questions that are challenging for the current Solver\n(Sec. 3.2). A training dataset of question-answer pairs is then constructed from these synthetic\nquestions using a filtering strategy and a majority-vote mechanism (Sec. 3.3). Next, the Solver (S\u03d5)\nis fine-tuned on this new dataset, also using GRPO (Sec. 3.4). This iterative process allows the\nChallenger and Solver to co-evolve, leading to a progressively more capable Solver. The entire\nframework is self-supervised, requiring no human intervention.\n3.2 Challenger Training\nThe Challenger, Q\u03b8, is an autoregressive language model trained to generate challenging questions.\nWe train Q\u03b8 using the GRPO algorithm detailed in Sec. 2. The core of this process lies in designing\na reward function that accurately captures the desired properties of a \u201cgood\u201d question. This\nfinal scalar reward, ri, is then used in the GRPO advantage calculation. We focus on generating\nquestions specifically within the domain of mathematics, as it provides a convenient and self-\ncontained setting for our framework; the objective nature of mathematical answers allows for the\nstraightforward generation of pseudo-labels via majority voting, without the need for external\nverification environments like code executors.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nFigure 2: An overview of our R-Zero framework, which illustrates the co-evolution of the Challenger\nand the Solver. Top: In the Challenger training phase, the Challenger is trained via GRPO to generate\ndifficult questions. The reward signal is derived from the uncertainty for the frozen Solver, which is\nmeasured by the self-consistency of its multiple generated answers. Bottom: In the Solver training\nphase, the Solver is fine-tuned with GRPO on a filtered set of these challenging questions generated\nby the now-frozen Challenger, using the pseudo-labels voted by itself.\nUncertainty Reward. To guide the Challenger toward producing challenging yet solvable ques-\ntions, we first define an uncertainty score. For a generated question x, we query the current Solver\nS\u03d5 for m responses {y1, . . . , ym}. The most frequent response is treated as the pseudo-label \u02dcy(x), and\n1{yj = \u02dcy(x)}. The uncertainty\nwe compute the Solver\u2019s empirical accuracy as \u02c6p(x; S\u03d5) = 1\nreward is then defined as:\nruncertainty(x; \u03d5) = 1 \u2212 2\n(cid:12) \u02c6p(x; S\u03d5) \u2212 1\nThis function incentivizes questions where the Solver is maximally uncertain (accuracy approaches\n50%). We provide a theoretical motivation for this reward function in Sec. 3.5.\nRepetition Penalty. To encourage diversity within a training batch X , we introduce a repetition\npenalty. We could use any similarity metric, but in our case, we specifically use the BLEU score\nfor faster computation, as this calculation must be performed numerous times during the rollout\nprocess. We compute pairwise distances using BLEU score similarity, dij = 1 \u2212 BLEU(xi, xj), and\ngroup questions where dij < \u03c4BLEU into clusters C = {C1, . . . , CK}. The penalty for a question xi in a\ncluster Ck is proportional to its relative size:\nwhere B is the batch size and \u03bb is a scaling factor. In our experiments, we set \u03bb = 1. The implemen-\ntation details are shown in Appendix A.4.\nFormat Check Penalty. A critical first step in the reward pipeline is a structural format check to\nverify that each generated question is correctly enclosed within <question> and <\/question> tags.\nIf the output does not adhere to this required structure, it is immediately assigned a final reward of\n0, and no further reward signals are computed.\nComposite Reward and Policy Update. For all questions that pass the format check, we calculate\na composite reward. The final scalar reward ri for each valid question xi combines signals for\nuncertainty and repetition:\nri = max(cid:0)0, runcertainty(xi; \u03d5) \u2212 rrep(xi)(cid:1)\nWith these rewards {r1, . . . , rG} for a batch of generated questions, we compute the advantage \u02c6Ai\nfor each question and update the Challenger\u2019s policy Q\u03b8 by minimizing the GRPO loss LGRPO(\u03b8).\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\n3.3 Solver Dataset Construction\nAfter updating the Challenger, we use it to generate a new, curated dataset to train the Solver. This\nprocess acts as a curriculum generator. We first sample a large pool of N candidate questions from\nthe Challenger\u2019s policy, xi \u223c Q\u03b8(\u00b7 | p0). For each question, we obtain m answers from the current\nSolver, determine the pseudo-label \u02dcyi via majority vote, and calculate the empirical correctness \u02c6pi.\nA question-answer pair (xi, \u02dcyi) is added to the training set S only if its correctness falls within an\ninformative band, | \u02c6pi \u2212 1\n2 | \u2264 \u03b4. This filtering step discards tasks that are either too easy or too hard.\nWhile the primary goal of this filtering is to discard tasks that are too easy or too hard, it also serves\nas an implicit quality control mechanism. Since our pseudo-labels are derived from a majority vote,\na very low empirical correctness \u02c6pi often indicates that the question itself is ambiguous, ill-posed,\nor that the resulting pseudo-label is unreliable. By filtering out these low-consistency items, our\nmethod simultaneously improves the quality and the uncertainty calibration of the training data.\n3.4 Solver Training\nThe Solver, S\u03d5, is then fine-tuned on the curated dataset of challenging problems S. We also use\nGRPO for this stage, but with a simpler, verifiable reward signal. For a given question xi \u2208 S with\nits pseudo-label \u02dcyi, the Solver generates a batch of answers, each assigned a binary reward rj:\nif xj is identical to the pseudo-label \u02dcyi,\nThis verifiable reward is used to compute the advantage \u02c6Aj, and the Solver\u2019s policy S\u03d5 is subse-\nquently updated by minimizing the GRPO loss LGRPO(\u03d5). This process enhances the Solver\u2019s ability\nto correctly answer the difficult questions generated by its co-evolving Challenger.\n3.5 Theoretical Analysis\nIn this section, we provide a theoretical motivation for our uncertainty reward function, runcertainty\n1 \u2212 2| \u02c6p(x; S\u03d5) \u2212 1\n2 |, which is maximized when the Solver\u2019s success probability, \u02c6p, is 50%. Our\nanalysis is grounded in recent work that formally establishes that the most efficient training occurs\nwhen a learner is exposed to tasks at the frontier of its capabilities (Shi et al., 2025a; Bae et al., 2025).\nThe core insight from these studies is that the learning potential of the current Solver, with policy S\u03d5,\ncan be quantified by the KL divergence to an optimal policy S\u2217. This divergence, DKL(S\u03d5||S\u2217), is\nlower-bounded by the variance of the Solver\u2019s reward. For the binary reward signal used in our\nframework, the success probability is \u02c6p. This leads to the specific lower bound:\nwhere \u03b2 is the temperature parameter controlling entropy regularization. The right-hand side of\nthe inequality, which is proportional to the reward variance, is maximized precisely when \u02c6p = 0.5.\nTherefore, by designing the Challenger\u2019s reward to incentivize questions that push the current\nSolver towards this point of maximum uncertainty, our framework is theoretically motivated to\ngenerate a maximally efficient curriculum in each iteration of the co-evolutionary process.\n4.1 Experiments Setting\nWe employ the Qwen3-4B-Base (Yang et al., 2025) and Qwen3-8B-Base models to assess the impact\nof scale within a single architectural family. Second, to ensure our approach is effective on a distinct\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nlineage, we utilize the OctoThinker-3B and OctoThinker-8B models (Wang et al., 2025b).This choice\nis particularly relevant as Wang et al. (2025b) reported that applying RL training directly to Llama\nmodels yielded suboptimal results. As the OctoThinker series is continually trained from the Llama-\n3.1 models (Dubey et al., 2024), this comprehensive selection allows us to test our framework across\ndifferent foundational architectures \u2013 Qwen vs. Llama.\n4.1.2 Evaluation Benchmark\nWe assess our framework on a comprehensive suite of benchmarks. Although the question-generator\nprompt for our method is primarily focused on mathematical problem-solving, a key objective of\nour evaluation is to explore whether the resulting improvements in reasoning ability can generalize\nto other domains. Therefore, our evaluation is divided into two main categories.\nMathematical Reasoning. We use seven challenging benchmarks: AMC, Minerva (Lewkowycz\net al., 2022), MATH-500 (Hendrycks et al., 2021b), GSM8K (Cobbe et al., 2021), Olympiad-Bench (He\net al., 2024), AIME-2024, and AIME-2025. For these tasks, where answers can be complex, we employ\nGPT-4o as a programmatic judge to semantically verify the correctness of the final answer against\nthe ground truth (Zhao et al., 2025c). For the difficult AMC and AIME benchmarks, we report the\nmean@32 metric. For all other math benchmarks, we report accuracy based on greedy decoding.\nGeneral Domain Reasoning. To test for the generalization of reasoning ability, we evaluate on the\nfollowing challenging benchmarks:\n\u2022 MMLU-Pro (Wang et al., 2024): An enhanced version of the MMLU (Hendrycks et al., 2021a)\nbenchmark, featuring a more challenging suite of multi-task questions designed to provide\na stricter evaluation of language model capabilities.\n\u2022 SuperGPQA (Du et al., 2025): A large-scale benchmark focused on graduate-level reasoning.\nIt comprises questions across 285 distinct disciplines that have been verified as unsearchable\non the web, thereby isolating true reasoning ability from simple knowledge recall.\n\u2022 BBEH (shoaa kazemi et al., 2025): This benchmark builds upon the foundation of BIG-Bench\nHard (Suzgun et al., 2023) by incorporating a new selection of tasks specifically engineered\nto be more difficult, thus providing a more accurate measure of complex reasoning skills.\nFor this category, we follow the experimental setup, prompts, and evaluation codes from (Ma et al.,\n2025), reporting Exact Match (EM) accuracy obtained via greedy decoding.\n4.1.3 Training Details\nOur entire framework is implemented based on the EasyR1 codebase (Zheng et al., 2025b). In\neach iteration of the R-Zero co-evolutionary loop, we follow a specific set of hyperparameters. The\nChallenger (Q\u03b8) first generates a candidate pool of N = 8, 000 questions. To construct the training\ndataset for the Solver, these questions are filtered based on consistency. For each candidate question,\nwe sample m = 10 answers from the current Solver (S\u03d5). A question is retained for the training\nset only if the number of answers matching the majority-vote pseudo-label is between 3 and 7,\ninclusive (\u03b4 = 0.25). This numerical range is consistent with the methodology used in previous\nresearch (Zhang & Zuo, 2025; Li et al., 2025b; Bercovich et al., 2025). When training the Challenger,\nthe uncertainty reward r(x; \u03d5) is calculated by sampling m = 10 responses from the Solver. For\nthe intra-batch repetition penalty, we set the clustering distance threshold to \u03c4BLEU = 0.5. Further\nimplementation details and prompts can be found in Appendix A.\n4.2 Results in Mathematical Reasoning\nThe comprehensive results of our experiments are presented in Table 1. The findings confirm that\nour proposed framework, R-Zero, is a highly effective, model-agnostic method for enhancing the\nperformance of language models on mathematical tasks across different architectures and scales.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nTable 1: Comprehensive results on mathematical reasoning benchmarks. We compare each base\nmodel against a Base Challenger baseline (where the Solver is trained on questions from an un-\ntrained Challenger) and our iterative method, R-Zero. The peak performance achieved during each\nmodel\u2019s training process is highlighted in bold.\nAVG AMC Minerva MATH GSM8K Olympiad AIME25 AIME24\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nOur iterative training process consistently and substantially improves upon the performance of the\nbase models. This holds true for large models like Qwen3-8B-Base, where three iterations of R-Zero\nraise the average performance from a baseline of 49.18 to 54.69, a significant gain of +5.51 points.\nSimilarly, on the smaller OctoThinker-3B, our method improves the average score from 26.64 to\n29.32 (+2.68 points), demonstrating the broad applicability of our self-supervised training loop.\nThis improvement is progressive, with the results showing a clear trend of performance gains across\niterations. For instance, the Qwen3-8B-Base model\u2019s average score climbs from a base performance\nof 49.18 to 53.39 (Iter 1) and ultimately reaches 54.69 (Iter 3). A similar monotonic improvement\nis observed on OctoThinker-3B, which progresses from its base score of 26.64 to 29.32 after three\niterations. This consistent growth underscores the benefits of the co-evolutionary dynamic, where\nthe progressively more capable Solver learns from an increasingly challenging curriculum.\nThe critical role of the Challenger\u2019s RL-based training is validated by the immediate performance\nleap from the Base Challenger to the first iteration of R-Zero. On Qwen3-8B-Base, this first iteration\nprovides a +1.52 point gain over the baseline, and the improvement is even more pronounced\non Qwen3-4B-Base at +3.7 points. This confirms that the intelligent curriculum generated by the\nRL-trained Challenger is significantly more effective than that of a non-trained generator.\n4.3 Results in General Reasoning\nPrevious work has demonstrated that training language models on reasoning-intensive domains,\nsuch as mathematics, can lead to improvements in general-domain capabilities (Huan et al., 2025). A\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nTable 2: Results on general-domain reasoning benchmarks. The table compares the Base Model, a\nBase Challenger baseline, and our iterative R-Zero. The peak performance achieved during each\nmodel\u2019s training process is highlighted in bold.\nOverall AVG MATH AVG SuperGPQA MMLU-Pro BBEH\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nBase Challenger\nR-Zero (Iter 1)\nR-Zero (Iter 2)\nR-Zero (Iter 3)\nkey question, however, is whether this generalization effect still holds when the training curriculum\nis not human-labeled, but entirely self-generated through R-Zero.\nAs shown in Table 2, this transfer of skills is evident across all tested models. For instance, three\niterations of our math-focused training improve the average general-domain score of Qwen3-8B-Base\nby +3.81 points and OctoThinker-3B by +3.65 points. This generalization also extends to the key\nperformance patterns observed in the mathematical results, with progressive iterative gains. This\nconfirms that our method does not merely teach domain-specific knowledge, but enhances the\nmodel\u2019s underlying capabilities in a way that successfully generalizes across domains.\nIn this section, we conduct a series of in-depth analyses to better understand the behavior and\neffectiveness of our R-Zero framework. To ensure consistency, all analytical experiments presented\nhere were conducted on the Qwen3-4B-Base model, unless explicitly stated otherwise.\n5.1 Ablation Study\nTo isolate the contribution of each key component within our R-Zero framework, we conduct\na comprehensive ablation study on the Qwen3-4B-Base model. We specifically investigate the\nimportance of three critical modules by disabling them one at a time and observing the impact on\nperformance. The results are summarized in Table 3.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nAs shown in the table, removing any core com-\nponents leads to a significant degradation in\nperformance. The largest drop occurs when we\ndisable the Challenger\u2019s reinforcement learn-\ning (w\/o RL-Challenger), with the Math and\nGeneral average scores decreasing by 3.7 and\n4.1 points, respectively. This result highlighting\nthe importance of our co-evolutionary curricu-\nlum generation process. Similarly, removing\nthe Repetition Penalty also harms performance,\nindicating that generating a diverse set of ques-\ntions is crucial for effective Solver training.\nTable 3: Ablation study results on the Qwen3-4B-\nBase model. w\/o RL-Challenger: Disables GRPO\ntraining for the Challenger. w\/o Filtering: Dis-\nables the difficulty-based curriculum filtering. w\/o\nRep. Penalty: Removes the repetition penalty\nfrom the Challenger\u2019s reward.\nMath AVG General AVG\n\u22a2 w\/o RL-Challenger\n\u22a2 w\/o Rep. Penalty\n\u22a2 w\/o Filtering\nFinally, disabling the Task Filtering module re-\nsults in a notable performance drop, particularly\non the general-domain average, which falls by over 6 points. As discussed in Section 3.3, this filtering\nserves a dual purpose: it calibrates the curriculum\u2019s difficulty and acts as an implicit quality control\nmechanism by removing questions with low answer consistency. Without this filter, the Solver is\ntrained on a noisy and poorly curated dataset that likely includes ambiguous or ill-posed questions,\nwhich harms its ability to learn robustly.\n5.2 Evolution of Question Difficulty and Data Accuracy\nTable 4: Performance and data accuracy analysis. The highlighted column represents the true\naccuracy of the self-generated pseudo-labels for each question set.\nPerformance of Evaluated Model (vs. Ground Truth)\nSolver (Iter 1)\nSolver (Iter 2)\nSolver (Iter 3)\nPseudo-Label Acc.\nTo understand the co-evolutionary dynamic, we analyzed how the Challenger\u2019s generated questions\nand their corresponding pseudo-labels change across iterations. We sampled 200 questions from the\nChallenger\u2019s policy after each of the first three training iterations, creating three distinct test sets:\nDIter 1, DIter 2, and DIter 3. For this analysis, we assumed the external oracle model, GPT-4o, to be a\nperfect annotator, providing the ground truth answers for all generated questions.\nThe evaluation was conducted as follows: the performance of our internal models was measured\nagainst these GPT-4o ground truth answers. The score reported for GPT-4o itself, however, reflects\nthe true accuracy of our self-generated pseudo-labels by comparing the pseudo label against the\nground truth from the oracle (GPT-4o). The results on the filtered dataset are summarized in Table 4.\nThis analysis reveals a multi-faceted dynamic. The first finding is that the questions generated by\nthe Challenger become progressively more difficult. This is directly evidenced by evaluating a\nfixed model against the evolving question sets. For instance, the performance of the static Solver\n(Iter 1), when measured against the consistent GPT-4o ground truth, drops from 59.0% on Iteration\n1 questions to 47.0% on Iteration 3 questions. This confirms that the Challenger is successfully\nincreasing the intrinsic difficulty of its curriculum. The second finding, revealed by the highlighted\ncolumn, pertains to the true accuracy of the self-generated dataset. Unfortunately, while the\naccuracy of the pseudo-labels is initially high at 79.0%, it systematically drops to 63.0% by the third\niteration. This trend indicates that as the system generates more difficult problems, the Solver\u2019s\nmajority vote becomes a less reliable source for ground truth. This decline in data quality is a critical\ntrade-off and a potential bottleneck for the framework\u2019s ultimate performance.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nFinally, despite this drop in absolute label accuracy, the framework\u2019s internal reward mechanism\nfunctions precisely as designed. The scores on the table\u2019s diagonal show how each Solver performs\non questions from its contemporary Challenger. The Solver (Iter 2) achieves 51.5% and the Solver\n(Iter 3) achieves 50.5% on their respective question sets. This demonstrates that the Challenger\nsuccessfully calibrates the question difficulty to match the Solver\u2019s evolving capabilities, consistently\ntargeting the 50% success rate that our reward function incentivizes.\n5.3 Synergy with Supervised Data\nTo analyze the utility of our framework in scenarios where a labeled dataset is available, we measure\nthe synergy between R-Zero and traditional supervised fine-tuning using labeled datasets1. The\nGRPO settings for this experiment were kept identical to our main experiments.\nWe first establish a supervised baseline by fine-\ntuning the base model directly on the labeled\ndata. For this process, we employ GRPO, an\napproach similar to Zero-RL (Zeng et al., 2025).\nWe then apply our R-Zero framework, where at\nthe end of each co-evolutionary iteration, the\nresulting checkpoint is also fine-tuned on the\nsame labeled dataset. The results show that our\nmethod provides significant additional gains.\nAs highlighted in Figure 3, this represents a gain\nof +2.35 points over the direct training baseline.\nThis finding confirms that R-Zero is not redun-\ndant with labeled data; instead, it acts as a pow-\nerful performance amplifier. The co-evolutionary process enables the model to better leverage the\nsupervised information and achieve performance levels unattainable by standard fine-tuning alone.\nFigure 3: Performance of R-Zero when combined\nwith supervised fine-tuning. The dashed line rep-\nresents the baseline of fine-tuning the base model\non labelled data alone, showing that our iterative\nmethod provides a better initialization.\nIteration Scaling\nPrevious results demonstrate that R-Zero generally enhance the Solver\u2019s capabilities across iterations.\nA closer inspection, however, reveals that the improvement is not consistent, with performance on\ncertain challenging benchmarks degrading in later iterations. This raises a critical question about\nthe long-term stability of our self-improvement loop: what are the limits of this process, and what causes\nthis eventual performance degradation? In this section, we conduct a dedicated analysis to investigate\nthese iteration scaling dynamics, aiming to diagnose the underlying cause of this instability.\n5.4.1 The Inevitability of Collapse: An Empirical Analysis\nAs illustrated in Figure 4, our framework ini-\ntially delivers on its promise, with models of\nall sizes showing significant performance im-\nprovements in the early stages of co-evolution.\nUnfortunately, this virtuous cycle does not con-\ntinue indefinitely. After multiple iterations, we\nobserve a consistent and concerning trend of\nperformance degradation across all models. In-\ntriguingly, we found a direct correlation be-\ntween model scale and resilience to this collapse:\nthe larger the model, the later the onset of per-\nformance degradation.\nFigure 4: Math performance across different iter-\nation times and model scales. The star markers\nindicate the peak performance for each model size.\n1https:\/\/huggingface.co\/datasets\/hiyouga\/math12k\nBase ModelIter 1Iter 2Iter 3Checkpoint Stage44464850Performance (AVG Score)+2.35R-Zero OnlyR-Zero + Human LabelsBaseIter 1Iter 2Iter 3Iter 4Checkpoint Version20304050Performance (AVG Score)Model Size: 0.6BModel Size: 1.7BModel Size: 4B\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nFor instance, the smallest 0.6B model reaches its peak performance as early as the first iteration\n(Iter 1), after which its capabilities begin to decline. In contrast, the largest 4B model sustains its\nupward trajectory for three full iterations, only experiencing a sharp drop at Iter 4. This pattern\nstrongly suggests that while larger model capacity can delay the negative effects, it does not prevent\nthem. This eventual collapse points to an inherent instability or limitation within our current\nself-improvement framework, highlighting a critical area for future investigation.\n5.4.2 Beyond Label Noise: Unpacking the Roots of Instability\nThe most immediate hypothesis for this performance col-\nlapse is the degradation of pseudo-label quality, a po-\ntential failure mode of the self-correction mechanism we\ndiscussed in Section 5.2. As the Challenger generates\nincreasingly difficult problems, it is plausible that the\nSolver\u2019s majority vote becomes a less reliable source for\nground truth, resulting in a noisy training signal that\ncould ultimately harm performance. To empirically test\nthe extent to which this is the primary cause, we sampled\n500 questions from a later training iteration to conduct a\nmore granular investigation into the relationship between\npseudo-label fidelity and the observed performance drop.\nTable 5: Accuracy of self-generated\npseudo-labels (%), labeled by Gemini.\nShaded and bolded values indicate the\nbest checkpoint for each model size.\nAlthough the degradation of pseudo-label accuracy is a consistent trend across iterations, our analy-\nsis suggests this is not the primary, nor even the sole, driver of the eventual performance collapse.\nTable 5 presents the pseudo-label data quality for each model at the onset of its performance collapse.\nIntriguingly, there appears to be no universal accuracy threshold that triggers this degradation.\nFor instance, the 0.6B model begins to decline when data accuracy is still as high as 70.6% (Iter 1),\nwhereas the 4B model tolerates an accuracy as low as 48.8% (Iter 3) before its performance drops.\nThis suggests that the absolute percentage of label noise is not the sole determinant of instability.\nAnother potential, and perhaps more fundamental, reason is a form of model collapse that can be\nintroduced when training exclusively on self-synthesized data (Tan et al., 2024b; Shumailov et al.,\n2024; Dohmatob et al., 2024b; Zhou et al., 2025b; Seddik et al., 2024; Dohmatob et al., 2024a; Briesch\net al., 2023; Zheng et al., 2025a). A model can enter a degenerative feedback loop, suffering from a\nloss of diversity or an amplification of its own biases, which presents a significant challenge.\n5.5 Parameter Sharing Between Challenger and Solver\nTable 6: Comparison of math performance and pseudo-label accuracy between the standard R-Zero\n(two-model) and Single-R-Zero (unified model, shared parameters) frameworks across iterations.\nPseudo-label Acc (%)\nPseudo-label Acc (%)\nTo investigate whether the separation of the Challenger and Solver into two independent models\nis a necessary component for the success of R-Zero, we conduct an ablation study using a unified\nmodel with shared parameters. In this configuration (Single-R-Zero), a single model is tasked with\nperforming both roles, i.e., generating a challenging curriculum and subsequently learning from it.\nThe results, presented in Table 6, clearly indicate that separating the Challenger and Solver into two\nindependent models is crucial for both performance and stability. We observe two key findings. First,\nour standard two-model R-Zero framework not only achieves a higher peak performance (49.07) but\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nalso sustains improvement for more iterations, with its collapse occurring after the third iteration.\nIn contrast, the unified Single-R-Zero model\u2019s performance peaks after the very first iteration and\ndegrades immediately thereafter. Second, the Single-R-Zero model, where the agent must generate\nand solve its own problems, produces pseudo-labels of significantly lower accuracy at every stage.\nFor example, in the first iteration, its pseudo-label accuracy is already substantially lower than the\nR-Zero\u2019s (63.4% vs. 71.0%). We hypothesize that this is because having the problem-setter and solver\noriginate from the same model leads to a form of overconfidence that comes from internal bias.\n6.1 Label-Free Reinforcement Learning\nA significant trend in recent research is Label-Free Reinforcement Learning, which aims to improve\nLLM reasoning without human-annotated data. Many such methods use the model\u2019s own outputs\nas a reward signal. This includes leveraging sequence-level confidence (Li et al., 2025a; Prabhudesai\net al., 2025), the consistency of answers derived from varied reasoning paths (Zhang et al., 2025a;\nZuo et al., 2025; Zhang et al., 2025b), minimizing the output entropy (Agarwal et al., 2025; Cheng\net al., 2025), or even random (Shao et al., 2025) or negative reward (Zhu et al., 2025). These signals\nare often used within self-training loops where models fine-tune on their own most plausible\nsolutions (Shafayat et al., 2025; Zhao et al., 2025b). While these methods all rely on a pre-existing set\nof unlabeled problems, R-Zero removes the need for any seed dataset.\n6.2 Self-Play in Large Language Models\nThe paradigm of self-play, where models take on dual roles to create a self-improvement loop, has\nrecently been adapted to improve language models without human data. This approach has been\nparticularly fruitful in verifiable domains like code generation, where a \u201cCoder\u201d agent\u2019s program is\nverified by a \u201cTester\u201d agent\u2019s unit tests (Lin et al., 2025; Wang et al., 2025a; Pourcel et al., 2025). More\nadvanced frameworks push autonomy further by learning to generate the problems themselves,\ncreating an adaptive curriculum from a small seed of examples or from scratch (Zhao et al., 2025a;\nLi et al., 2025c; Zhou et al., 2025a; Fang et al., 2025). Our work distinguishes itself by extending this\nparadigm to general reasoning domains that lack such verifiable environments, instead learning\nfrom a reward signal derived from the model\u2019s own internal consistency.\n6.3 Reinforcement Learning with Verifiable Rewards (RLVR)\nReinforcement Learning with Verifiable Rewards (RLVR) has been widely adopted as a versatile\nparadigm for enhancing LLMs across a multitude of tasks. Its effectiveness is demonstrated in\ndiverse applications such as relation extraction (Dai et al., 2025), interactive GUI navigation (Shi\net al., 2025b) and search-engine utilization (Jin et al., 2025). While early implementations relied on\nrule-based verifiers, recent work has begun to explore more sophisticated, model-based verifiers (Ma\net al., 2025; Li et al., 2025b; 2024).\n7 Conclusion and Future Work\nIn this paper, we introduced R-Zero, a fully autonomous self-evolving framework that overcomes\ndata dependency by having a Challenger and Solver co-evolve to create a self-generating curriculum.\nOur experiments demonstrate that R-Zero significantly improves LLM\u2019s reasoning capability on\nmultiple domains. Future work could further focus on improving efficiency, exploring more robust\nlabeling techniques, and expanding R-Zero to new domains. It is crucial to note, however, that\nthe core mechanism of R-Zero is currently suited for domains where correctness can be objectively\ndetermined. Extending this self-evolutionary paradigm to open-ended generative tasks, such as\ncreative writing or dialogue, where evaluation is subjective, remains a significant hurdle for future\nresearch. We believe R-Zero is a significant step towards creating truly self-evolving LLMs.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nShivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effective-\nness of entropy minimization in llm reasoning. ArXiv preprint, abs\/2505.15134, 2025.\nSanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, et al. Online difficulty\nfiltering for reasoning oriented reinforcement learning. ArXiv preprint, abs\/2504.03380, 2025.\nAkhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, et al. Llama-nemotron:\nEfficient reasoning models. ArXiv preprint, abs\/2505.00949, 2025.\nMartin Briesch, Dominik Sobania, and Franz Rothlauf. Large language models suffer from their own\noutput: An analysis of the self-consuming training loop. ArXiv preprint, abs\/2311.16822, 2023.\nDaixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, et al. Reasoning with\nexploration: An entropy perspective. ArXiv preprint, abs\/2506.14758, 2025.\nKarl Cobbe, Vineet Kosaraju, Mo Bavarian, Mark Chen, Heewoo Jun, et al. Training verifiers to solve\nmath word problems. ArXiv preprint, abs\/2110.14168, 2021.\nRunpeng Dai, Tong Zheng, Run Yang, and Hongtu Zhu. R1-re: Cross-domain relationship extraction\nwith rlvr. ArXiv preprint, abs\/2507.04642, 2025.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, et al. Deepseek-r1: Incen-\ntivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs\/2501.12948,\nElvis Dohmatob, Yunzhen Feng, Arjun Subramonian, and Julia Kempe. Strong model collapse.\nArXiv preprint, abs\/2410.04840, 2024a.\nElvis Dohmatob, Yunzhen Feng, Pu Yang, Franc\u00b8ois Charton, and Julia Kempe. A tale of tails: Model\ncollapse as a change of scaling laws. In Forty-first International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024, 2024b.\nXinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et al. Supergpqa: Scaling llm\nevaluation across 285 graduate disciplines. ArXiv preprint, abs\/2502.14739, 2025.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, et al.\nThe llama 3 herd of models. ArXiv preprint, abs\/2407.21783, 2024.\nWenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, et al. Serl: Self-play rein-\nforcement learning for large language models with limited data. ArXiv preprint, abs\/2505.20347,\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, et al. Olympiadbench: A\nchallenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific\nproblems. In Annual Meeting of the Association for Computational Linguistics, 2024.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In Proc. of ICLR, 2021a.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, et al. Measuring\nmathematical problem solving with the math dataset. ArXiv preprint, abs\/2103.03874, 2021b.\nMaggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, et al. Does math reasoning\nimprove general llm capabilities? understanding transferability of llm reasoning. 2025.\nChengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time\nscaling via self-calibration. ArXiv preprint, abs\/2503.00031, 2025.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nBowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1:\nTraining llms to reason and leverage search engines with reinforcement learning. ArXiv preprint,\nabs\/2503.09516, 2025.\nNathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, et al.\nT \u00a8ulu 3: Pushing frontiers in open language model post-training. ArXiv preprint, abs\/2411.15124,\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V.\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam\nNeyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with\nlanguage models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and\nA. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural\nInformation Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December\nPengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan V. Oseledets. Confi-\ndence is all you need: Few-shot rl fine-tuning of language models. ArXiv preprint, abs\/2506.06395,\nRuosen Li, Ziming Luo, and Xinya Du. Fg-prm: Fine-grained hallucination detection and mitigation\nin language model mathematical reasoning. ArXiv preprint, abs\/2410.06304, 2024.\nZongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo Yeon Sung, and Jordan Lee\nBoyd-Graber. Semantically-aware rewards for open-ended r1 training in free-form generation.\nArXiv preprint, abs\/2506.15068, 2025b.\nZongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha,\nand Jordan Lee Boyd-Graber. Videohallu: Evaluating and mitigating multi-modal hallucinations\non synthetic video understanding. ArXiv preprint, abs\/2505.01481, 2025c.\nZi Lin, Sheng Shen, Jingbo Shang, Jason Weston, and Yixin Nie. Learning to solve and verify: A\nself-play framework for code and test generation. ArXiv preprint, abs\/2502.14948, 2025.\nXueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, et al. General-reasoner: Advancing\nllm reasoning across all domains. ArXiv preprint, abs\/2505.14652, 2025.\nJulien Pourcel, C\u00b4edric Colas, and Pierre-Yves Oudeyer. Self-improving language models for evolu-\ntionary program synthesis: A case study on arc-agi. 2025.\nMihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, et al. Maximizing\nconfidence alone improves reasoning. ArXiv preprint, abs\/2505.22660, 2025.\nMohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, and M\u00b4erouane Debbah.\nHow bad is training on synthetic data? a statistical analysis of language model collapse. ArXiv\npreprint, abs\/2404.05090, 2024.\nSheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can\nlarge reasoning models self-train? ArXiv preprint, abs\/2505.21444, 2025.\nRulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, et al. Spurious rewards: Rethinking\ntraining signals in rlvr. ArXiv preprint, abs\/2506.10947, 2025.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, et al. Deepseekmath: Pushing the\nlimits of mathematical reasoning in open language models. ArXiv preprint, abs\/2402.03300, 2024.\nTaiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning\nvia adaptive curriculum learning. ArXiv preprint, abs\/2504.05520, 2025a.\nYucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, et al. Mobilegui-rl: Advanc-\ning mobile gui agent through reinforcement learning in online environment. 2025b.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nMehrangiz shoaa kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou,\net al. Big-bench extra hard. In Annual Meeting of the Association for Computational Linguistics, 2025.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai\nmodels collapse when trained on recursively generated data. Nature, 631, 2024.\nYi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, et al. Crossing the reward bridge: Expanding rl\nwith verifiable rewards across diverse domains. ArXiv preprint, abs\/2503.23829, 2025.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench\ntasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and\nNaoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, 2023.\nZhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Man-\nsooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. Large language models for data annotation\nand synthesis: A survey. In Conference on Empirical Methods in Natural Language Processing, 2024a.\nZhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Man-\nsooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. Large language models for data annotation\nand synthesis: A survey. In Proc. of EMNLP, 2024b.\nZhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, et al. A survey on self-evolution\nof large language models. ArXiv preprint, abs\/2404.14387, 2024.\nYinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. Co-evolving llm coder and unit tester\nvia reinforcement learning. ArXiv preprint, abs\/2506.03136, 2025a.\nYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming\nRen, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi\nFan, Xiang Yue, and Wenhu Chen. Mmlu-pro: A more robust and challenging multi-task language\nunderstanding benchmark.\nIn Amir Globersons, Lester Mackey, Danielle Belgrave, Angela\nFan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information\nProcessing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS\n2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024.\nZengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes\nreinforcement learning scaling. ArXiv preprint, abs\/2506.20512, 2025b.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. Qwen3 technical report.\nArXiv preprint, abs\/2505.09388, 2025.\nWeihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, et al. Simplerl-zoo: Investigat-\ning and taming zero reinforcement learning for open base models in the wild. ArXiv preprint,\nabs\/2503.18892, 2025.\nJixiao Zhang and Chunsheng Zuo. Grpo-lead: A difficulty-aware reinforcement learning approach\nfor concise mathematical reasoning in language models. ArXiv preprint, abs\/2504.09696, 2025.\nKongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, et al. Consistent paths lead to\ntruth: Self-rewarding reinforcement learning for llm reasoning. ArXiv preprint, abs\/2506.08745,\nQingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question\nis already half the answer: Fully unsupervised llm reasoning incentivization. ArXiv preprint,\nabs\/2504.05812, 2025b.\nAndrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, et al. Absolute zero: Reinforced self-play\nreasoning with zero data. ArXiv preprint, abs\/2505.03335, 2025a.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nXuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Xiaodong Song. Learning\nto reason without external rewards. ArXiv preprint, abs\/2505.19590, 2025b.\nYulai Zhao, Haolin Liu, Dian Yu, S. Y. Kung, Haitao Mi, and Dong Yu. One token to fool llm-as-a-\njudge. volume abs\/2507.08794, 2025c.\nTong Zheng, Lichang Chen, Simeng Han, R Thomas McCoy, and Heng Huang. Learning to reason\nvia mixture-of-thought for logical reasoning. arXiv preprint arXiv:2505.15817, 2025a.\nYaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, et al. Easyr1: An\nefficient, scalable, multi-modality rl training framework. 2025b.\nYifei Zhou, Sergey Levine, Jason E. Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging\nlanguage model agents. ArXiv preprint, abs\/2506.01716, 2025a.\nYujun Zhou, Jiayi Ye, Zipeng Ling, Yufei Han, Yue Huang, Haomin Zhuang, Zhenwen Liang, Kehan\nGuo, Taicheng Guo, Xiangqi Wang, et al. Dissecting logical reasoning in llms: A fine-grained\nevaluation and supervision study. arXiv preprint arXiv:2506.04810, 2025b.\nXinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, et al. The surprising effective-\nness of negative reinforcement in llm reasoning. ArXiv preprint, abs\/2506.01347, 2025.\nYuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, et al. Ttrl: Test-time reinforcement\nlearning. ArXiv preprint, abs\/2504.16084, 2025.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nA Experiment Details\nA.1 Training Hyperparameter\nThis section summarizes the most critical algorithmic hyperparameters for the Solver and Chal-\nlenger training stages. All experiments were conducted using BFloat16 (BF16) mixed precision and\nFlashAttention 2.\nA.1.1 Solver Training\n\u2022 Global Batch Size: 128\n\u2022 Learning Rate: 1 \u00d7 10\u22126\n\u2022 Weight Decay: 1 \u00d7 10\u22122\n\u2022 KL Penalty Coefficient (\u03bbKL): 1 \u00d7 10\u22122\n\u2022 Max Steps: 15\n\u2022 Number of Rollouts: 5\n\u2022 Rollout Temperature: 1.0\n\u2022 Rollout Top-p: 0.99\nA.1.2 Challenger Training\n\u2022 Global Batch Size: 128\n\u2022 Learning Rate: 1 \u00d7 10\u22126\n\u2022 Weight Decay: 1 \u00d7 10\u22122\n\u2022 KL Penalty Coefficient (\u03bbKL): 1 \u00d7 10\u22122\n\u2022 Number of Rollouts: 4\n\u2022 Rollout Temperature: 1.0\n\u2022 Rollout Top-p: 0.99\nA.2 Prompt Templates\nThis section presents the exact prompt templates used for the solver and challenger models.\nSolver Prompt Template\nSystem Message:\nPlease reason step by step, and put your final answer within \\boxed{}.\n{problem statement}\nNote: {problem statement} is a placeholder for the actual math problem.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\nChallenger Prompt Template\nSystem Message:\nYou are an expert competition-math problem setter. FIRST, in your private scratch-pad, think\nstep-by-step to design a brand-new, non-trivial problem. The problem could come from\nany field of mathematics, including but not limited to algebra, geometry, number theory,\ncombinatorics, prealgebra, probability, statistics, and calculus. Aim for a difficulty such that\nfewer than 30% of advanced high-school students could solve it. Avoid re-using textbook\nclich\u00b4es or famous contest problems.\nTHEN, without revealing any of your private thoughts, output exactly the following two\n{The full problem statement on one or more lines}\n\\boxed{final answer}\nDo NOT output anything else\u2014no explanations, no extra markup.\nGenerate one new, challenging reasoning question now. Remember to format the output\nexactly as instructed.\nA.3 GPT-4o Judge Prompt\nTo programmatically evaluate the correctness of answers on mathematical benchmarks where the\nfinal answer can be complex (e.g., simplified expressions), we use GPT-4o as a judge. The exact\nprompt and configuration used for this evaluation are detailed below.\nConfiguration for GPT-4o as Judge\n\u2022 Model: gpt-4o\n\u2022 Temperature: 0.1\nSystem Message:\nYou are a math answer checker.\nUser Message Template:\nHi, there is an answer: {answer},\nand the ground truth answer is: {response},\nplease check whether the answer is correct or not, and return the **only**\nNote: {answer} is a placeholder for the model-generated solution, and {response} is the ground-\ntruth answer from the benchmark.\nA.4 Repetition Penalty Implementation\nTo encourage the Challenger to generate a diverse set of questions within each batch, we apply\na repetition penalty, rrep. This penalty is designed to disincentivize the model from producing\nsemantically similar questions in the same batch. The implementation is a multi-step process based\non clustering questions by their BLEU score similarity.\nR-Zero: Self-Evolving Reasoning LLM from Zero Data\n1. Pairwise Distance Calculation via BLEU Score First, we compute a pairwise distance matrix\nfor all questions in a batch. The distance dij between any two questions, xi and xj, is defined as one\nminus their BLEU score:\ndij = 1 \u2212 BLEU(xi, xj)\nFor this calculation, we specifically use the sentence bleu function from the NLTK library\n(nltk.translate.bleu score). To ensure numerical stability, especially for shorter questions with\nlimited n-gram overlap, we employ its first smoothing function, SmoothingFunction().method1.\nThe questions are tokenized for the BLEU calculation by splitting on whitespace; no further text\nnormalization, such as lowercasing or punctuation removal, is performed.\n2. Agglomerative Clustering With the pairwise distance matrix computed, we then group similar\nquestions using agglomerative hierarchical clustering. This step is performed using the Clustering\nimplementation from the scikit-learn library. The clustering algorithm is configured with the\nfollowing key parameters:\n\u2022 Metric: Set to \u2019precomputed\u2019, indicating that we provide our custom BLEU-based distance\nmatrix instead of having the algorithm compute distances.\n\u2022 Linkage: Set to \u2019average\u2019. This method defines the distance between two clusters as the\naverage of the distances between all pairs of questions across the two clusters.\n3. Final Penalty Calculation Once each question in the batch is assigned to a cluster, the repetition\npenalty rrep(xi) for a given question xi is determined by the relative size of the cluster Ck to which it\nbelongs. The penalty is calculated as:\nHere, |Ck| represents the number of questions in cluster Ck, and B is the total number of questions in\nthe batch (i.e., the batch size).",
    "embedding":[
      -0.0190636832,
      -0.1015641764,
      -0.0033665891,
      0.0542734489,
      0.0868564174,
      -0.0147903627,
      -0.0260430798,
      0.0291145518,
      -0.0011866381,
      -0.0216415152,
      -0.0025517298,
      -0.0807507932,
      0.0194120277,
      0.0514813587,
      0.0218730923,
      0.0573648028,
      0.0738706514,
      -0.0266214591,
      -0.0921445042,
      -0.1035084724,
      0.0042285398,
      0.0462384,
      0.0686654672,
      0.0058489977,
      0.0085719144,
      -0.0088346899,
      0.0436881669,
      -0.0091582667,
      0.0519323833,
      -0.003482783,
      0.1096448302,
      0.0111652175,
      -0.0054730796,
      0.0224216208,
      -0.0216185842,
      0.0537740998,
      -0.044322148,
      0.0748390928,
      0.0347459726,
      -0.027205212,
      0.0030232591,
      -0.0101353787,
      0.0468467362,
      -0.0392274968,
      0.1103146374,
      -0.0055680317,
      -0.0791802853,
      -0.002322074,
      0.0241442882,
      0.0139901703,
      -0.0872269273,
      -0.0304989126,
      -0.0061895628,
      0.0633152276,
      0.0092716245,
      0.006213285,
      0.0090563735,
      0.0295828003,
      -0.0495347083,
      -0.066839464,
      -0.0173069015,
      -0.1155315489,
      0.030505795,
      -0.0066518034,
      -0.0537789837,
      0.0496376343,
      -0.0143105602,
      0.0844524801,
      0.0108997039,
      0.051123146,
      0.0200588796,
      0.0533568636,
      -0.0196709074,
      0.039151825,
      -0.0019269896,
      0.0268816315,
      0.0115738632,
      0.0089728618,
      0.1042676419,
      0.0129943322,
      -0.033084441,
      0.0166755859,
      0.0171654224,
      0.0241010394,
      0.0085701263,
      0.0517142378,
      0.0626580417,
      0.0564097241,
      0.0562761575,
      0.0291612111,
      -0.0250071641,
      -0.0310244374,
      -0.0173539873,
      0.0122773889,
      0.0086324615,
      0.0737562776,
      0.0091192741,
      -0.0587639213,
      -0.0753446668,
      0.0395685025,
      0.0154484576,
      0.0679609925,
      0.0011620477,
      0.0098992465,
      0.0049210056,
      -0.0022736967,
      -0.0334356651,
      0.0271688439,
      0.0350472778,
      -0.1081438512,
      0.0208471194,
      0.0009086161,
      0.0096030058,
      0.0138531644,
      -0.0335393883,
      -0.0226576049,
      0.0090162223,
      -0.0191037282,
      -0.0426970795,
      0.0353469588,
      0.0175238512,
      0.0291966386,
      -0.0462973863,
      0.0603981987,
      -0.0112154707,
      -0.0265366025,
      -0.0979885086,
      3.59665529e-33,
      0.0557390675,
      0.1060489118,
      0.0490622967,
      0.0077969898,
      0.0026407242,
      -0.0480944365,
      0.0511557013,
      0.0709014982,
      -0.0382333137,
      0.025570428,
      -0.033659514,
      0.0453783087,
      -0.0674105063,
      -0.0085372953,
      0.0314905234,
      0.0122091556,
      -0.0444566645,
      0.0044835065,
      -0.0103441793,
      -0.0676537305,
      0.0461587198,
      0.0261094011,
      0.0129860733,
      -0.0737357885,
      0.0684819445,
      0.0350042842,
      0.1188188121,
      -0.0847120434,
      -0.0599570982,
      -0.005245558,
      -0.0963011086,
      0.0025985744,
      -0.0204458553,
      0.0912230313,
      -0.0071577835,
      -0.0624494813,
      -0.0411112048,
      -0.0055275047,
      0.0548183508,
      0.0170420725,
      0.0305072553,
      0.0570546538,
      0.0146723306,
      -0.002351657,
      -0.0658495724,
      -0.086984247,
      0.0237234496,
      0.0569343045,
      -0.0436372049,
      -0.0210255329,
      0.0083530881,
      0.024888901,
      -0.0664150119,
      -0.024189841,
      -0.0340018272,
      -0.0279024038,
      -0.0487273932,
      0.0882975608,
      0.0204704385,
      -0.0315910392,
      0.0400130227,
      -0.0246726386,
      -0.0478242561,
      0.1380029023,
      0.0562593602,
      0.0228953231,
      -0.0458894148,
      0.0274019409,
      0.1110161841,
      -0.0108678164,
      0.0435419157,
      -0.0437947325,
      0.0131716812,
      -0.0564096421,
      0.0783152506,
      -0.0312583596,
      0.0778673366,
      -0.1410298347,
      0.0401534364,
      0.0289363712,
      0.0164991654,
      0.0527780987,
      -0.0385259986,
      -0.0316257104,
      -0.0134879323,
      -0.0971415341,
      0.0573807657,
      -0.0463734642,
      0.0066382596,
      -0.0591969863,
      -0.0368293822,
      -0.0845552087,
      0.0082958099,
      -0.0205167904,
      -0.0614531226,
      -2.53610996e-33,
      -0.0256204177,
      -0.028412912,
      -0.1004790142,
      0.0467492156,
      -0.014777475,
      -0.0585126653,
      0.0247292742,
      0.0569259748,
      -0.0245615486,
      -0.0481491648,
      -0.0811551958,
      -0.0336650051,
      0.0994476154,
      0.0099078203,
      0.0138821807,
      -0.0675367489,
      0.0112104025,
      0.0589936711,
      -0.0288364887,
      0.095138967,
      -0.0147916116,
      0.1082354337,
      -0.1475174427,
      0.0059308801,
      0.0040301331,
      0.0531443469,
      0.0126317097,
      0.110484831,
      -0.025168255,
      0.0718809888,
      -0.0432105288,
      0.0250101704,
      -0.0165787227,
      -0.0366794206,
      -0.0543694571,
      0.0906012729,
      0.0343845934,
      -0.0162598751,
      -0.0053543304,
      0.0427682437,
      0.0019410094,
      -0.027304437,
      -0.0901245773,
      0.012949151,
      -0.0215400346,
      -0.0673176497,
      -0.1378785521,
      0.0272109434,
      0.0053646625,
      0.0038738525,
      -0.0522174351,
      0.0103248712,
      -0.0401019715,
      -0.0319389924,
      -0.025543265,
      -0.0694296062,
      -0.0118291331,
      -0.0170212705,
      -0.0328608043,
      -0.0039993408,
      -0.0955892801,
      -0.0130512426,
      0.0556471199,
      -0.0204799566,
      -0.0402457789,
      -0.0074531673,
      -0.0352722444,
      0.002483357,
      -0.0296968352,
      -0.0215904899,
      0.0896771252,
      -0.0040001506,
      -0.0052563339,
      -0.0490909591,
      -0.0051488979,
      -0.0072431788,
      -0.0878708065,
      -0.0318495706,
      -0.0246267579,
      -0.0775757954,
      0.0289228093,
      0.0298407786,
      0.0380242504,
      0.1201459542,
      0.0748810098,
      -0.0320270285,
      0.0467866994,
      0.1272138953,
      0.0374464169,
      0.0321648642,
      -0.011664968,
      -0.0142351286,
      -0.0818060264,
      0.0971979424,
      -0.058769539,
      -0.0000000468,
      -0.0506617688,
      0.0164628346,
      0.0334849358,
      0.137554273,
      0.0605490357,
      0.0112627717,
      -0.0448157303,
      0.0147437984,
      0.0140498057,
      0.0001743554,
      0.0311677195,
      0.0025177286,
      -0.0461616851,
      -0.0538275205,
      0.0194472335,
      0.0933831483,
      0.0320534185,
      -0.0083973417,
      -0.0199789666,
      0.0014870052,
      0.0133506693,
      0.0142220678,
      -0.0485285223,
      -0.0493894666,
      0.076457262,
      -0.1040690839,
      -0.0600693375,
      0.0310364179,
      0.0477488674,
      -0.0064357864,
      -0.0627188906,
      -0.0359487124,
      0.0384164713,
      0.000075149,
      0.0814863592,
      0.0389799103,
      0.0209407173,
      0.0248178486,
      -0.0206719339,
      -0.0974864587,
      0.001580109,
      0.0893604755,
      -0.0799072385,
      0.0054654819,
      0.0041681146,
      -0.0747840926,
      -0.0598544143,
      -0.1084898412,
      0.0420201533,
      -0.0170365684,
      0.0169792995,
      -0.0192194469,
      -0.062959455,
      0.0015200429,
      0.1059327424,
      0.0319727398,
      -0.0342591405,
      0.0537942499,
      -0.0044661378,
      0.0775939748,
      0.0284896046,
      0.0617262796,
      -0.0050389459,
      -0.0243832078
    ],
    "cluster":7,
    "time":"00:01:40"
  },
  {
    "id":2,
    "url":"https:\/\/interactivetextbooks.tudelft.nl\/showthephysics\/Introduction\/About.html",
    "content":"1. Show the Physics \u2014 ShowingPhysics\nSkip to main content\n1. Show the Physics\n3. Preface from the editor\n4. About the authors\n6. Python summary\n7. Table of contents\n8. Introduction\n9. Nature of Science\n10. Argumentation\n11. Thinking-Back-and-Forth\n12. Predict Explain Observe Explain\nDemos on Nature of Science\n13. Demos on NoS\n13.1. The returning can\n13.2. Introducing a particle model to explain the coloring of eggs\n13.3. Crookes\u2019 radiometer\n13.4. Cylinder Puzzle\n13.5. Magic, trick or physics?\n13.6. Roll and Stop\n13.7. Mysterious fountain\n13.8. Magic Memory Balls\nDemos on scientific inquiry\n14. Demos on scientific inquiry\n14.1. Can it be air?\n14.2. Does Van de Graaff not like a shower?\n14.3. Why does the water rise?\n14.4. Boyle\u2019s law\n14.5. Investigating a devastating Flame\n14.6. A battle between two bottles\n14.7. Flamecooler\n14.8. Trapped candles\n14.9. Speed of light in a liquid\n14.10. Powerful Balloon\n14.11. Determination of g\n14.12. On a roll\nDemos for conceptual development\n15. Waves, optics, quantum\n15.1. Light in darkness\n15.2. Reflection\n15.3. Making Einstein young again\n15.4. Surprising infra-red investigations\n15.5. LEDs and photons\n15.6. Writing with a laser using phosphorescence\n15.7. Shadow of a flame 1\n15.8. Shadow of a flame 2\n15.9. Standing wave with an electric toothbrush\n15.10. Physics of the panflute\n15.11. Ranking the \u2018stars\u2019\n15.12. Singing rod\n15.13. Bouncing ball in a tube\n15.14. Fluorescent olive oil\n15.15. Hot spots in the microwave\n15.16. Strange shadows\n15.17. Cold radiation\n15.18. Captured laser beam\n16. Electricity, magnetism\n16.1. Merry-go-around of floating candles\n16.2. Blowing out a light bulb\n16.3. Measuring charge\n16.4. Alternating voltage across a lamp\n16.5. Lorentz force on charged particles\n16.6. Slow bulbs\n16.7. Induction tubes\n16.8. High voltage\n17.1. Newton\u2019s First Law\n17.2. Falling faster than g\n17.3. Rotational inertia\n17.4. Tug-of-War\n17.5. Stick-slip motion\n17.6. Self-propelling car\n17.7. Does the magnet challenge gravity?\n17.8. Who dares?\n17.9. Dropping balls and bottles\n17.10. Feeling physics with your own body!\n17.11. Upward and downward force\n17.12. Twice as much isn\u2019t twice as big\n17.13. Floating pearls\n17.14. Tension in a pendulum\n17.15. Rotating balloon\n17.16. Colliding with Newton\u2019s third law\n17.17. Up and down the hill\n17.18. Rotating cubes\n18. Thermodynamics\n18.1. Making a perfect cappuccino\n18.2. Playing with density\n18.3. Ocean currents\n18.4. Rich boiling phenomena\n18.5. Boiling by cooling\n18.6. Miscommunicating vessels\n18.7. Cool cans\n18.8. Condensation heat in infrared\n18.9. Cooling metal spheres\n18.10. Black and white\n19.1. Extinguishing without water\n19.2. Air is not nothing: weighing air\n19.3. Two springs, series or parallel?\n19.4. Force and motion with a bowling ball\n19.5. Upward dripping\n19.6. Curve ball\n19.7. Colliding balls\n19.8. A balloon that can do everything\n19.9. Transit of a planet\n19.10. A light bulb as hot as the sun?\n19.12. Gauss\u2019 rifle\n19.13. Falling spring\n19.14. Radioactive decay - A simulation\nDemos for special occasions\n20. Demos for special occasions\n20.1. Spectacular spectrum\n20.2. Making a slanting tower stacking blocks\n20.3. Magic with pendulums\n20.4. Falling candle\n20.5. Boiling without heating in a vacuum\n20.6. Stick on wine glass\n20.7. Cloud formation\n20.8. Optics with LEDs\n20.9. Spinning circles in a wine glass\n20.10. Pulling a spool\n20.11. Implosion\n21. Introduction\n23. Electricity\n25. Oscillations and waves\n27. Liquids and air\n28. Magnetism, electro-magnetic induction\n29. Heat and temperature\n30. Earth science\n31. Modern Physics\nShow the Physics\n1.1. About this book\n1.2. How to use the book\n1.4. Your contribution?\n1.5. Two hundred backpocket demos\n1.7. References\n1.8. Last updated\nShow the Physics\nThis book presents a selection of the 99 best, most beautiful physics demonstrations from the Dutch book series \u201cShow\nas published by the\nDutch Association for Science Education\n. The three original volumes are cherished in the Netherlands by physics teachers and are used as a useful resource for many lectures. We hope that this translated and updated open access book will be just as valuable to a wider range of physics teachers. Where most physics demonstration books cover a large range of demonstrations, we included  strategies to make the most of these - making the demonstrations both magical and educational! Moreover, we included videos and opportunities to live code and use python simulations without the need for any software installation.\nOne of the updated demonstrations:\nFluorescent olive oil\nThe demonstrations are structured using the four categories:\nDemonstrations on\nnature of science\nDemonstrations on\nscientific inquiry\nDemonstrations for\nconceptual development\nDemonstrations for\nspecial occasions\nSo, whether you want to deepen students\u2019 understanding of a specific topic, want to engage them in thorough thinking, or if you were asked to demonstrate physics on a festive occasion, you can find demonstrations and inspiration in this book.\nYour name can be amongst the list of contributors. If you have suggestions, you can open an issue using the git button\nat the top right of this page.\nAbout this book\nThis book originated in the collaboration between physics teachers and physics teacher educators, united in the\nDutch Association for Science Education(NVON)\n. The three Dutch volumes of Show\nFysica cover over a 200 demonstrations for secondary school physics, along with additional text on the pedagogy of physics demonstrations. Each demonstration has been tested by various secondary school physics teachers and their technicians. The process of developing, writing, and checking ensures that all demonstrations are effective and accessible.\ntwo heated metal balls\n. Which one cools down fastest, the big or the small ball?\nAre our demonstrations original? Don\u2019t expect them to be so. Most demonstrations were inspired by earlier versions of the experiments - where possible referenced to earlier descriptions - and edited by the\n. For several demonstration we included questions to check students\u2019 understanding of the phenomenon at hand!\nThis book is published under the CC-BY-NC, what means that you are allowed to use and adapt the materials for non-commercial use and under the condition that you refer to the original work.\nHow to use the book\nDemonstrations are fantastic. They offer so many wonderful possibilities to show the beauty of physics and amaze pupils, that we really should do at least one in every physics class. That may not always be feasible, but the objection \u201cwhich demo then?\u201d no longer applies. After all, this book provides 99 examples!\nSurely you can read this book from cover to cover, but that was not our intention. Our main idea was to help provide teachers with ample demonstrations that could be readily used in the classroom, thereby helping students to grasp physics. Therefore, we choose and developed the demonstrations that really add value to the physics lessons. Hence, rather than reading the whole book, you can use the search function of the book and look for the topic that you are teaching next week to find a demonstration that suits well.\nThe different chapters on the pedagogy of physics demonstrations provide an excellent start as well!\nWhy do soap bells behave strangely in the surrounding of a\nWe have carefully conceived, developed, described and tested each of the experiments. Moreover, we specified safety issues where necessary. However, we are not responsible for whatever accidents, or injuries when doing a demonstration described in this book. We want to emphasize as well the requirement to test a demonstration before doing it with public.\nIn this online book you will find Python coding cells that you can run in your browser without the need for any additional materials. Our colleagues from\ncontributed to this feature.\nEven on this page we have made a code-cell that you can run and play with. The start of running a code-cell is always to click the\nat the top of the page. After loading the packages (\n) you are good to go. Go to the cell and click run!\nSome contain more advanced code, we try to keep it as simple as possible. Below we have executable code. Its output is a plot. You can run the cell, see the output, and alter anything inside to see how it functions\u2026\n# import libraries\nmatplotlib.pyplot\n# some values for x\n# some values for y\n# plot data points as black (k) dots (.)\n# plot data points as red (r) dottedline (--)\nDo you need more explanation? See our chapter on\nYour contribution?\nAn absolute advantages of Jupyter Online Books is the ability to have readers contribute. If you see a typo, if something is not clear, or anything needs to be adjusted, click the\nbutton at the top of the screen and open an issue (\n) (you need to have a github account though).\nTwo hundred backpocket demos\nInterested in smaller demo\u2019s? We now have\n200 backpocket demos available\nThis book is licensed under a Creative Commons Attribution 4.0 International License (CC BY) NC, except the following.\nCC BY NC conditions are not applicable to the figures in the\nIneke Frederik, Ed\u00a0van\u00a0den Berg, Peter Dekkers, Wim Sonneveld, Wouter Spaan, Freek Pols, Kirsten Stadermann, Karel Langendonck, and Norbert van Veen.\nShowdeFysica 3: natuurkunde laat je zien!\nIneke Frederik, Ed\u00a0van\u00a0den Berg, Leo te\u00a0Brinke, Peter Dekkers, Freek Pols, Wim Sonneveld, Wouter Spaan, Norbert van Veen, and Maarten van Woerkom.\nShowdeFysica 2: natuurkunde laat je zien!\nIneke Frederik, Ed\u00a0van\u00a0den Berg, Leo te\u00a0Brinke, Peter Dekkers, Wim Sonneveld, Wouter Spaan, and Maarten van Woerkom.\nShowdeFysica: natuurkunde laat je zien\n25 juli 2025, including 200 pocket demos\n1.1. About this book\n1.2. How to use the book\n1.4. Your contribution?\n1.5. Two hundred backpocket demos\n1.7. References\n1.8. Last updated\nBy Freek Pols & Peter Dekkers (ed.)\n\u00a9 Copyright Delft University of Technology & NVON, CC BY NC 4.0.",
    "embedding":[
      -0.0736161843,
      0.0101216808,
      0.0449103452,
      0.0473808311,
      0.0577392168,
      -0.0016228224,
      -0.0141561842,
      0.0012601558,
      -0.0078376336,
      0.0376405604,
      0.0023990027,
      -0.0507023484,
      -0.0774080381,
      0.0257897191,
      0.0073048715,
      -0.0367565975,
      -0.0150852436,
      -0.0090832198,
      -0.0407341421,
      0.0032683201,
      0.0504305996,
      -0.0375036672,
      0.0990292728,
      0.0532903709,
      -0.0224870853,
      -0.0102126868,
      -0.04327612,
      0.0164934583,
      0.0195241291,
      -0.041441083,
      0.0281032193,
      0.0908511207,
      -0.0413450785,
      -0.0501119979,
      0.0614184365,
      -0.0088816341,
      0.0561986603,
      0.0197561961,
      0.057567507,
      0.0561398119,
      -0.0502536185,
      -0.0868407562,
      0.0126680788,
      0.0535100661,
      0.007241243,
      0.0237411093,
      0.0602975711,
      -0.0532088839,
      -0.033119861,
      0.0017206128,
      -0.0884534046,
      -0.0253248848,
      -0.0844581947,
      0.0194893032,
      0.02426471,
      -0.0299891084,
      -0.0070501692,
      -0.0471768416,
      0.0531978346,
      -0.119162716,
      0.0264455304,
      0.0320717841,
      -0.1072980091,
      0.0689674318,
      -0.0251521245,
      -0.0137960017,
      -0.0026235573,
      0.0563137718,
      0.0554336533,
      -0.0085670017,
      0.0514336191,
      0.0602131411,
      0.0206689369,
      -0.0047426177,
      0.0114023192,
      -0.0060235453,
      -0.0390632413,
      0.0124324663,
      -0.0236330833,
      -0.0177500881,
      -0.0060220994,
      -0.0852326229,
      -0.0770227686,
      0.0313612036,
      -0.0892845094,
      -0.0330872349,
      0.063238278,
      0.0346679799,
      -0.0529407859,
      -0.0165634621,
      0.0050084866,
      -0.0532945693,
      -0.0260843281,
      0.0258349609,
      0.0434609838,
      0.0906060413,
      0.0461586192,
      -0.0950688571,
      0.0417568274,
      0.0404250324,
      0.0568174347,
      0.08465565,
      0.002133552,
      0.0672849938,
      0.0161257274,
      -0.1264126897,
      -0.0021758843,
      -0.0669548139,
      0.0899551958,
      0.0256404858,
      0.0426720195,
      -0.0511083193,
      0.0061168075,
      0.0177128967,
      0.0109015247,
      -0.025648579,
      0.0379748158,
      -0.0111444658,
      -0.0741428211,
      0.0138563719,
      0.0551304482,
      0.0345776081,
      0.0210868381,
      0.0053253924,
      0.0645913482,
      0.0094905505,
      -0.0449609682,
      3.314568972e-33,
      0.0441564769,
      -0.0179627836,
      0.0531610958,
      0.0631775483,
      0.0714731216,
      -0.0102649191,
      -0.0124011636,
      -0.0019602997,
      0.020979479,
      0.0826026872,
      -0.0507383756,
      0.0681067556,
      -0.0132744322,
      0.0177324973,
      -0.0694114566,
      0.0022876242,
      -0.0144608282,
      0.0141520258,
      -0.0404969044,
      -0.0241564307,
      -0.0239494294,
      -0.0240981653,
      0.0226554144,
      -0.0817140192,
      -0.0265400112,
      0.0679537803,
      -0.0419800766,
      0.0290013887,
      -0.0137262763,
      0.0173281897,
      -0.0363819934,
      -0.0247523692,
      -0.0234069545,
      0.0829880908,
      -0.0512228049,
      -0.0367896594,
      0.0199117623,
      -0.1158789992,
      0.0120201241,
      -0.0560080186,
      -0.0531325601,
      0.0367625318,
      0.0163292624,
      -0.0237978976,
      0.0171141978,
      0.0168274902,
      0.009683975,
      0.0066962275,
      -0.0137664648,
      0.0352996662,
      0.0282265376,
      0.0142057156,
      0.0555676855,
      -0.0348745212,
      0.07461337,
      0.0266256761,
      -0.0292603038,
      0.0049451045,
      -0.0147914505,
      0.0102509623,
      0.0396197326,
      0.1112704575,
      0.0314324573,
      -0.0105965259,
      -0.0280942135,
      0.0560140945,
      -0.1173918545,
      0.0207350161,
      0.0496448688,
      -0.0359794423,
      -0.0959871411,
      0.0506884977,
      -0.0920951515,
      -0.0467551388,
      0.0888288245,
      -0.0000962162,
      0.0291677415,
      -0.0607363358,
      0.0031125918,
      0.0062109185,
      0.0231173635,
      -0.0831717104,
      -0.0492882282,
      -0.0560313389,
      -0.1407920867,
      -0.0819943547,
      0.0205846764,
      -0.0169093218,
      -0.0139525607,
      -0.0397201143,
      -0.0703683645,
      -0.0099687651,
      0.1062947661,
      -0.0330994353,
      -0.0125338295,
      -3.040387853e-33,
      -0.0163862742,
      -0.0612399578,
      -0.1210275143,
      0.0748577341,
      0.0815917552,
      0.0098518617,
      -0.0513775684,
      -0.0622882806,
      0.0372571461,
      -0.0767896175,
      -0.0258913767,
      -0.0097025326,
      0.0102693914,
      0.0211998504,
      -0.0171414781,
      0.0035711345,
      -0.0316264667,
      -0.0211317092,
      0.0353599414,
      0.0796689168,
      -0.0232557468,
      0.1040563732,
      -0.1358288825,
      -0.066326648,
      -0.046791058,
      0.0771136433,
      0.1161877513,
      -0.0674800128,
      0.0196245424,
      0.0236353669,
      -0.0285709277,
      -0.0203818101,
      0.0434078015,
      0.0193324536,
      -0.0099506136,
      0.0297898147,
      0.1538418233,
      -0.039785888,
      -0.0212708265,
      -0.091215238,
      0.0781725794,
      0.0178454909,
      0.03749099,
      -0.047227785,
      -0.035657879,
      0.0275401119,
      -0.0104475273,
      0.0337281562,
      -0.0091237696,
      0.0495813228,
      0.0124633005,
      -0.0449374355,
      -0.0412018225,
      -0.0110181756,
      0.0204148013,
      0.0196967702,
      0.0178057496,
      0.0187577493,
      -0.019436907,
      0.0544305667,
      0.0210268702,
      0.03476917,
      -0.0276479907,
      0.059879154,
      -0.0157135073,
      0.01163253,
      -0.0963098034,
      0.1038646027,
      0.0380167738,
      -0.0913587734,
      -0.026934823,
      0.0077770664,
      0.0032614355,
      -0.0329392292,
      0.0935564786,
      0.0926742777,
      0.0357039273,
      -0.0726120844,
      0.0281386338,
      -0.03665407,
      -0.0416114628,
      0.0539175868,
      -0.0137936492,
      0.037369512,
      -0.0010137412,
      -0.0507528447,
      -0.0317573957,
      -0.0191741679,
      -0.0403574444,
      -0.0047882516,
      0.0249418132,
      0.0119475983,
      0.0359908454,
      0.0896885544,
      0.0748463348,
      -0.0000000738,
      -0.0025729223,
      0.0374029428,
      0.0841153562,
      0.0240158997,
      0.0446622968,
      0.0466442145,
      0.0590371042,
      0.0200629402,
      -0.0408425406,
      -0.009871169,
      0.0602861047,
      0.0502423234,
      0.0089892009,
      0.0679619312,
      0.0199562013,
      0.0228508022,
      -0.0260540899,
      -0.0724521503,
      -0.0901469216,
      -0.0070200879,
      0.0579524972,
      0.0229419712,
      0.0167890135,
      -0.0679261461,
      -0.0159619004,
      0.0640484616,
      -0.0324690714,
      0.0304052252,
      0.0183350556,
      -0.0134431003,
      0.0231571272,
      -0.0253978521,
      -0.0632064119,
      -0.0634904131,
      0.0038980173,
      0.0029326933,
      -0.0483099967,
      -0.0070757112,
      0.0259929225,
      0.0042202338,
      -0.0549912527,
      -0.0310968347,
      -0.1043609679,
      -0.0257937759,
      0.0093167312,
      -0.0367029011,
      -0.0747754127,
      -0.0500967838,
      -0.0854603946,
      0.1494201124,
      -0.0141787026,
      -0.0445151106,
      0.0620758384,
      -0.0467535816,
      0.0748587325,
      0.0571910068,
      -0.0668986589,
      0.0464791469,
      -0.1114452109,
      0.0235803593,
      0.1298431605,
      0.1093902662,
      0.0169269908,
      0.0470704511
    ],
    "cluster":3,
    "time":"00:01:40"
  },
  {
    "id":3,
    "url":"https:\/\/arxiv.org\/pdf\/2005.08100",
    "content":"Conformer: Convolution-augmented Transformer for Speech Recognition\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\nWang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang\n{anmolgulati, jamesqin, chungchengc, nikip, ngyuzh, jiahuiyu, weihan, shibow, zhangzd,\nyonghui, rpang}@google.com\nRecently Transformer and Convolution neural network (CNN)\nbased models have shown promising results in Automatic\nSpeech Recognition (ASR), outperforming Recurrent neural\nnetworks (RNNs). Transformer models are good at captur-\ning content-based global interactions, while CNNs exploit lo-\nIn this work, we achieve the best of\ncal features effectively.\nboth worlds by studying how to combine convolution neural\nnetworks and transformers to model both local and global de-\npendencies of an audio sequence in a parameter-ef\ufb01cient way.\nTo this regard, we propose the convolution-augmented trans-\nformer for speech recognition, named Conformer. Conformer\nsigni\ufb01cantly outperforms the previous Transformer and CNN\nbased models achieving state-of-the-art accuracies. On the\nwidely used LibriSpeech benchmark, our model achieves WER\nof 2.1%\/4.3% without using a language model and 1.9%\/3.9%\nwith an external language model on test\/testother. We also\nobserve competitive performance of 2.7%\/6.3% with a small\nmodel of only 10M parameters.\nIndex Terms: speech recognition, attention, convolutional neu-\nral networks, transformer, end-to-end\n1. Introduction\nEnd-to-end automatic speech recognition (ASR) systems based\non neural networks have seen large improvements in recent\nyears. Recurrent neural networks (RNNs) have been the de-\nfacto choice for ASR [1, 2, 3, 4] as they can model the temporal\ndependencies in the audio sequences effectively [5]. Recently,\nthe Transformer architecture based on self-attention [6, 7] has\nenjoyed widespread adoption for modeling sequences due to its\nability to capture long distance interactions and the high train-\ning ef\ufb01ciency. Alternatively, convolutions have also been suc-\ncessful for ASR [8, 9, 10, 11, 12], which capture local context\nprogressively via a local receptive \ufb01eld layer by layer.\nHowever, models with self-attention or convolutions each\nhas its limitations. While Transformers are good at modeling\nlong-range global context, they are less capable to extract \ufb01ne-\ngrained local feature patterns. Convolution neural networks\n(CNNs), on the other hand, exploit local information and are\nused as the de-facto computational block in vision. They learn\nshared position-based kernels over a local window which main-\ntain translation equivariance and are able to capture features like\nedges and shapes. One limitation of using local connectivity is\nthat you need many more layers or parameters to capture global\ninformation. To combat this issue, contemporary work Con-\ntextNet [10] adopts the squeeze-and-excitation module [13] in\neach residual block to capture longer context. However, it is still\nlimited in capturing dynamic global context as it only applies a\nglobal averaging over the entire sequence.\nRecent works have shown that combining convolution and\nFigure 1: Conformer encoder model architecture. Conformer\ncomprises of two macaron-like feed-forward layers with half-\nstep residual connections sandwiching the multi-headed self-\nattention and convolution modules. This is followed by a post\nself-attention improves over using them individually [14]. To-\ngether, they are able to learn both position-wise local features,\nand use content-based global interactions. Concurrently, papers\nlike [15, 16] have augmented self-attention with relative posi-\ntion based information that maintains equivariance. Wu et al.\n[17] proposed a multi-branch architecture with splitting the in-\nput into two branches: self-attention and convolution; and con-\ncatenating their outputs. Their work targeted mobile applica-\ntions and showed improvements in machine translation tasks.\nIn this work, we study how to organically combine con-\nvolutions with self-attention in ASR models. We hypothesize\nthat both global and local interactions are important for being\nparameter ef\ufb01cient. To achieve this, we propose a novel combi-\nnation of self-attention and convolution will achieve the best of\nboth worlds \u2013 self-attention learns the global interaction whilst\nthe convolutions ef\ufb01ciently capture the relative-offset-based lo-\ncal correlations. Inspired by Wu et al. [17, 18], we introduce\na novel combination of self-attention and convolution, sand-\nwiched between a pair feed forward modules, as illustrated in\nOur proposed model, named Conformer, achieves state-of-\nthe-art results on LibriSpeech, outperforming the previous best\npublished Transformer Transducer [7] by 15% relative improve-\nFeed Forward ModuleMulti-Head Self AttentionModuleConvolution Module1\/2 x1\/2 xFeed Forward ModuleDropoutConformer BlocksLinearSpecAug10 ms rate10 ms rateConvolutionSubsampling40 ms ratex N40 ms rate40 ms rate++++Layernorm\nFigure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the\nnumber of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a\nBatchnorm and then a swish activation layer.\nment on the testother dataset with an external language model.\nWe present three models based on model parameter limit con-\nstraints of 10M , 30M and 118M. Our 10M model shows an im-\nprovement when compared to similar sized contemporary work\n[10] with 2.7%\/6.3% on test\/testother datasets. Our medium\n30M parameters-sized model already outperforms transformer\ntransducer published in [7] which uses 139M model parameters.\nWith the big 118M parameter model, we are able to achieve\n2.1%\/4.3% without using language models and 1.9%\/3.9% with\nan external language model.\nWe further carefully study the effects of the number of at-\ntention heads, convolution kernel sizes, activation functions,\nplacement of feed-forward layers, and different strategies of\nadding convolution modules to a Transformer-based network,\nand shed light on how each contributes to the accuracy improve-\n2. Conformer Encoder\nOur audio encoder \ufb01rst processes the input with a convolution\nsubsampling layer and then with a number of conformer blocks,\nas illustrated in Figure 1. The distinctive feature of our model is\nthe use of Conformer blocks in the place of Transformer blocks\nA conformer block is composed of four modules stacked\ntogether, i.e, a feed-forward module, a self-attention module,\na convolution module, and a second feed-forward module in\nthe end. Sections 2.1, 1, and 2.3 introduce the self-attention,\nconvolution, and feed-forward modules, respectively. Finally,\n2.4 describes how these sub blocks are combined.\n2.1. Multi-Headed Self-Attention Module\nWe employ multi-headed self-attention (MHSA) while integrat-\ning an important technique from Transformer-XL [20], the rel-\native sinusoidal positional encoding scheme. The relative po-\nsitional encoding allows the self-attention module to general-\nize better on different input length and the resulting encoder is\nmore robust to the variance of the utterance length. We use pre-\nnorm residual units [21, 22] with dropout which helps training\nand regularizing deeper models. Figure 3 below illustrates the\nmulti-headed self-attention block.\nFigure 3: Multi-Headed self-attention module. We use multi-\nheaded self-attention with relative positional embedding in a\npre-norm residual unit.\n2.2. Convolution Module\nInspired by [17], the convolution module starts with a gating\nmechanism [23]\u2014a pointwise convolution and a gated linear\nunit (GLU). This is followed by a single 1-D depthwise convo-\nlution layer. Batchnorm is deployed just after the convolution\nto aid training deep models. Figure 2 illustrates the convolution\n2.3. Feed Forward Module\nThe Transformer architecture as proposed in [6] deploys a feed\nforward module after the MHSA layer and is composed of two\nlinear transformations and a nonlinear activation in between. A\nresidual connection is added over the feed-forward layers, fol-\nlowed by layer normalization. This structure is also adopted by\nTransformer ASR models [7, 24].\nWe follow pre-norm residual units [21, 22] and apply layer\nnormalization within the residual unit and on the input before\nthe \ufb01rst linear layer. We also apply Swish activation [25] and\ndropout, which helps regularizing the network. Figure 4 illus-\ntrates the Feed Forward (FFN) module.\n2.4. Conformer Block\nOur proposed Conformer block contains two Feed Forward\nmodules sandwiching the Multi-Headed Self-Attention module\nand the Convolution module, as shown in Figure 1.\nThis sandwich structure is inspired by Macaron-Net [18],\nwhich proposes replacing the original feed-forward layer in the\nTransformer block into two half-step feed-forward layers, one\nbefore the attention layer and one after. As in Macron-Net, we\nemploy half-step residual weights in our feed-forward (FFN)\nmodules. The second feed-forward module is followed by a\n\ufb01nal layernorm layer. Mathematically, this means, for input xi\nto a Conformer block i, the output yi of the block is:\ni = \u02dcxi + MHSA( \u02dcxi)\ni + Conv(x(cid:48)\nx(cid:48)(cid:48)\nyi = Layernorm(x(cid:48)(cid:48)\nFFN(x(cid:48)(cid:48)\nwhere FFN refers to the Feed forward module, MHSA refers to\nthe Multi-Head Self-Attention module, and Conv refers to the\nConvolution module as described in the preceding sections.\nOur ablation study discussed in Sec 3.4.3 compares the\nMacaron-style half-step FFNs with the vanilla FFN as used in\nprevious works. We \ufb01nd that having two Macaron-net style\nfeed-forward layers with half-step residual connections sand-\nwiching the attention and convolution modules in between pro-\nvides a signi\ufb01cant improvement over having a single feed-\nforward module in our Conformer architecture.\nThe combination of convolution and self-attention has been\nstudied before and one can imagine many ways to achieve\nLayernormGluActivationPointwiseConvBatchNormSwishActivation1DDepthwiseConvPointwiseConvDropout+LayernormMulti-Head Attention withRelative PositionalEmbeddingDropout+\nFigure 4: Feed forward module. The \ufb01rst linear layer uses an expansion factor of 4 and the second linear layer projects it back to the\nmodel dimension. We use swish activation and a pre-norm residual units in feed forward module.\nthat. Different options of augmenting convolutions with self-\nattention are studied in Sec 3.4.2. We found that convolution\nmodule stacked after the self-attention module works best for\nspeech recognition.\nWe evaluate the proposed model on the LibriSpeech [26]\ndataset, which consists of 970 hours of labeled speech and\nan additional 800M word token text-only corpus for building\nlanguage model. We extracted 80-channel \ufb01lterbanks features\ncomputed from a 25ms window with a stride of 10ms. We use\nSpecAugment [27, 28] with mask parameter (F = 27), and ten\ntime masks with maximum time-mask ratio (pS = 0.05), where\nthe maximum-size of the time mask is set to pS times the length\nof the utterance.\n3.2. Conformer Transducer\nWe identify three models, small, medium and large, with 10M,\n30M, and 118M params, respectively, by sweeping different\ncombinations of network depth, model dimensions, number of\nattention heads and choosing the best performing one within\nmodel parameter size constraints. We use a single-LSTM-layer\ndecoder in all our models. Table 1 describes their architecture\nhyper-parameters.\nFor regularization, we apply dropout [29] in each residual\nunit of the conformer, i.e, to the output of each module, before\nit is added to the module input. We use a rate of Pdrop = 0.1.\nVariational noise [5, 30] is introduced to the model as a regu-\nlarization. A (cid:96)2 regularization with 1e \u2212 6 weight is also added\nto all the trainable weights in the network. We train the models\nwith the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and\n(cid:15) = 10\u22129 and a transformer learning rate schedule [6], with\n10k warm-up steps and peak learning rate 0.05\/\nthe model dimension in conformer encoder.\nWe use a 3-layer LSTM language model (LM) with width\n4096 trained on the LibriSpeech langauge model corpus with\nthe LibriSpeech960h transcripts added, tokenized with the 1k\nWPM built from LibriSpeech 960h. The LM has word-level\nperplexity 63.9 on the dev-set transcripts. The LM weight \u03bb\nfor shallow fusion is tuned on the dev-set via grid search. All\nmodels are implemented with Lingvo toolkit [32].\n3.3. Results on LibriSpeech\nTable 2 compares the (WER) result of our model on Lib-\nriSpeech test-clean\/test-other with a few state-of-the-art mod-\nels include: ContextNet [10], Transformer transducer [7], and\nQuartzNet [9]. All our evaluation results round up to 1 digit\nafter decimal point.\nWithout a language model,\nthe performance of our\nmedium model already achieve competitive results of 2.3\/5.0\non test\/testother outperforming the best known Transformer,\nLSTM based model, or a similar sized convolution model. With\nthe language model added, our model achieves the lowest word\nTable 1: Model hyper-parameters for Conformer S, M, and L\nmodels, found via sweeping different combinations and choos-\ning the best performing models within the parameter limits.\nAttention Heads\nConv Kernel Size\nTable 2: Comparison of Conformer with recent published mod-\nels. Our model shows improvements consistently over various\nmodel parameter size constraints. At 10.3M parameters, our\nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our\nmodel already signi\ufb01cantly outperforms the previous published\nstate of the art results of Transformer Transducer [7] with 139M\n#Params (M) WER Without LM\nTransformer [33]\nTransformer [34]\nTransformer [19]\nTransformer [7]\nContextNet(S) [10]\nContextNet(M) [10]\nContextNet(L) [10]\nConformer (Ours)\nerror rate among all the existing models. This clearly demon-\nstrates the effectiveness of combining Transformer and convo-\nlution in a single neural network.\n3.4. Ablation Studies\n3.4.1. Conformer Block vs. Transformer Block\nA Conformer block differs from a Transformer block in a\nnumber of ways, in particular, the inclusion of a convolution\nblock and having a pair of FFNs surrounding the block in the\nMacaron-style. Below we study these effects of these differ-\nences by mutating a Conformer block towards a Transformer\nblock, while keeping the total number of parameters unchanged.\nTable 3 shows the impact of each change to the Conformer\nblock. Among all differences, convolution sub-block is the most\nLayernormLinearLayerDropoutLinearLayerSwishActivationDropout+\nimportant feature, while having a Macaron-style FFN pair is\nalso more effective than a single FFN of the same number of\nparameters. Using swish activations led to faster convergence\nin the Conformer models.\nTable 3: Disentangling Conformer. Starting from a Conformer\nblock, we remove its features and move towards a vanilla Trans-\n(1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style\nFFN pairs with a single FFN; (4) replacing self-attention with\nrelative positional embedding [20] with a vanilla self-attention\nlayer [6]. All ablation study results are evaluated without the\nConformer Model\n\u2013 Convolution Block\n\u2013 Relative Pos. Emb.\n3.4.2. Combinations of Convolution and Transformer Modules\nWe study the effects of various different ways of combining the\nmulti-headed self-attention (MHSA) module with the convolu-\ntion module. First, we try replacing the depthwise convolution\nin the convolution module with a lightweight convolution [35],\nsee a signi\ufb01cant drop in the performance especially on the dev-\nother dataset. Second, we study placing the convolution mod-\nule before the MHSA module in our Conformer model and \ufb01nd\nthat it degrades the results by 0.1 on dev-other. Another pos-\nsible way of the architecture is to split the input into parallel\nbranches of multi-headed self attention module and a convolu-\ntion module with their output concatenated as suggested in [17].\nWe found that this worsens the performance when compared to\nour proposed architecture.\nThese results in Table 4 suggest the advantage of placing\nthe convolution module after the self-attention module in the\nConformer block.\nTable 4: Ablation study of Conformer Attention Convolution\nBlocks. Varying the combination of the convolution block with\nthe multi-headed self attention: (1) Conformer architecture; (2)\nUsing Lightweight convolutions instead of depthwise convolu-\ntion in the convolution block in Conformer; (3) Convolution be-\nfore multi-headed self attention; (4) Convolution and MHSA in\nparallel with their output concatenated [17].\nModel Architecture\n\u2013 Depthwise conv + Lightweight convolution\nConvolution block before MHSA\nParallel MHSA and Convolution\n3.4.3. Macaron Feed Forward Modules\nInstead of a single feed-forward module (FFN) post the atten-\ntion blocks as in the Transformer models, the Conformer block\nhas a pair of macaron-like Feed forward modules sandwiching\nthe self-attention and convolution modules. Further, the Con-\nformer feed forward modules are used with half-step residuals.\nTable 5 shows the impact of changing the Conformer block to\nuse a single FFN or full-step residuals.\nTable 5: Ablation study of Macaron-net Feed Forward mod-\nules. Ablating the differences between the Conformer feed for-\nward module with that of a single FFN used in Transformer\nmodels: (1) Conformer; (2) Conformer with full-step residuals\nin Feed forward modules; (3) replacing the Macaron-style FFN\npair with a single FFN.\nFull step residuals\n3.4.4. Number of Attention Heads\nIn self-attention, each attention head learns to focus on different\nparts of the input, making it possible to improve predictions\nbeyond the simple weighted average. We perform experiments\nto study the effect of varying the number of attention heads from\n4 to 32 in our large model, using the same number of heads\nin all layers. We \ufb01nd that increasing attention heads up to 16\nimproves the accuracy, especially over the devother datasets, as\nshown in Table 6.\nTable 6: Ablation study on the attention heads in multi-headed\nself attention.\n3.4.5. Convolution Kernel Sizes\nTo study the effect of kernel sizes in the depthwise convolu-\ntion, we sweep the kernel size in {3, 7, 17, 32, 65} of the large\nmodel, using the same kernel size for all layers. We \ufb01nd that the\nperformance improves with larger kernel sizes till kernel sizes\n17 and 32 but worsens in the case of kernel size 65, as show\nin Table 7. On comparing the second decimal in dev WER, we\n\ufb01nd kernel size 32 to perform better than rest.\nTable 7: Ablation study on depthwise convolution kernel sizes.\nIn this work, we introduced Conformer, an architecture that\nintegrates components from CNNs and Transformers for end-\nto-end speech recognition. We studied the importance of each\ncomponent, and demonstrated that the inclusion of convolution\nmodules is critical to the performance of the Conformer model.\nThe model exhibits better accuracy with fewer parameters than\nprevious work on the LibriSpeech dataset, and achieves a new\nstate-of-the-art performance at 1.9%\/3.9% for test\/testother.\n[1] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., \u201cState-\nof-the-art speech recognition with sequence-to-sequence models,\u201d\nin 2018 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2018, pp. 4774\u20134778.\n[2] K. Rao, H. Sak, and R. Prabhavalkar, \u201cExploring architectures,\ndata and units for streaming end-to-end speech recognition with\nrnn-transducer,\u201d in 2017 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU).\nIEEE, 2017, pp. 193\u2013199.\n[3] Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez,\nD. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang,\nD. Bhatia, Y. Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby,\nS.-Y. Chang, K. Rao, and A. Gruenstein, \u201cStreaming End-to-end\nSpeech Recognition For Mobile Devices,\u201d in Proc. ICASSP, 2019.\n[4] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen, and et al., \u201cA streaming\non-device end-to-end model surpassing server-side conventional\nmodel quality and latency,\u201d in ICASSP, 2020.\n[5] A. Graves, \u201cSequence transduction with recurrent neural net-\nworks,\u201d arXiv preprint arXiv:1211.3711, 2012.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\n[7] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and\nS. Kumar, \u201cTransformer transducer: A streamable speech recog-\nnition model with transformer encoders and rnn-t loss,\u201d in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2020, pp. 7829\u20137833.\n[8] J. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Co-\nhen, H. Nguyen, and R. T. Gadde, \u201cJasper: An end-to-end convo-\nlutional neural acoustic model,\u201d arXiv preprint arXiv:1904.03288,\n[9] S. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev,\nV. Lavrukhin, R. Leary, J. Li, and Y. Zhang, \u201cQuartznet: Deep\nautomatic speech recognition with 1d time-channel separable con-\nvolutions,\u201d arXiv preprint arXiv:1910.10261, 2019.\n[10] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati,\nR. Pang, and Y. Wu, \u201cContextnet: Improving convolutional neural\nnetworks for automatic speech recognition with global context,\u201d\narXiv preprint arXiv:2005.03191, 2020.\n[11] T. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhad-\nran, \u201cDeep convolutional neural networks for lvcsr,\u201d in 2013 IEEE\ninternational conference on acoustics, speech and signal process-\nIEEE, 2013, pp. 8614\u20138618.\n[12] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn,\nand D. Yu, \u201cConvolutional neural networks for speech recogni-\ntion,\u201d IEEE\/ACM Transactions on audio, speech, and language\nprocessing, vol. 22, no. 10, pp. 1533\u20131545, 2014.\n[13] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d\nin Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7132\u20137141.\n[14] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \u201cAttention\naugmented convolutional networks,\u201d in Proceedings of the IEEE\nInternational Conference on Computer Vision, 2019, pp. 3286\u2013\n[15] B. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, \u201cConvolu-\ntional self-attention networks,\u201d arXiv preprint arXiv:1904.03107,\n[16] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\nand Q. V. Le, \u201cQanet: Combining local convolution with\nglobal self-attention for reading comprehension,\u201d arXiv preprint\narXiv:1804.09541, 2018.\n[17] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, \u201cLite transformer with\nlong-short range attention,\u201d arXiv preprint arXiv:2004.11886,\n[18] Y. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and\nT.-Y. Liu, \u201cUnderstanding and improving transformer from a\nmulti-particle dynamic system point of view,\u201d arXiv preprint\narXiv:1906.02762, 2019.\n[19] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., \u201cA\ncomparative study on transformer vs rnn in speech applications,\u201d\narXiv preprint arXiv:1909.06317, 2019.\n[20] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhut-\ndinov, \u201cTransformer-xl: Attentive language models beyond a\n\ufb01xed-length context,\u201d 2019.\n[21] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n\u201cLearning deep transformer models for machine translation,\u201d in\nProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics. Association for Computational Lin-\nguistics, Jul. 2019, pp. 1810\u20131822.\n[22] T. Q. Nguyen and J. Salazar, \u201cTransformers without\nImproving the normalization of self-attention,\u201d arXiv preprint\narXiv:1910.05895, 2019.\n[23] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \u201cLanguage\nmodeling with gated convolutional networks,\u201d in Proceedings of\nthe 34th International Conference on Machine Learning-Volume\nJMLR. org, 2017, pp. 933\u2013941.\n[24] L. Dong, S. Xu, and B. Xu, \u201cSpeech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,\u201d in 2018\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2018, pp. 5884\u20135888.\n[25] P. Ramachandran, B. Zoph, and Q. V. Le, \u201cSearching for activa-\ntion functions,\u201d arXiv preprint arXiv:1710.05941, 2017.\n[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: an asr corpus based on public domain audio books,\u201d\nin 2015 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2015, pp. 5206\u20135210.\n[27] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmen-\ntation method for automatic speech recognition,\u201d arXiv preprint\narXiv:1904.08779, 2019.\n[28] D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V.\nLe, and Y. Wu, \u201cSpecaugment on large scale datasets,\u201d arXiv\npreprint arXiv:1912.05533, 2019.\n[29] N. Srivastava, G. Hinton, A. Krizhevsky,\nI. Sutskever, and\nR. Salakhutdinov, \u201cDropout: A simple way to prevent neural net-\nworks from over\ufb01tting,\u201d Journal of Machine Learning Research,\nvol. 15, no. 56, pp. 1929\u20131958, 2014.\n[30] K.-C. Jim, C. L. Giles, and B. G. Horne, \u201cAn analysis of noise\nin recurrent neural networks: convergence and generalization,\u201d\nIEEE Transactions on neural networks, vol. 7, no. 6, pp. 1424\u2013\n[31] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[32] J. Shen, P. Nguyen, Y. Wu, Z. Chen, and et al., \u201cLingvo: a modu-\nlar and scalable framework for sequence-to-sequence modeling,\u201d\n[33] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar,\nH. Huang, A. Tjandra, X. Zhang, F. Zhang et al., \u201cTransformer-\nbased acoustic modeling for hybrid speech recognition,\u201d arXiv\npreprint arXiv:1910.09799, 2019.\n[34] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave,\nV. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-\nend asr: from supervised to semi-supervised learning with modern\narchitectures,\u201d 2019.\n[35] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli, \u201cPay\nless attention with lightweight and dynamic convolutions,\u201d arXiv\npreprint arXiv:1901.10430, 2019.",
    "embedding":[
      -0.1859026849,
      -0.166969493,
      0.0037825357,
      -0.011899896,
      -0.0059855198,
      0.0495901741,
      -0.0315768719,
      -0.0809599459,
      0.0295690186,
      -0.1091482043,
      -0.0065582558,
      -0.0603744164,
      -0.0376010388,
      0.0821132511,
      -0.0611128286,
      -0.0404707529,
      -0.0052248966,
      0.0849252567,
      -0.0825621188,
      -0.0939679593,
      0.0392621644,
      0.0909138396,
      -0.0087142531,
      0.0343720019,
      0.0927849039,
      0.0383535326,
      -0.0150579344,
      -0.0621698722,
      0.0358126871,
      0.0021555398,
      0.0148120495,
      0.0603773482,
      0.0221518874,
      0.0394587591,
      -0.0852045342,
      0.0700138956,
      -0.0117232809,
      -0.0259683765,
      -0.0031071955,
      -0.0859356746,
      0.0892494172,
      0.0164099932,
      0.0254017711,
      -0.0418762937,
      0.0577393733,
      -0.0317872316,
      -0.0246993341,
      -0.0015279335,
      0.0590087287,
      0.0790505707,
      0.0427311361,
      0.0241355579,
      0.0046112202,
      0.1419858634,
      -0.0734557658,
      0.0078719473,
      0.0678301156,
      -0.0301172789,
      0.0027787499,
      -0.007649262,
      -0.0211013518,
      -0.0676930174,
      0.0241380688,
      -0.0360167772,
      -0.0444779955,
      -0.0498648286,
      0.027926771,
      0.0334575847,
      -0.0729502216,
      0.0394165814,
      -0.0845049396,
      0.0560082234,
      0.017652005,
      0.0264927782,
      0.0650893748,
      -0.034281306,
      0.0680944026,
      -0.0462572314,
      0.073983863,
      0.0081767673,
      0.0789714381,
      -0.0033090112,
      0.1084672436,
      -0.0190701634,
      0.1040993482,
      -0.0259807482,
      -0.063985616,
      -0.0530985668,
      0.0246255323,
      0.0314560719,
      -0.1250670105,
      -0.0503557846,
      0.0018676459,
      -0.0453049541,
      0.004393999,
      0.0586889796,
      -0.035677664,
      0.0098696453,
      0.0100226924,
      0.029287558,
      -0.0026184861,
      -0.0074784937,
      0.0138393501,
      -0.0687790662,
      0.024378838,
      -0.019472478,
      0.0106821181,
      0.0216977615,
      0.0231039487,
      -0.0179831479,
      0.0085650869,
      -0.0155585464,
      0.0743711665,
      -0.0500660129,
      0.087623857,
      -0.0073143644,
      -0.0452620089,
      -0.0115885809,
      0.0107467165,
      0.0051988377,
      -0.0852858499,
      0.0519664474,
      -0.0837513804,
      0.0779009014,
      0.0168600809,
      0.0094019342,
      -0.0436938778,
      2.543960242e-33,
      -0.0192142632,
      0.008401854,
      0.0773727819,
      0.0266127009,
      0.0675833672,
      -0.0449621528,
      -0.0234785117,
      0.0703857392,
      -0.050379321,
      -0.0420860052,
      -0.0668522641,
      0.0481638685,
      -0.0657265335,
      0.0115947807,
      -0.0027940548,
      0.0684594139,
      -0.0167265832,
      0.0178194027,
      -0.036349114,
      -0.036240384,
      0.0106380004,
      0.0072432416,
      0.0361852422,
      0.0047632535,
      0.0197702385,
      -0.0243612565,
      0.0243747085,
      -0.0749018341,
      -0.0443864278,
      -0.0058362558,
      -0.0322714895,
      -0.012921703,
      -0.0141082434,
      0.0062125511,
      0.0242389161,
      -0.0363387093,
      0.0364360511,
      0.0362943858,
      -0.0077688661,
      -0.0386376083,
      -0.0355414003,
      0.0232184604,
      -0.0886925533,
      0.0183238722,
      -0.0590662658,
      -0.0465390123,
      0.0764493346,
      0.0428012274,
      0.0247406848,
      -0.0498485789,
      0.0422475263,
      0.0333774388,
      -0.1928374022,
      -0.0628617182,
      0.0245951284,
      -0.0633552074,
      0.0798447058,
      0.0793942958,
      0.0106282113,
      0.0167401154,
      0.0103448499,
      0.0309108067,
      -0.0231235456,
      0.0493715517,
      0.0439990573,
      -0.0368320867,
      -0.0485132858,
      0.0392649472,
      0.0288163573,
      0.0046659275,
      -0.0083435616,
      0.0384007767,
      0.013232992,
      0.0432512984,
      -0.009958311,
      0.0505102389,
      -0.0061279368,
      -0.0579973273,
      0.0120045952,
      0.0925754309,
      -0.0295335967,
      0.0468011685,
      -0.0130587528,
      0.0064224964,
      -0.1348285377,
      -0.0178424064,
      0.0813731775,
      -0.0925713107,
      0.0358577296,
      -0.0829176158,
      -0.0975235626,
      0.0152944354,
      -0.0065707546,
      0.0070001208,
      -0.0072405697,
      -4.347735936e-33,
      0.0308795273,
      0.1300005317,
      -0.0127087142,
      0.0590664186,
      -0.055305358,
      -0.0536803529,
      0.0277045816,
      0.0123752616,
      0.065451391,
      -0.0411192328,
      0.0318458565,
      -0.011548684,
      0.0817561075,
      -0.021285966,
      0.0758800581,
      -0.0360862352,
      -0.0182968695,
      0.0332981683,
      -0.0496129543,
      0.0140489386,
      0.0584898628,
      0.1059547514,
      -0.0499872863,
      0.054337088,
      -0.1337542087,
      -0.039238032,
      -0.0683018938,
      0.0053863702,
      0.0262669101,
      0.0589184985,
      -0.0619017445,
      -0.0070916051,
      -0.0451047942,
      -0.028369097,
      -0.034463007,
      0.0179244485,
      0.0211284831,
      -0.0044709258,
      0.0027557798,
      -0.018089911,
      0.1023152173,
      0.0102407234,
      -0.0110662077,
      -0.0031920648,
      -0.0173320696,
      -0.0809480101,
      -0.0478706472,
      0.060269244,
      -0.0123150218,
      0.0260463562,
      0.0589221194,
      -0.0146321971,
      -0.0063620917,
      0.0124394679,
      -0.0608284585,
      0.0111248475,
      -0.0602186956,
      -0.0589050688,
      0.0846642926,
      -0.004808933,
      -0.0457194969,
      -0.0126674036,
      0.0218918696,
      -0.0929788202,
      -0.0246038213,
      0.0469824225,
      0.0060575185,
      0.04233348,
      0.0843858942,
      0.0299035404,
      0.0020731306,
      -0.0229433607,
      0.0673563778,
      0.0205125064,
      -0.0448476076,
      -0.0561981276,
      -0.0432870351,
      -0.1272156239,
      -0.0987350047,
      -0.0337216295,
      -0.0510186441,
      -0.0432468094,
      0.0061225551,
      0.001637482,
      0.070712015,
      0.0598884076,
      0.0374299996,
      0.0338604152,
      0.0908282027,
      0.0832401812,
      -0.0523873642,
      0.0405964665,
      -0.0034615323,
      0.0337219015,
      -0.0182714108,
      -0.000000047,
      -0.0686282814,
      0.0869502947,
      -0.0233408306,
      -0.045104038,
      -0.0253901593,
      -0.0978712589,
      0.0165796336,
      0.0039817477,
      0.0579492524,
      -0.0095004942,
      0.0637722984,
      -0.0297098048,
      -0.0062167696,
      -0.0069337972,
      -0.0489921123,
      0.0124970498,
      0.0543920361,
      0.0346476808,
      -0.0006963972,
      -0.0513469689,
      -0.0004509656,
      0.0736576319,
      -0.0024860736,
      -0.0363205969,
      0.0250542425,
      -0.0397676416,
      -0.0495186932,
      0.0605850145,
      -0.008642613,
      0.0165632647,
      -0.0244796462,
      0.0320457444,
      -0.0253444873,
      -0.0411720872,
      -0.0134319849,
      0.0129321897,
      0.0395665951,
      -0.0404854827,
      0.0020147667,
      0.0451778471,
      0.0325565375,
      0.0534344539,
      -0.0337079167,
      0.0247355122,
      -0.0142369196,
      -0.0130310757,
      0.0492663719,
      -0.0969054177,
      0.0648487955,
      0.0525891297,
      0.0419800803,
      -0.0087034358,
      -0.0169065613,
      0.0546596348,
      0.0421296395,
      0.0258568898,
      0.0106624383,
      0.0025222644,
      0.0214406066,
      0.0404037684,
      -0.0366715752,
      0.0233296733,
      -0.0289133396,
      -0.0085887611
    ],
    "cluster":0,
    "time":"00:01:40"
  },
  {
    "id":4,
    "url":"https:\/\/inference-docs.cerebras.ai\/quickstart",
    "content":"QuickStart - Cerebras Inference\nCerebras Inference\nStreaming Responses\nStructured Outputs\nCePO: Cerebras Planning & Optimization\u200b\nOpenAI Compatibility\nGet started with the Cerebras API.\nTo get started with a free API key,\nThis QuickStart guide is designed to assist you in making your first API call. If you are an experienced AI applications developer, you may find it more beneficial to go directly to the\nAPI reference documentation\nIf you would like to interact with the models using Cerebras\u2019 Inference solution before making an API call, please visit the\ndeveloper playground\nThis guide will walk you through:\nSetting up your developer environment\nInstalling the Cerebras Inference library\nMaking your first request to the Cerebras API\nTo complete this guide, you will need:\nA Cerebras account\nA Cerebras Inference API key\nPython 3.7+ or TypeScript 4.5+\nSet up your API key\nThe first thing you will need is a valid API key. Please visit\nand navigate to \u201cAPI Keys\u201d on the left nav bar.\nFor security reasons and to avoid configuring your API key each time, it is recommended to set your API key as an environment variable. You can do this by running the following command in your terminal:\nReport incorrect code\nCEREBRAS_API_KEY\n\"your-api-key-here\"\nInstall the Cerebras Inference library\nThe Cerebras Inference library is available for download and installation through the Python Package Index (PyPI) and the npm package manager. To install the library run either of the following commands in your terminal, based on your language of choice:\nNote: You can also call the underlying API directly (see cURL request example below in Step 3).\nReport incorrect code\ncerebras_cloud_sdk\nMaking an API request\nIf your request is being blocked by CloudFront, ensure that\nis included in your headers\nOnce you have configured your API key, you are ready to send your first API request.\nThe following code snippets demonstrate how to make an API request to the Cerebras API to perform a chat completion.\nReport incorrect code\ncerebras.cloud.sdk\n# This is the default and can be omitted\nos.environ.get(\n\"CEREBRAS_API_KEY\"\nchat_completion\nclient.chat.completions.create(\n\"Why is fast inference important?\"\n\"llama-4-scout-17b-16e-instruct\"\n(chat_completion)\nVisit our repositories for our\nto learn about the details of our available endpoints and request parameters.\nstream responses\nWas this page helpful?\nBuild with Cerebras Inference\nResponses are generated using AI and may contain mistakes.",
    "embedding":[
      -0.0899421722,
      -0.0712989345,
      -0.0170264598,
      -0.0130155031,
      -0.0468864664,
      0.0173307937,
      -0.0616453215,
      0.1218675002,
      0.0143883135,
      -0.031675037,
      -0.0621397346,
      -0.1545172334,
      -0.0523332581,
      0.0339076892,
      0.0788058862,
      -0.0134420637,
      0.0310330223,
      -0.0436403714,
      0.0308648907,
      -0.0422586873,
      0.0814747289,
      -0.0659550875,
      -0.0175999124,
      -0.0523962826,
      -0.035946887,
      -0.052829735,
      0.0318625793,
      0.0087879924,
      0.0535121933,
      -0.0210302602,
      0.1221909598,
      -0.0219879709,
      -0.0219185259,
      0.0098484643,
      0.0218407027,
      0.0356376655,
      -0.0540548824,
      -0.1617264152,
      -0.0465839952,
      -0.0264617614,
      0.0764791965,
      0.0502851605,
      -0.014787362,
      -0.0672800988,
      0.0712245256,
      0.0265008137,
      -0.0113029201,
      -0.0050543775,
      -0.0329792872,
      -0.0009876532,
      -0.042726364,
      -0.0684952289,
      -0.0193389766,
      -0.0116434274,
      -0.0159986541,
      0.0180018935,
      0.0103914747,
      -0.0126335109,
      0.0274031889,
      -0.0439490229,
      0.0220298581,
      -0.0472144522,
      0.0555422232,
      -0.0155355819,
      -0.0471789278,
      0.0628486946,
      -0.0825152695,
      0.0701305196,
      0.0804099292,
      -0.0400409028,
      -0.125227049,
      -0.0074140071,
      0.0450745523,
      -0.0058431923,
      0.0276257824,
      -0.0577770434,
      0.0657859221,
      -0.0626751408,
      0.0482520424,
      -0.090930894,
      -0.0558950864,
      -0.0507463962,
      -0.0766338855,
      0.0352448672,
      0.0275442041,
      0.0597617887,
      0.0768733695,
      0.0671980754,
      0.0954201445,
      0.0583090745,
      0.0859977379,
      -0.0667982921,
      -0.0720117241,
      0.0081683407,
      0.0127984472,
      0.0940056145,
      -0.0177215617,
      -0.1006551012,
      -0.0046556792,
      0.0255441889,
      -0.0025031306,
      0.0742301047,
      0.0665603355,
      0.0127588287,
      -0.0293024573,
      -0.0153536573,
      0.0556497127,
      0.0367496796,
      -0.0524563231,
      -0.0265400317,
      -0.0182438828,
      0.0382514484,
      0.0604496598,
      -0.0273876637,
      0.0021027057,
      0.0910277218,
      -0.0324384309,
      -0.0189559441,
      0.085814558,
      -0.0528054871,
      0.0211780258,
      -0.0254920106,
      -0.0459471159,
      0.0481748804,
      -0.0022788781,
      -0.0046203644,
      -0.0631991774,
      6.668826306e-33,
      0.0193265118,
      -0.0040718284,
      0.0451305769,
      -0.0132605406,
      -0.0164635908,
      0.0154934423,
      0.017331453,
      0.0749694258,
      -0.062103726,
      -0.0492645577,
      -0.0263164006,
      0.0422715917,
      -0.0662185699,
      -0.0298280008,
      -0.0288236365,
      -0.0222451929,
      -0.07617075,
      0.0633813441,
      -0.034661416,
      -0.050317198,
      0.0074136308,
      -0.0254494175,
      -0.0163597111,
      0.0264097601,
      0.0538223274,
      0.0848320276,
      0.0106804334,
      0.0532718487,
      -0.0496615246,
      -0.004772156,
      0.008519779,
      -0.0396981128,
      -0.0158885308,
      0.0353439525,
      0.0676515847,
      0.0308641475,
      0.0375450291,
      -0.0168573335,
      -0.08913441,
      0.0204308238,
      -0.0010715885,
      0.0364395007,
      0.0143679576,
      -0.029705029,
      -0.0798767507,
      -0.0892209113,
      0.0038725683,
      -0.033873044,
      0.1326373518,
      -0.0007527284,
      0.0112153729,
      -0.0150858043,
      -0.011089066,
      -0.0590910316,
      0.0007189226,
      0.0135127604,
      -0.0281739272,
      -0.0304495562,
      0.0277536456,
      -0.0964917615,
      -0.0382373556,
      -0.0640834942,
      -0.0342644043,
      -0.0638234839,
      0.0135335261,
      0.0661011636,
      -0.0340326875,
      0.0127879344,
      0.0417889282,
      -0.0138097163,
      -0.0477039479,
      0.0372238718,
      -0.0170857441,
      -0.0257364511,
      0.0515193418,
      0.0403589532,
      -0.0155554451,
      -0.0160930045,
      -0.0174803901,
      -0.0028258422,
      0.0207388159,
      -0.0118791936,
      -0.0477047637,
      0.0465912521,
      0.1084393784,
      0.0482783355,
      -0.0029739887,
      -0.0137614999,
      0.0145564917,
      -0.0372235179,
      -0.0196322072,
      -0.0171693992,
      0.0358139612,
      0.0230009556,
      -0.081225425,
      -6.221671928e-33,
      0.0151840383,
      -0.0298134331,
      -0.0126537485,
      0.0118102524,
      0.0375950336,
      -0.0299974661,
      -0.0282672364,
      0.0423034802,
      0.0035323682,
      -0.0500254147,
      -0.0436638705,
      -0.0613637976,
      0.0795186907,
      -0.0187589284,
      0.0660834536,
      0.0244492423,
      -0.0906384513,
      -0.0067883143,
      -0.0023483322,
      0.0436233245,
      0.0178411566,
      0.0735549331,
      -0.0570715107,
      -0.0647653639,
      -0.0053484584,
      0.0624136925,
      -0.0245348979,
      0.0614421032,
      0.0565270968,
      -0.0471724384,
      -0.066604048,
      0.025854392,
      -0.0696569905,
      0.0099194907,
      -0.0521141924,
      -0.0172446594,
      0.0638504848,
      0.0140471617,
      0.0049138148,
      0.0295015611,
      0.0584391616,
      0.0182082821,
      -0.070111312,
      -0.0153958555,
      0.0495268181,
      0.0757486001,
      0.0218021236,
      0.1020841002,
      0.0008690191,
      0.0748412237,
      -0.0368612222,
      0.0236646403,
      -0.0634567216,
      -0.0638820156,
      -0.029801337,
      -0.0235725883,
      0.0648852363,
      -0.0525023043,
      -0.0209117681,
      -0.0040297192,
      -0.0706784129,
      -0.0906439424,
      0.0734064505,
      -0.0327644981,
      0.0193216167,
      0.0083906436,
      -0.0777690187,
      0.02093607,
      -0.0151126971,
      -0.0656823888,
      -0.0240264852,
      -0.0376558863,
      0.0176807716,
      -0.0450696684,
      -0.0019276303,
      -0.0119642727,
      -0.071564205,
      0.0030367915,
      0.1090328619,
      -0.0022544167,
      -0.0403504595,
      0.0129049392,
      0.1625590771,
      0.0135811046,
      0.0344067663,
      0.023495404,
      0.0283822548,
      -0.0135000432,
      0.0613957718,
      0.0529959723,
      -0.1236109957,
      0.0534432642,
      0.11523211,
      0.0648616925,
      -0.0167152714,
      -0.0000000551,
      -0.0603458025,
      0.0657643378,
      -0.0223996658,
      0.0679450408,
      -0.0247466192,
      0.0363142639,
      -0.0645505413,
      0.0077440171,
      0.0180111974,
      -0.0317843445,
      -0.0275558699,
      -0.0305893123,
      -0.0264491104,
      0.0289200768,
      0.0207795631,
      0.0806722939,
      0.0181362778,
      -0.0216610879,
      -0.0165792219,
      -0.0544627644,
      0.0409874953,
      -0.0396971852,
      0.0399279483,
      0.0729692802,
      0.0630736873,
      0.0093375221,
      -0.0068732128,
      0.0816216916,
      -0.009890046,
      -0.05189633,
      -0.0703401342,
      0.0421661474,
      0.0988503247,
      -0.0145237017,
      0.0410217158,
      0.0797802135,
      0.0022218623,
      -0.0252242275,
      0.0354874767,
      0.0521404222,
      0.061754372,
      0.0651093498,
      -0.1010756865,
      0.0114194918,
      0.0332130864,
      0.0988373309,
      0.0844327882,
      -0.0617010109,
      0.0054463069,
      0.0396245457,
      -0.0539351553,
      -0.0747597218,
      -0.0721880943,
      0.0289620776,
      -0.0777777731,
      0.0649744943,
      -0.0093896817,
      -0.0291030053,
      0.0128015578,
      0.0055948575,
      0.0776174739,
      0.0836476311,
      0.005926976,
      -0.0092229974
    ],
    "cluster":5,
    "time":"00:01:40"
  },
  {
    "id":5,
    "url":"https:\/\/docs.exa.ai\/reference\/quickstart",
    "content":"Quickstart - Exa\nGetting Started\nGetting Started\nFind similar links\nOpenAPI Specification\nHow Exa Search Works\nContents Retrieval\nCrawling Subpages\nLivecrawling Contents\nMigrating from Bing\nAnthropic Tool Calling\nOpenAI Tool Calling\nOpenAI SDK Compatibility\nOpenAI Responses API\nManaging Your Team\nEnterprise Documentation & Security\nCreate and setup your API key\nCreate a .env file\nMake an API request\nGetting Started\nMake your first request to one of Exa\u2019s API endpoints\nCreate and setup your API key\nGet your Exa API key\nCreate a .env file\nCreate a file called\nin the root of your project and add the following line.\nMake an API request\nUse our python or javascript SDKs, or call the API directly with cURL.\nInstall the python SDKs with pip.  If you want to store your API key in a\nfile, make sure to install the dotenv library.\nOnce you\u2019ve installed the SDKs, create a file called\nand add the code below.\nSearch and crawl\nChat Completions\nFind similar links and get full text\nGet a list of results and their full text content.\n# Use .env to store your API key or paste it directly into the code\nexa.search_and_contents(\n\"An article about the state of AGI\"\nPowered by Mintlify\nResponses are generated using AI and may contain mistakes.",
    "embedding":[
      -0.0521692559,
      -0.002862426,
      -0.0955064371,
      0.0321790837,
      0.0098975301,
      -0.0693340898,
      -0.0159749836,
      0.0302432198,
      0.0153430654,
      -0.0109432591,
      0.0269705504,
      -0.0047562947,
      -0.0244683176,
      -0.0102355871,
      0.0957058817,
      0.0928679332,
      0.0363511629,
      -0.021070078,
      0.000160964,
      -0.0475941375,
      0.0309451316,
      0.0281605851,
      0.0247474667,
      -0.0145610506,
      -0.0939395651,
      -0.0368671454,
      -0.0218738467,
      -0.0217101593,
      -0.0147725567,
      0.0280864313,
      0.0453709029,
      -0.0210295226,
      0.013395994,
      0.0666246042,
      0.0439265147,
      0.06888666,
      0.0458276197,
      -0.0414364338,
      -0.0433467291,
      -0.0521062389,
      0.0622947738,
      -0.0164408423,
      -0.0849088505,
      0.0123229995,
      0.0124079278,
      -0.0220557563,
      -0.0171383452,
      -0.0226723421,
      0.0267558992,
      0.0649843216,
      -0.1051042899,
      -0.0301763657,
      -0.0458786264,
      -0.041718889,
      0.0053907032,
      0.0190426316,
      -0.0309051592,
      -0.0014769862,
      -0.0192431044,
      -0.0268453434,
      0.0728784129,
      -0.0654445291,
      0.0160278957,
      -0.0354900956,
      -0.0423693545,
      0.0071453345,
      -0.0273914281,
      -0.0625776798,
      0.0372559614,
      -0.1489740908,
      -0.0871566236,
      -0.0188726429,
      -0.0547093004,
      0.0080323359,
      -0.0576230735,
      0.0068720598,
      0.0103685223,
      0.0458622016,
      -0.0732704327,
      -0.0148392748,
      -0.0333546326,
      -0.012560701,
      0.000036403,
      0.1371491104,
      -0.0035463816,
      0.0382279418,
      0.0025420391,
      -0.0189337172,
      0.0734171793,
      0.0518859401,
      -0.0065480522,
      -0.0705836043,
      -0.0056857159,
      -0.080479078,
      0.1365180314,
      0.0429978557,
      -0.0409814939,
      -0.0674665645,
      -0.0141215855,
      0.0410887003,
      -0.0100132097,
      -0.049646724,
      -0.0478252769,
      -0.0763648376,
      -0.0668162629,
      0.0152486321,
      0.0678909123,
      0.0727241337,
      0.0096554952,
      0.0644109398,
      -0.0530483536,
      -0.039548371,
      0.0470589735,
      -0.0817219317,
      0.011436834,
      0.02312438,
      -0.044308275,
      0.0049961125,
      0.0792893171,
      0.0472562946,
      0.0224698465,
      0.0431410037,
      -0.0128501346,
      -0.0220465995,
      0.0157606509,
      0.0613206066,
      0.0123751583,
      7.222772876e-33,
      0.1025476009,
      0.0658345744,
      0.0256702211,
      0.0135188261,
      -0.0170559995,
      0.0016888489,
      0.131308496,
      0.0126713654,
      -0.0484796502,
      -0.0656147003,
      -0.0702062324,
      0.127665326,
      -0.0269458983,
      -0.0373718925,
      -0.0008878311,
      -0.0021430252,
      -0.0369400382,
      0.0703986064,
      0.0690619722,
      -0.0173529964,
      -0.0105078435,
      -0.0537456833,
      0.0741229355,
      -0.0061056698,
      0.0422294773,
      0.0188632905,
      -0.0427454188,
      -0.0521213822,
      -0.0212953966,
      -0.0118052932,
      0.0321427919,
      0.0115246316,
      -0.0468339175,
      -0.0021272616,
      0.0254603978,
      -0.0046473369,
      -0.0123511413,
      0.0292010792,
      -0.058297433,
      0.0102526601,
      -0.020369567,
      0.0193834957,
      0.0335763581,
      -0.1024452373,
      -0.0394283719,
      -0.0255576856,
      -0.1299384236,
      0.0165486112,
      0.1401886642,
      0.007220462,
      0.031200109,
      -0.025684474,
      -0.0386140756,
      -0.0034253213,
      -0.0153357405,
      -0.0551865883,
      -0.0187884234,
      -0.0230916347,
      0.0163856912,
      -0.0406861454,
      -0.0292427205,
      -0.0416666418,
      0.0308595952,
      -0.0124591989,
      -0.0288390443,
      -0.0571701154,
      0.031929262,
      0.0449753813,
      -0.0111959567,
      -0.0096047474,
      0.0420765728,
      0.0343854167,
      0.0397954509,
      -0.0437176898,
      -0.0477166213,
      -0.0393279791,
      -0.0150907664,
      -0.0339016616,
      0.0520792976,
      0.0274471082,
      0.0597978979,
      -0.052356787,
      0.0057556899,
      -0.0342657566,
      0.0113435974,
      0.0237454008,
      0.0043705399,
      -0.0472153015,
      0.0508352667,
      0.0052439948,
      -0.0359413996,
      -0.0216890723,
      -0.0899655223,
      0.018212134,
      -0.0233502574,
      -6.570336777e-33,
      0.0481689945,
      -0.0848324597,
      0.0395644009,
      -0.0407857895,
      0.0356656052,
      0.0293683391,
      -0.0046942048,
      0.0205754358,
      0.0053387019,
      0.0747143626,
      -0.0452332795,
      -0.0044740713,
      0.0732465535,
      -0.0077613606,
      0.0135011841,
      0.0349004827,
      -0.004175554,
      -0.0127445031,
      0.0634887815,
      0.0097720977,
      -0.0668403506,
      0.0111265937,
      -0.0773507953,
      -0.0169807188,
      0.0543441139,
      -0.0256789699,
      -0.0058596893,
      0.0361990668,
      -0.0662405714,
      -0.0819644481,
      0.0232452638,
      0.0274532326,
      -0.127082184,
      0.0389898457,
      -0.0053944718,
      0.0327231549,
      0.0322210006,
      0.0067599178,
      0.0121784173,
      -0.0006297978,
      0.0874074325,
      -0.0870196223,
      -0.0643574297,
      -0.0235192943,
      0.0113304034,
      -0.0006834252,
      -0.0971267745,
      0.0791666582,
      -0.1197628602,
      0.0375240557,
      0.0828037485,
      -0.003016535,
      -0.00148095,
      -0.059127558,
      -0.0065121762,
      -0.0029589734,
      0.0296129994,
      0.0198425055,
      -0.1200462803,
      -0.0681470782,
      0.0075048157,
      -0.0964048207,
      -0.024656482,
      0.1147140861,
      -0.0356332436,
      -0.0374349393,
      0.0427170284,
      0.0325611755,
      -0.1230260506,
      -0.0652525723,
      -0.0396574959,
      -0.0921725258,
      0.0106370868,
      0.0086032115,
      0.051212389,
      0.0397716761,
      0.0562038608,
      -0.0026140355,
      0.0516392849,
      0.0369438678,
      -0.0432643592,
      0.0098884366,
      0.076644145,
      0.0204739794,
      0.0360081196,
      0.0392558016,
      0.027035173,
      0.0637013987,
      0.0240169577,
      0.0537243411,
      -0.0687064528,
      0.0287830811,
      0.0062605832,
      0.0895397142,
      0.1071775556,
      -0.000000051,
      -0.0659460276,
      -0.0386866182,
      -0.0895886347,
      0.118931368,
      -0.0822451487,
      0.093769215,
      -0.001733809,
      0.1117275208,
      -0.001604878,
      -0.027865354,
      0.0090315714,
      -0.0313343406,
      -0.0592529178,
      0.0254828222,
      -0.0334313624,
      0.0807982981,
      0.0183713324,
      -0.0133365868,
      0.0470914356,
      -0.0739236847,
      0.0064331722,
      0.032428734,
      0.0910708755,
      0.0003989166,
      -0.0173418429,
      -0.0180675518,
      -0.0605022199,
      0.0682305768,
      -0.0682553872,
      -0.0258274078,
      -0.0060385102,
      -0.0440990292,
      0.0084928507,
      -0.0516710207,
      0.0281963497,
      0.0064697997,
      0.0061875754,
      -0.0254118238,
      -0.0484798104,
      0.0399900451,
      0.0901295096,
      0.1181079075,
      -0.0178491399,
      -0.0729879662,
      0.0105144605,
      0.0670316964,
      -0.0438436754,
      -0.070568867,
      0.0445286818,
      0.003045103,
      -0.0012146422,
      -0.0390707888,
      0.0814031735,
      0.0813267753,
      -0.0652940497,
      0.048386924,
      -0.0141715929,
      -0.0264547132,
      0.0555970035,
      0.0806812569,
      0.067899771,
      0.083520934,
      0.0163485855,
      0.0663105771
    ],
    "cluster":2,
    "time":"00:01:40"
  },
  {
    "id":6,
    "url":"https:\/\/energy-based-transformers.github.io\/static\/pdfs\/paper.pdf",
    "content":"Energy-Based Transformers are Scalable Learners and\nAlexi Gladstone1,2, Ganesh Nanduru1, Md Mofijul Islam1,3, Peixuan Han2,\nHyeonjeong Ha2, Aman Chadha3,4, Yilun Du5, Heng Ji3, Jundong Li1, Tariq Iqbal1\n1UVA 2UIUC 3Amazon GenAI\u2020\n4Stanford University\n5Harvard University\n(cid:128) energy-based-transformers.github.io \u00a7 github.com\/alexiglad\/EBT\nInference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like math\nand coding), or require additional supervision\/training on top of unsupervised\npretraining (e.g., verifiers or verifiable rewards). In this paper, we ask the question\n\u201cIs it possible to generalize these System 2 Thinking approaches, and develop models\nthat learn to think solely from unsupervised learning?\u201d Interestingly, we find the\nanswer is yes, by learning to explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs)\u2014a new class of Energy-Based Models (EBMs)\u2014to assign an energy (un-\nnormalized probability) value to every input and candidate-prediction pair, enabling\npredictions through gradient descent-based energy minimization until convergence.\nThis formulation enables System 2 Thinking to emerge from unsupervised learn-\ning, making it modality and problem agnostic. Across both discrete (text) and\ncontinuous (visual) modalities, we find EBTs scale faster than the dominant Trans-\nformer++ approach during training, achieving an up to 35% higher scaling rate with\nrespect to data, batch size, parameters, FLOPs, and depth. During inference, EBTs\nimprove performance with System 2 Thinking (i.e., extra computation) by 29%\nmore than the Transformer++ on language tasks, and EBTs outperform Diffusion\nTransformers on image denoising while using fewer forward passes. Further, we\nfind that System 2 Thinking with EBTs yields larger performance improvements\non data that is farther out-of-distribution, and that EBTs achieve better results than\nexisting models on most downstream tasks given the same or worse pretraining\nperformance, suggesting that EBTs generalize better than existing approaches.\nConsequently, EBTs are a promising new paradigm for scaling both the learning\nand thinking capabilities of models.\nIn psychology, human thinking is often classified into two different types: System 1 (thinking fast) and\nSystem 2 (thinking slow) [1\u20134]. System 1 thinking is characterized by quick, intuitive and automatic\nresponses, relying on previous experience to solve simple or familiar problems. Alternatively, System\n2 Thinking is slow, deliberate and analytical, requiring conscious effort and logical reasoning to\nprocess more complex information. System 2 Thinking is essential for complex problems that go\nCorrespondence to Alexi Gladstone: # alexigladstone@gmail.com. \u2020Work does not relate to position at\nPreprint. Under review.\n(a) AR Transformer\n(c) Diffusion Transformer\nExisting Autoregressive Approaches\nFigure 1: Autoregressive Architecture Comparison. (a) Autoregressive (AR) Transformer is the\nmost common, with (b) RNNs becoming more popular recently [22, 23]. (c) Diffusion Transform-\ners [26] (which are often bidirectional but can also be autoregressive) are the most similar to EBT,\nbeing able to dynamically allocate computation during inference, but predict the noise rather than the\nenergy [27, 28]. Consequently, diffusion models cannot give unnormalized likelihood estimates at\neach step of the thinking process, and are not trained as explicit verifiers, unlike EBTs.\nbeyond automatic pattern recognition, such as in mathematics, programming, multistep reasoning, or\nnovel out-of-distribution situations [5, 6], where precision and depth of understanding are important.\nAlthough current models perform well on tasks suitable for System 1 thinking [7], they continue to\nstruggle with tasks that demand System 2 capabilities [8\u201310].\nConsequently, the recent pursuance of System 2 Thinking capabilities has become a growing interest\namong AI researchers, leading to the rise of several foundation models such as O1 [11], R1 [12],\nGrok3 [13], and Claude 3.7 Sonnet [14]. These \u201creasoning models\u201d excel on math and coding\nbenchmarks, particularly by increasing the time models spend thinking. However, publicly available\ninformation on training methods, particularly from the open-source R1 model [12], suggests that\nthe Reinforcement Learning (RL) based training approach for these models only works in domains\nwhere rule-based rewards can easily verify answers, such as math and coding. This limitation reduces\napplicability to a small range of problem types, and as a consequence, often deteriorates performance\nin other tasks such as writing [15\u201317]. Further, recent evidence suggests this RL-based approach\nmay not induce new reasoning patterns, but rather just increase the probability of reasoning patterns\nalready known to the base model [18], which limits performance on problems requiring exploration.\nAlong similar lines, there has been strong interest in achieving System 2 Thinking in both diffusion\nmodels and Recurrent Neural Networks (RNNs). Diffusion models support iterative inference\nthrough denoising steps, where increasing denoising steps can improve performance. However,\nthey typically fail to benefit from denoising steps beyond what they were trained on [19], and aside\nfrom increasing denoising steps, diffusion models require an external verifier to improve System 2\nThinking capabilities [19\u201321]. RNNs also offer iterative computation via recurrent state updates [22\u2013\n24], however, most modern RNNs only update their internal state with new information, meaning\nthey cannot be used for thinking longer. Additionally, RNNs that do support recurrent depth still lack\nmechanisms for explicit verification [25], resulting in limited adoption for System 2 Thinking.\nAs one of the primary goals of AI is to figure out how we can create systems that learn to think\non their own on any type of problem, these approaches ultimately bring about the following core\nresearch question: \u201cCan we rely entirely on unsupervised learning to develop System 2 Thinking?\u201d\nSuch a capability would enable generalization of current System 2 Thinking approaches to any\nproblem, any modality, and avoid the reliance on external human, reward, or model supervision.\nIn this work, we argue and demonstrate empirically that the answer to this core research question\nis yes, but that there are several limitations in existing model architectures that prevent this type\nof unsupervised System 2 Thinking from emerging. Particularly, when comparing the qualities of\nhuman System 2 Thinking with the current modeling paradigms (Figure 1, Table 1), we observe\nseveral key differences, outlined below as three key Facets of System 2 Thinking1:\n1We acknowledge that these are not comprehensive for achieving System 2 Thinking, but rather, a good first\nstep (more info is in Section C.1).\nTable 1: Architectures and Cognitive Facets. For each prediction, Feed Forward (FF) Transformers\nand RNNs generally2 have a finite amount of computation. While diffusion models have potentially\nmore computation during inference by increasing the number of denoising steps, they do not learn\nto explicitly verify or estimate uncertainty for predictions. EBMs can use a dynamic amount of\ncomputation during inference by iterating for any number of steps, and give an energy scalar that can\nbe used to evaluate uncertainty and verify the strength of predictions.\nFF Transformers\nDiffusion Transformers\nDynamic Compute\nAllocation (Facet 1)\nUncertainty (Facet 2)\nVerification (Facet 3)\nFacet 1: Dynamic Allocation of Computation. Humans naturally allocate varying amounts of\neffort to different tasks depending on difficulty, which is widely supported by psychology and\nneuroscience [1, 31, 32]. As the difficulty of tasks humans face varies widely, the ability to adjust\nthe magnitude of computational resources allocated towards a task is fundamental to success.3 For\nexample, a decision regarding whether to change careers generally takes people much more time than\ndeciding what to eat for lunch.\nFacet 2: Modeling Uncertainty in Continuous State Spaces. While thinking longer is important\nfor improving performance, humans also weigh how uncertain they are before committing to a\ndecision. In language, LLMs can simulate this through token-level probabilities [33]. In the context\nof continuous state spaces, such as in vision, without the usage of discretization schemes such as\nVector Quantization [34] or pseudo losses\/objectives (such as ELBO [35]), standard implementations\nof the most successfully used approaches with Transformers, RNNs, or Diffusion models generally do\nnot provide strong or reliable uncertainty estimates [36\u201339].4 EBMs can naturally model uncertainty\nwithout having to model exact likelihoods by modeling the relative unnormalized likelihoods of\npredictions [42], as demonstrated in Figure 3. As the real world often contains many inherently\nunpredictable elements, for instance, when a pedestrian might emerge from behind a parked vehicle,\nthe ability to express uncertainty in predictions is essential to being cautious, and is a natural capability\nof humans [43\u201345].\nFacet 3: Verification of Predictions. In addition to allocating computation and modeling uncertainty,\neffective thinking also benefits from the ability to verify predictions, which can guide decisions about\nwhen to stop thinking or to select the most accurate predictions. There is strong evidence that such\nverification is a core component of human thinking [46, 47]. In fact, it can be shown that verifying\nsolutions is exponentially easier than generating solutions [48], which means that verifiers can often\ngeneralize better than explicit generators [48]. Training an explicit verifier means that at each step of\nthe thinking process an estimate of the current prediction\u2019s quality can be extracted. This supports\nmore dynamic inference time behavior such as early stopping when a prediction is known to be\ncorrect or allocating more compute when a problem is difficult, which can intuitively be seen as\nadjusting resources according to the difficulty of a problem [1]. Training an explicit verifier also\nallows for capabilities such as Monte Carlo Tree Search [49] or sampling many times and choosing\nthe best prediction, which traditionally have involved training new models on more data [19, 50].\nFor more information on additional Facets, please refer to Section F.\n2Recent works attempt to enable dynamic computation per prediction [25, 29, 30], but these approaches\nare generally not modality agnostic and have not been widely adopted. While RNNs can theoretically support\ndynamic computation, they are typically updated only with new state information, except in specialized cases\nsuch as in [25].\n3It\u2019s important to note that we refer to this dynamic computation capability at the granularity of each\nprediction being made, meaning current LLMs built with traditional AR transformers\/RNNs cannot dynamically\nallocate compute per token generated as they have a finite depth\/width and are only updated with new text tokens.\nPlease check Section G for more information.\n4We acknowledge that there are approaches to achieve uncertainty with the models discussed, such as Mixture\nDensity Networks [40] as well as score-based diffusion models [41]. However, these approaches have seen less\nwidespread success and scalability than the current dominant approaches.\nFigure 2: EBT for Autoregressive Modeling. Each blue box corresponds to a different prediction\nbased on the current step of the thinking process, where the initial prediction starts as random. At\neach step, a new prediction is fed into the model, which gives an energy scalar for the prediction\u2019s\ncurrent compatibility (unnormalized likelihood) with the context (Facets 2 and 3). Then, the gradient\nof this energy with respect to the prediction is calculated and used to update the prediction. This\ngradient descent update is done iteratively to refine the prediction until convergence of the predicted\nenergy, which allows for dynamic use of computation (Facet 1).\nTo achieve all facets described, we propose viewing thinking as an optimization procedure with\nrespect to a learned verifier, which evaluates the compatibility (unnormalized probability) between an\ninput and candidate prediction (Figure 2). More concretely, we train Energy-Based Models (EBMs) to\nlearn an energy (unnormalized probability) landscape over all possible input-prediction pairs, where\nlower energy indicates higher compatibility. Then, thinking corresponds to starting from an initial\nrandom prediction, and progressively refining it through energy minimization until convergence\n(visualized in Figure 3). Optimizing over a learned energy landscape naturally allows for dynamic\ncompute allocation (Facet 1), allowing models to think for longer on harder problems by iteratively\nperforming more steps. Additionally, at each step of this thinking process, EBMs act as verifiers\nin the forward pass, giving an energy scalar which represents the compatibility of the context with\nthe prediction. This energy scalar directly addresses Facets 2 and 3 by serving as an unnormalized\nlikelihood as well as the score that verifies whether a prediction is correct (Figure 3).\nWhile viewing thinking through the lens of inference-time energy minimization is a promising\nperspective, EBMs have traditionally struggled with scalability [51], with zero publicly known\nFoundation EBMs. This can be attributed to issues with EBM training stability [51\u201354] and long\ntraining times [53, 54]. To address these challenges and establish a scalable EBM training paradigm,\nwe introduce Energy-Based Transformers (EBTs), Transformer implementations specifically for\nEBMs. We release two variants: a decoder-only EBT inspired by the GPT architecture [55], which\nparallelizes all predictions simultaneously; and a bidirectional EBT, enabling bidirectional attention\nacross entire sequences, similar to BERT [56] and Diffusion Transformers (DiT) [26].\nAdditionally, we identify practical enhancements that improve the training efficiency of EBTs and\nprovide theoretical insights into why our optimization-based training approach achieves strong\nscalability. Finally, we introduce energy landscape regularization techniques, which are methods that\nenhance the convexity and smoothness of learned energy landscapes, thereby fostering the emergence\nof strong System 2 Thinking capabilities during pretraining.\nTo investigate the learning and thinking scalability of EBTs, we compare EBTs to the Transformer++\nfor autoregressive modeling and the Diffusion Transformer (DiT) for bidirectional modeling, across\nboth discrete and continuous modalities. During pretraining, we find that EBTs achieve an up to\n35% higher scaling rate than the Transformer++ across several axes, including data, batch size,\nparameters, FLOPs, and depth. Notably, EBTs are the first approach to out-scale the standard\nfeed-forward Transformer++ recipe across several modalities and axes, including improved data\nefficiency. At inference, EBTs outperform existing paradigms on System 2 Thinking, or solving\nmore challenging problems with additional computation. For example, EBTs can improve language\nmodeling performance 29% more than the Transformer++, and for image denoising EBTs exhibit\na higher performance-compute scaling rate and improved performance over DiTs with 99% fewer\nforward passes. In investigations of the effect of System 2 Thinking on generalization, we consistently\nobserve two phenomena. First, with the same or worse pretraining performance, EBTs still outperform\nFigure 3: Thinking Process Visualization. A learned energy landscape and its optimization\nthrough gradient descent, interpreted as a thinking process. In this example, the model predicts a\ndistribution over text tokens, progressively shifting from an initial random distribution toward the\ntarget distribution. At each step, the EBM assigns an energy scalar indicating how compatible the\ncurrent prediction is with the context, visualized as the landscape\u2019s height (Facet 3). This scalar\u2019s\nconvergence allows the model to determine whether the prediction is adequate or if further thinking\nis necessary. Uncertainty (Facet 2) can be represented by landscapes that are harder to optimize or by\nlandscapes with many local minima, allowing the model to know when it requires more steps to think\n(Facet 1). Adapted from [57].\nexisting models at inference by performing System 2 Thinking, demonstrating its importance and\nhow EBTs often generalize better than existing models. Second, System 2 Thinking with EBTs\nyields more substantial performance gains on data that is further out-of-distribution, aligning with\nobservations in psychology where humans use System 2 Thinking on challenging, unseen tasks.\nWe believe the EBT implementations, along with novel techniques for EBMs to maximize the learning\nand thinking scalability, will advance the EBM paradigm by addressing key challenges in stable,\nparallelizable, and efficient training.\n2 Energy-Based Transformers (EBT) Intuition\nEnergy-Based Models (EBMs) learn an energy function that assigns a scalar value to each input\nconfiguration, with lower energy indicating higher compatibility or likelihood between input vari-\nables [51], and high energy indicating lower compatibility or likelihood. Accordingly, the energy\nfunction acts as a verifier of input data coherence. Leveraging this principle, Energy-Based Trans-\nformers (EBTs) are trained to verify the compatibility of a given context-prediction pair (give an\nenergy value), and then make predictions by optimizing with respect to this verifier (minimizing\nthe energy), which is shown in Figure 2. Starting from an initial prediction, such as random noise,\nEBTs iteratively refine their output by progressively minimizing the energy, ultimately converging to\npredictions that are consistent with the given context. Performing energy minimization through this\nprocess simulates the thinking process during pretraining, unlike with traditional models, enabling\neach prediction (e.g., a token for LLMs) to have its own thinking process.\n2.1 Learning to Verify\nVerifying solutions is often substantially more tractable than generating them, a distinction well-\nestablished in complexity theory and foundational to developments in knowledge proofs and learning\nalgorithms [58\u201360]. For example, in solving a maze, verifying the correctness of a given path is\nsignificantly easier than discovering such a path. This asymmetry has been recognized and utilized\nfor several decades, notably in the field of cryptography [59, 61, 62]. EBMs are built on this principle\nthat verification is easier than generation: rather than learning to generate directly, as in most existing\nparadigms, EBMs learn to generate by optimizing predictions with respect to this learned verification\n(energy function); this is depicted in Figure 3.\nRecent works have attempted to leverage this characteristic of verifiers [19, 63\u201365], but these\napproaches decouple the verifier and generator, resulting in adversarial dynamics [19] and challenges\nin scalability [64]. For example, researchers combining tree search and LLMs required thousands\nor even millions of samples to achieve optimal performance [63]. In contrast, EBMs combine the\nverifier and generator into a single model, where the generator is defined implicitly by the gradient\nof the verifier [51]. We show that this coupling resolves scalability and adversarial issues (Figures 6b\nAn additional advantage of verifiers is generalization. Because verification is usually easier than gen-\neration [66], prediction verification on Out-Of-Distribution (OOD) data is often easier than explicit\nprediction generation for OOD data [48]. This characteristic often results in better generalization of\nverifiers than explicit generators [48]. As an example, models trained to explicitly solve mazes on a\nsmall grid may not generalize to larger grids, whereas verifiers for maze solutions learn whether a\npath is correct, which more easily transfers to larger mazes. This characteristic may be why EBMs\noften generalize better than existing models [48, 67], which we further support in our larger-scale\nexperiments (Figure 6a and Table 4).\n2.2 Learning to Understand\nThis verifier-centric perspective also relates to a deeper limitation in contemporary generative models,\nreferred to as \u201cThe Generative AI Paradox [68].\u201d Although current generative models achieve strong\ngenerative capabilities, they frequently lack basic discrimination skills, such as the ability to assess\nthe plausibility or coherence of their own predictions [68, 69]. These limitations impede their ability\nto engage in reasoning, planning, and decision-making [9, 70]. In contrast, EBMs offer a potential\nsolution to this challenge: as EBMs generate by learning a verifier (which is conceptually similar to a\ndiscriminator), they develop strong discrimination skills [71]. Experimental results further support\nthis observation (Table 4).\n3 Energy-Based Transformers (EBT) Approach\n3.1 Energy-Based Models (EBM) Background\nEnergy-Based Models (EBMs) assign a scalar energy value to each configuration of input variables,\nenabling them to model the compatibility and interactions between variables, such as between\na context and candidate-prediction. In the case of probabilistic EBMs, this defines a probability\nZ(\u03b8) where Z(\u03b8) = (cid:82) e\u2212E\u03b8(x)dx is the\ndistribution using a Boltzmann distribution p\u03b8(x) = e\u2212E\u03b8 (x)\nintractable partition function involving an integral over all possible values of x. Due to the negative\nexponential, lower energy corresponds to higher probability, and higher energy corresponds to\nlower probability. To avoid the intractability of the partition function, it is common to work with\nunnormalized EBMs, which dispense of the partition function in favor of representing relative\nunnormalized probabilities. This formulation shifts the focus from addressing the partition function,\nto simply assigning low energy to the true data manifold and high energy elsewhere [42, 51], offering\nbenefits such as scalability to spaces where the true data manifold is thin and therefore a probabilistic\nEBMs would have an infinite score [42]. In supervised or predictive self-supervised learning (e.g.,\nclassification, autoregressive modeling, masked modeling), unnormalized EBMs can be formulated as:\np\u03b8(x, \u02c6y) \u221d e\u2212E\u03b8(x,\u02c6y), where the goal of the EBM is to learn to predict \u02c6y given x.5 For an accessible\nand beginner-friendly introduction to EBMs, including intuitive explanations and pseudocode, please\nrefer to Section H.\n3.2 Scalable EBM Learning\nWhile EBMs offer a flexible modeling framework, training them scalably remains an open research\nproblem. Two primary training approaches exist\u2014contrastive and regularized methods [72]. Con-\ntrastive methods increase the energy of negative samples while decreasing the energy of positive\nsamples. Due to the curse of dimensionality, where the volume of spaces grows exponentially with\n5In the case of self-supervised learning, x is some unmasked portion of the original x and \u02c6y is the masked\nAlgorithm 1: Training\nInputs: Context x, Target y, EBM E\u03b8(x, \u02c6y)\nHparams: Steps N , Step Size \u03b1, Loss J(\u00b7)\nAlgorithm 2: Inference with Verification\nInputs: Context x, EBM E\u03b8(x, \u02c6y)\nHparams: Steps N , Step Size \u03b1, Samples M\n1 Sample \u02c6y0 \u223c N (0, I);\n2 for i = 0, . . . , N \u2212 1 do\n4 L \u2190 J(\u02c6yN , y);\n5 return L, update E\u03b8;\n\u02c6yi+1 \u2190 \u02c6yi \u2212 \u03b1\u2207\u02c6yi E\u03b8(x, \u02c6yi);\n1 for j = 1, . . . , M do\nSample \u02c6y0,j \u223c N (0, I);\nfor i = 0, . . . , N \u2212 1 do\n\u02c6yi+1,j \u2190 \u02c6yi,j \u2212 \u03b1\u2207\u02c6yi,j E\u03b8(x, \u02c6yi,j);\n5 return \u02c6y\u2217 = argminj E\u03b8\n(cid:0)x, \u02c6yN,j\ntheir dimension, contrastive methods struggle to scale because they must increase the energy of an\nexponentially higher number of negative samples [42].\nAn alternative is to frame EBM learning as an optimization problem [48, 71], which avoids the curse\nof dimensionality by implicitly regularizing the energy landscape, enabling scalable learning. In this\napproach, EBMs are trained to optimize an initial prediction to the ground truth solution through\ngradient descent, as shown in Figure 3. This pushes the energy landscape to be convex surrounding\nthe ground truth solution, thereby regularizing the energy landscape to only have low energy on the\ntrue data manifold. Intuitively, this training approach is similar to GANs [73]. During the forward\npass, EBMs can be seen as a GAN discriminator by giving an energy \u201cverification\u201d; on the backward\npass they can be seen a GAN generator by optimizing predictions through energy minimization to try\nand fool the discriminator.\nTraining EBMs to perform optimization can be formalized as follows. We begin with an EBM E\u03b8,\na prior (initial prediction) \u02c6y0, an input (context) for the model x, and seek to predict y. We aim to\nfind the minimum energy (most compatible\/likely) \u02c6y given an x, which we search for using gradient\n\u02c6yi+1 = \u02c6yi \u2212 \u03b1\u2207\u02c6yiE\u03b8(x, \u02c6yi),\nwhere \u03b1 is the step size (this is formalized in Algorithm 1). Then, the loss can be computed using\nany standard objective function. For this work, we use the same loss functions as existing papers\nto simplify experiments, so we use categorical cross-entropy for language modeling [74] and mean\nsquared error for image denoising [75]. Importantly, this loss is backpropagated through the entire\noptimization process, requiring second-order derivatives (i.e., gradients of gradients). These are\ncomputed efficiently via Hessian-vector products, which scale linearly with model size [76], similar\nto standard first-order backpropagation in feed-forward models. More details and pseudocode can be\nfound in Section I.2.\n3.3 Scalable EBM Thinking\nWhile this training approach is scalable, achieving smooth and convex energy landscapes, such as the\nlandscape visualized in Figure 3, remains challenging on real-world problems. Because y is high-\ndimensional, the energy landscape spans a high-dimensional space, and must remain well-shaped\nthroughout. To address this, we found three key energy landscape regularization techniques to be\nessential in ensuring the smoothness and convexity of learned energy landscapes, enabling strong\nthinking capabilities to be learned during training.\nFirst, we found a replay buffer (following existing EBM works [48, 51]) helps simulate longer\noptimization trajectories, enabling energy landscapes to be well defined near their minimum. Second,\na variant of Langevin Dynamics [51] (random noise), was found to be helpful for encouraging\nexploration of the energy landscape:\n\u02c6yi+1 = \u02c6yi \u2212 \u03b1\u2207\u02c6yiE\u03b8\nwhere \u03c3 is the magnitude of the noise \u03b7. Without this random noise term, exploration is often limited\nto paths leading directly to the energy minimum, leaving other regions poorly defined. Third, varying\nthe paths taken towards predicting solutions, by randomizing the gradient descent step size \u03b1 and\nnumber of optimization steps, significantly improved generalization. Together, these techniques\nsubstantially improved the System 2 Thinking capabilities of models, as confirmed by ablation\nexperiments in Table 2.\nWith these energy landscape regularization techniques established, to understand the System 2\nThinking capabilities of EBTs, we explored two main thinking approaches, corresponding to two of\nthe cognitive facets described. First, corresponding to dynamic computation allocation (Facet 1), we\nconduct experiments that involve changing the number of steps taken for optimization of a single\nprediction. This is conceptually similar to increasing the denoising steps performed with a diffusion\nmodel. Second, corresponding to the ability to verify predictions (Facet 3), we generate N predictions\nfrom an EBM and then choose the minimum energy prediction. This is conceptually similar to\nBest of N (BoN) sampling using language models [77]. However, EBMs generalize this approach\nto both discrete and continuous modalities, don\u2019t require additional supervision (e.g., an external\nreward\/verification model), and perform it on every single prediction, not just to entire sequences. We\ndemonstrate performance improvements gained from both of these techniques in several Thinking\nexperiments (Figures 6, 7, and 12), which further confirm the importance of the described cognitive\nfacets. This thinking process is formalized in Algorithm 2 and we more formally define and justify\nusage of the term thinking in Section C.1.\n3.4 Energy-Based Transformers (EBTs) Architecture\nTransformers have demonstrated exceptional performance across numerous domains [12, 78\u201381].\nThree primary advantages of Transformers include their parallelizability [82, 83], their stability [84],\nand their scalability [85]. Because Energy-Based Models (EBMs) have encountered difficulties with\nall three of these characteristics [51\u201354], Transformers represent a particularly suitable architecture\nfor scaling EBMs. Consequently, to advance the EBM paradigm, we introduced Energy-Based\nTransformers (EBTs), which are Transformer implementations designed for EBMs. We developed\ntwo variants: a decoder-only EBT, inspired by the GPT architecture [55] for autoregressive modeling,\nas well as a bidirectional EBT with bidirectional attention across sequences, enabling capabilities\nsuch as infilling and masked modeling [26, 56]. Although the bidirectional EBT implementation\nis relatively straightforward, the autoregressive EBT presents greater implementation challenges,\nprimarily due to the potential for information leakage in na\u00efve implementations. Comprehensive\ndetails of this implementation are discussed in Section C.3.\n4 Experimentation and Results\nWe experiment with EBT across both Autoregressive (AR) [82] as well as Bidirectional models [56]\nin discrete and continuous spaces.6 In discrete spaces, we focus primarily on the language modeling\nobjective. In continuous spaces, we focus on vision tasks of next frame prediction and image\ndenoising. We perform the most comprehensive experiments using Autoregressive Language Models\ntrained to predict the next token, as this is the most extensively studied pretraining approach [86]. All\nmodels are pretrained from scratch under a tightly constrained compute budget, as the architecture of\nEBTs is incompatible with existing foundation models, making them incompatible for adaptation\nvia fine-tuning. We focus on two primary types of results. First, we examine learning scalability,\ninvestigating how quickly models can fit the pretraining data, which is standard in the pretraining\nliterature [22, 85\u201390].\nSecond, we study thinking scalability, or how the performance of models changes as we scale\nthe System 2 Thinking of models (Definition C.1). To measure the thinking scalability, we use the\nNumber of Function Evaluations (NFEs) [19, 91], which we deem to be one for every forward pass\ncompleted (for EBMs this is one function evaluation per optimization step). By scaling the number\nof forward passes during inference, we can determine whether model performance improves with\nincreased thinking.\n4.1 Autoregressive Language Modeling Experiments\nIn this section, we detail and discuss the results for all Natural Language Processing (NLP) experi-\nments using Autoregressive (AR) Language Models trained to predict the next discrete token in a text\nsequence [82]. All language models are pretrained on the RedPajamaV2 text corpus [92, 93] 100B\nsample from HuggingFace using the GPT-NeoX tokenizer [94] (following [22]) to predict the next\ntoken. Following existing pretraining work, we compare AR EBT with the standard Transformer++\nrecipe [22, 87, 95]. We manually created a training and validation split of 66 million and 33 thousand\nsamples, respectively.\n6Here, Autoregressive and Bidirectional refer to the procedure for generation. It\u2019s worth noting that autore-\ngressive models are compatible with bidirectional attention, as in [27].\n(a) Scaling for data.\n(b) Scaling for batch size.\n(c) Scaling for depth.\nFigure 4: Language Learning Scalability\u2014Data, Batch Size, and Depth. A comparison between\nthe scaling of the Transformer++ recipe [87] and EBTs across data, batch size, and depth during\npretraining. On all of these axes, EBTs out-scale the Transformer++ recipe significantly, indicating\nimproved data efficiency and suggesting potential benefits for generalization. Additionally, the\nimproved depth scaling offers promise for reasoning, where depth is crucial [96]. These results\nsuggest that if these scaling trends persist, EBTs would likely outperform Transformer++ models at\nfoundation model data scale.\nFor downstream evaluation, we used four key datasets in addition to the pretraining dataset, spanning\nreasoning, question answering, and syntax understanding. Ordered roughly by increasing perplexity\ndifficulty, these include GSM8K [97], SQuAD [98], BigBench Elementary Math QA [99], and Big-\nBench Dyck Languages [99]. We intentionally design the evaluation towards reasoning benchmarks\ndue to their alignment with System 2 Thinking. Additionally, we focus on reporting perplexity as our\nrelatively small models trained from scratch, when compared to current foundation models, do not\nachieve high accuracies on many of these benchmarks. Furthermore, perplexity often functions as\na more linear metric than accuracy [22, 100], enabling a more comparable analysis of downstream\nperformance as we scale compute during inference. Further details on the exact hyperparameters and\nsetup used for all experiments are in Section D.\n4.1.1 Autoregressive Language Model Learning Scalability\nThe scaling trends related to the learning speed of models, commonly referred to as \u201cscaling laws,\u201d\nare challenging to measure. For example, a recent survey [86] found that the \u201cscaling laws\u201d observed\noften depend on several implementation details and axes to measure, which can result in several\ndifferent conclusions.7 Therefore, to be as comprehensive as possible in determining how EBTs scales\ncompared to the Transformer++, we conduct scaling experiments for six different axes\u2013including\ndata, batch size, depth, parameters, FLOPs,8 and embedding dimension. The results for the data,\nbatch size, and depth scaling are shown in Figure 4; and the results for parameters, FLOPs, and\nembedding dimension are visualized in Figure 5. Across all axes EBT consistently out-scales (has\na higher scaling rate) the Transformer++ recipe, becoming the first model to achieve such a feat\nwithout using a different tokenizer ([101] was the first and only work to our knowledge to out-scale\nthe Transformer++ recipe, but they use a different tokenizer, and do not out-scale the Transformer++\nacross multiple axes unlike EBTs). These results suggest that EBTs are more data efficient, batch size\nefficient, parameter efficient, depth efficient, and compute efficient than the Transformer++ recipe.\nThus, at the scale of modern foundation models trained on 1, 000\u00d7 more data with models 1, 000\u00d7\nlarger (following [84]), we expect the pretraining performance of EBTs to be significantly better than\nthat of the Transformer++ recipe.\n4.1.2 Autoregressive Language Model Thinking Scalability\nBuilding on the learning results, we investigate EBTs for thinking at inference time. We found that\nthe thinking capabilities of EBT emerge with a sufficiently large data scale, and therefore, due to\nlimited resources, we focus on conducting thinking experiments with smaller models trained on\nsubstantial amounts of data. We test thinking capabilities along two axes: thinking longer, which\ndenotes more optimization steps, and self-verification, which denotes generating many candidate\npredictions and selecting the minimum energy prediction. In Table 2, we conduct ablation studies\nto confirm the benefits of our energy landscape regularization techniques for System 2 Thinking\n7For more information on this perspective please refer to Section D.1.1\n8The FLOP calculation is nuanced and depends on specific hyperparameters. For more information please\nrefer to Section D.5.\n(a) Scaling for number of Parameters.\n(b) Scaling for number of FLOPs.\n(c) Scaling for the embed. dimension.\nFigure 5: Language Learning Scalability\u2014Parameters, FLOPs, and Width. Pretraining scaling\ncomparisons between the Transformer++ recipe [87] and EBTs across model size (parameters),\ncompute (FLOPs), and width (embedding dimension). EBTs slightly out-scale the Transformer++ in\nFLOP and parameter scaling, becoming the first approach to achieve a higher scaling rate without\nmodifying the tokenizer [101] to our knowledge. These results suggest that EBTs offer high promise\nas a pretraining paradigm in both parameter and FLOP efficiency as scale increases.\nTable 2: System 2 Thinking Ablations. All energy landscape regularization techniques described\nin Section 3.3 and their impact on System 2 Thinking performance, measured by percent perplex-\nity improvement. Thinking Longer denotes more optimization steps and Self-Verification denotes\noptimizing many predictions and choosing the best. Bolded highlights the default System 2 Hyperpa-\nrameters, leveraging all energy landscape regularization techniques described. This configuration\nresults in the best performance when thinking longer and doing self-verification. Removing regular-\nization, such as Langevin Dynamics, results in less energy landscape exploration, which improves\nsingle path performance (thinking longer) at the expense of self-verification performance.\nThinking Longer \u2191 Thinking Longer and Self-Verification \u2191\nNo Random Step Size\nNo Random Num. Steps\nNo Langevin Dynamics\nNo Replay Buffer\nFull System 2 Configuration\non Out-of-Distribution Data from the BigBench Dyck Languages benchmark [99]. We find that\nusing all techniques yields the best System 2 Thinking performance when combining extended\nthinking and self-verification. Additionally, the results show that randomizing the step size is critical\u2014\nremoving it nearly eliminates Thinking gains, while disabling Langevin Dynamics degrades combined\nperformance but improves results without verification, offering a performance-compute tradeoff.\nHaving established the importance of these landscape regularization techniques, in Figure 6, we\nanalyze the scalability of thinking with EBTs, where the results yield two main insights. First, as\nshown in Figure 6a, EBTs are able to improve performance by as much as 29% by increasing the\namount of forward passes (thinking time), whereas the Transformer++ cannot improve performance\nat all.9 This aligns with our claims that because traditional feed-forward Transformers cannot\ndynamically allocate additional computation for each prediction being made, they are unable to\nimprove performance for each token by thinking for longer.\nSecond, as demonstrated in Figure 6b, the thinking capabilities of EBTs scale, showing that as EBTs\nare trained for longer, their ability to achieve improvements from verification improves, increasing up\nto 10%\u221214% from 4%\u22128%. This suggests that EBTs trained at the same scale as modern foundation\nmodels, such as the 15T tokens Llama3 [84] was trained on (\u2248 1000\u00d7 the current data scale), would\nhave significantly more substantial results from self-verifying. Lastly, we visualize results from EBT\nat representing uncertainty while predicting tokens in Figure 8. The results demonstrate that for easier\nto predict tokens, such as \u201cthe\u201d or \u201cbut\u201d, EBTs optimize to lower energies faster, whereas for harder\nto predict tokens, such as \u201cfox\u201d or \u201cproblem\u201d EBTs have higher energy that does not converge across\n9Because we pretrained language models from scratch, and are unable to train models the size of modern\nfoundation models, we find models did not benefit from inference time techniques such as Chain-of-Thought.\nHowever, we expect both EBT and Transformer++ models to benefit equally from existing techniques.\n(a) OOD Thinking Performance Comparison.\n(b) Verification Capabilities as Scale Increases.\nFigure 6: EBT Thinking Analysis. (a) Mean performance degradation of the standard Transformer++\nrecipe [87] and the Energy-Based Transformer (EBT) on four Out of Distribution (OOD) datasets.\nWhile the Transformer++ cannot reduce perplexity at a per-token level, EBTs can by performing more\nforward passes over a single token\/sample (Thinking Longer) as well as generating many samples\nand choosing the minimum energy one (Self-Verifying\/BoN in addition to longer thought). (b) The\nSelf-Verification capabilities of EBTs using BoN-5 compared to not doing any self-verification. As\ndata scale increases, the benefit from doing self-verification increases. These results suggest EBTs\ngeneralize OOD better than the Transformer++ because of their System 2 Thinking capabilities, and\nthat the thinking capabilities of EBTs scale during training.\nsteps. This suggests that during pretraining EBTs learn to capture uncertainty regarding which tokens\nare harder or easier to predict, achieving Facet 2.\n4.1.3 Autoregressive Language Model Generalization\nAs System 2 Thinking in humans is associated\nwith generalization to novel scenarios, we con-\nduct experiments directly aimed at measuring\nthe effects of System 2 Thinking on generaliza-\ntion. In Figure 7, we visualize the performance\nof EBTs on the datasets described, which have\nvarying levels of Out-of-Distribution (OOD)\nshift (measured as the ratio of downstream task\nperplexity to pretraining perplexity). We ob-\nserve a strong linear trend: as the data becomes\nmore OOD, thinking leads to greater perfor-\nmance improvements. Therefore, these findings\nsuggest that the benefits of EBTs\u2019 thinking are\nnot uniform across all data but scale positively\nwith the magnitude of distributional shifts, high-\nlighting thinking as a critical mechanism for\nrobust generalization beyond training distribu-\ntions. These findings align with observations\nin psychology where humans rely on deliberate\nSystem 2 Thinking to tackle challenging OOD\nFigure 7: OOD Thinking Performance. As the\ndata becomes more OOD, thinking leads to greater\nperformance improvements, with a roughly linear\ntrend. These findings highlight that EBTs thinking\nis especially critical for robust generalization to\nOOD data. Performance is measured on 5 differ-\nent datasets varying in Out-of-Distribution (OOD)\nmagnitude shift, which is measured as the ratio of\ndownstream dataset perplexity to pretraining per-\nplexity. Max Thinking denotes combining thinking\nlonger and self-verification.\nNext, we investigate the relation between OOD\ngeneralization and pretraining performance. Pre-\ntraining performance is strongly correlated with\ndownstream task performance in language mod-\nels [102\u2013104], and can even be a predictor of downstream performance [84, 105, 106]. Because we\nknow that EBTs scale at a faster rate than the Transformer++, as demonstrated in Figures 4 and 5,\nit is reasonable to hypothesize that they may also perform better on downstream tasks at scale. To\ninvestigate this, we compare models with identical training setups, where EBTs have slightly worse\npretraining perplexity than Transformer++ models. As shown in Table 3, despite achieving a higher\nTable 3: Language Model Task Generalization Comparison. Despite exhibiting a slightly higher\npretraining perplexity, EBTs usually achieve lower perplexity on downstream tasks than the Trans-\nformer++. This suggests that EBTs generalize better than the Transformer++. Additionally, because\nEBTs scale better than the Transformer++ during pretraining (Figure 4), these findings suggest that\nEBTs would outperform Transformer++ at foundation model scale. BB stands for BigBench.\nPretrain GSM8K \u2193\nSQuAD \u2193 BB Math QA \u2193 BB Dyck \u2193\nFigure 8: Learning Uncertainty on Text Results. EBTs learn to vary uncertainty across text tokens\nwithout any explicit supervision. As an example, in both (a) and (b), simple tokens such as \u201c.\u201d, \u201cis\u201d,\n\u201ca\u201d, \u201cbut\u201d, or \u201cthe\u201d have lower energies across inference-time optimization (thinking) steps, indicating\nlower uncertainty. On the other hand, harder to predict tokens such as \u201cquick\u201d, \u201cbrown\u201d, \u201cresearch\u201d,\nand \u201cproblem\u201d have higher energies across optimization steps, and more difficulty in achieving energy\nconvergence, meaning the model is more uncertain. Inspired by [25].\npretraining perplexity, EBTs achieve lower (better) perplexity on most downstream tasks, suggesting\nstronger generalization, particularly to Out-of-Distribution (OOD) data. Together, with the better\nlearning scalability results, and knowing that improved pretraining performance usually leads to\nimproved downstream task performance [84, 105, 106], these results suggest that at scale EBTs\nwould outperform the Transformer++.\n4.2 Autoregressive Video Experiments\nTo assess how EBTs scale in continuous domains, we train models to predict the next image in a video\nconditioned on all previous frames\u2014a common pretraining objective for video models [107\u2013111].\nUnlike in the NLP experiments, where models see each sample only once due to the dataset size,\ncurrent popular video datasets are relatively small, requiring models to train repeatedly on the same\ndata. As a result, this setting probes a different question: \u201chow well can models fit a fixed dataset?\u201d\nrather than how efficiently models scale under non-data-bound regimes. This distinction is especially\nimportant given the recent scarcity of high-quality datasets [112, 113] and the perspective that data\nwill increasingly become a bottleneck.\nFor experiments, we encode all 224 \u00d7 224 images into 3136 dimensional features with the frozen\nSD-XL VAE [114, 115]. Then, all models are trained using a Smooth L1 loss with \u03b2 = 1.0 on the\nSomething Something V2 dataset [116], where we report the minimum validation loss achieved.\nIn Figure 9 we report scaling results for the embedding dimension and non-embedding parameter\ncount, as we found these axes behaved the most linearly. The results demonstrate that, despite\nachieving a higher initial loss, EBTs scale at a more than 33% faster rate than the Transformer++.\nThis suggests that at foundation model scale EBTs would achieve significantly better performance\nthan the Transformer++.\nWe believe this large scaling rate gap can be linked to the fact that EBTs more seamlessly model\ncontinuous distributions than standard feed forward transformers due to being able to express\n(a) Scaling for the embedding dimension.\n(b) Scaling for the Non-Embedding Parameters.\nFigure 9: Video Learning Scalability\u2014Width and Parameters. The minimum validation loss\nachieved on the Something Something V2 (SSV2) dataset. While EBTs achieve higher validation\nloss than the Transformer++ at smaller scales, the scaling rate is more than 33% higher, suggesting\nthat at foundation model scale with hundreds of billions of parameters EBTs would perform much\nbetter than the Transformer++. Notably, scaling with respect to the embedding dimension behaves\nmore linearly than for the number of parameters, likely due to the embedding dimension serving as a\nbottleneck for the image representation.\nTable 4: Image Denoising and Classification Comparison. For image denoising, EBTs significantly\noutperform DiTs [26] in Peak Signal to Noise Ratio (PSNR), as well as MSE, on both in-distribution\nand Out-Of-Distribution (OOD) data, while using 99% less forward passes. This suggests that EBTs\ngeneralize better than DiTs while using less computation. On image classification, EBTs also perform\nbetter than DiTs, yielding around 10\u00d7 higher accuracy, suggesting that EBTs learn better image\nrepresentations and therefore understand images better than DiTs.\nIn Distribution Noise \u03c3 = 0.1\nOOD Noise \u03c3 = 0.2\nImageNet-1k Classification\nPSNR \u2191 MSE Pixel \u2193\nuncertainty (Facet 2) through their energy scalar. To confirm this, we visualize results for different\nenergies when predicting video frames in Figure 11. The results demonstrate that EBTs successfully\nlearn to capture uncertainty\u2014where frames earlier on in the video have higher energy (higher\nuncertainty) due to no large objects being within the frame, and then as the major object in the\nscene becomes revealed more EBT predicts lower energy (lower uncertainty). EBTs learn to exhibit\nthis behavior without any supervision using a Smooth L1 loss, whereas the standard feed-forward\nTransformer++ would require discretization schemes such as Vector Quantization [117] with a\ncategorical loss, or other tricks to achieve the same effect.\n4.3 Bidirectional Image Experiments\nIn addition to investigating autoregressive EBTs, we also explore the performance of EBTs trained\nbidirectionally. These experiments allow for a fairer comparison with diffusion models, which\nare not commonly trained autoregressively. Following [48, 118], models are trained to denoise\nimages that have been noised. To make these denoising experiments compatible with diffusion\nmodels, we deviate from the original noising schemes performed in these works and use a noising\nscheme based on the noising schedule from diffusion models. Specifically, we follow [26], and use a\nlinear variance schedule ranging from 1 \u00d7 10\u22124 to 2 \u00d7 10\u22122. To control the noise level, we use a\nhyperparameter denoted \u03c3 representing the percentage of the diffusion schedule to noise samples;\n\u03c3 was set to 0.1 during training, and 0.2 during testing to test generalization. We use the COCO\n2014 dataset [119, 120] with 128 by 128 images, a patch size of 16, and the Diffusion Transformer\nimplementation from [26].\nFollowing [48, 118], during inference we investigate denoising on the same level of noise performed\nduring training (\u03c3 = 0.1), as well as at a higher noise level (\u03c3 = 0.2), representing Out-of-Distribution\nFigure 10: Qualitative OOD Image Denoising. EBTs achieve better denoising quality during\ninference while using one step for every 100 denoising steps of a DiT. The overall image quality of\nEBT denoised images is less blurry than images denoised by DiT.\n(OOD) noisier images. The results are in Table 4, where we observe that EBTs perform better than\nDiTs at both in and out of distribution image denoising across various metrics, by as much as 3.5\nin Peak Signal to Noise Ratio (PSNR). Following [91], we also plot the performance based on the\nnumber of forward passes (Number of Function Evaluations NFEs) in Figure 12. These results\ndemonstrate that EBTs perform better at denoising while using 99% less denoising steps than DiTs,\nand that the System 2 Thinking scaling rate for EBTs is higher than for DiTs. Lastly, qualitative\nresults for denoised out-of-distribution images for EBT compared to the DiT baseline are shown in\nFigure 10. These results further demonstrate that the visual quality of EBT denoised images is much\nbetter than the visual quality of DiT denoised images, while using less compute.\nIn an effort to understand whether the repre-\nsentations learned from denoising captured use-\nful visual features, we perform a linear probe\nevaluation on ImageNet-1k [122], following\ncommon practice in visual representation learn-\ning [79, 80]. For both models, we take the aver-\nage of all the final patch tokens, and for DiTs we\nfeed in T = 0. The results are shown in Table 4,\nwhere the performance of EBTs is much higher\nthan DiTs, achieving a Top-1 and Top-5 accu-\nracy around 10\u00d7 higher than that of DiTs. This\nsuggests that EBTs learn better image represen-\ntations than DiTs, and therefore that EBTs offer\npromise in generative models with an improved\nunderstanding of what they generate.\nAcross both discrete (text) and continuous\n(video) autoregressive models, the results show\nthat EBTs scale at a faster rate than the stan-\ndard Transformer++ approach during pretrain-\ning across all measured axes, including data,\nbatch size, depth, parameters, FLOPs, and width.\nThis is especially apparent with data and batch\nscaling for text, as well as width and param-\neter scaling for video, where the scaling rate\nwas over 30% higher. These results are particu-\nlarly important for two reasons: first, they sug-\ngest that at the scale of modern foundation mod-\nels, EBTs would outperform the current Trans-\nformer++ approach even without their System\n2 Thinking capabilities. Second, EBTs appear\nto be the first approach that has better data ef-\nFigure 11: Learning Uncertainty on Video Re-\nsults. In line with cognitive Facet 2, EBTs learn\nto express uncertainty across continuous video\nframes without supervision. At the start of the\nvideo, uncertainty is high (high energy) because\nthe frame is mostly empty and the scene is highly\nunpredictable. As a blue garment is placed into the\nframe, uncertainty decreases (low energy), reflect-\ning the greater predictability of the scene. When\nthe blue garment is removed from the scene, uncer-\ntainty increases again, indicating a return to higher\nunpredictability. Such a capability is significantly\nmore difficult to achieve in continuous spaces with\ntraditional feed-forward transformers without dis-\ncretization schemes [121].\nficiency than the Transformer++. As data has become one of the major limiting factors in further\nscaling [123], this makes EBTs especially appealing.\nWith System 2 Thinking, the EBT-Transformer++ performance gap would be expected to increase, as\nwe found that System 2 Thinking could improve performance by as much as 29% for text (Figure 6a).\nFurther, we found that the thinking capabilities of EBTs scale well, as they improve during pretraining\n(Figure 6b) and perform better for data that is more OOD (Figure 7). These findings imply that\nOOD generalization benefits from System 2 Thinking will further increase at larger scales, paving\nthe way for principled generalization to OOD data. In addition, EBTs become increasingly robust to\nself-generated errors during verification (Figure B.1a), indicating that their self-verification process\nscales reliably and sidesteps the adversarial instabilities reported in prior works [19, 124].\nWe hypothesize that the superior scaling of\nEBTs compared to the Transformer++ can be\nattributed to EBTs learning to verify (Facet 3)\nrather than solely learning to predict. As dis-\ncussed in Section 2, verification often general-\nizes better than amortized generation, which we\nbelieve leads to improved learning efficiency.\nThe results in our generalization experiments\nfurther support this idea of verification leading\nto improved generalization, as we found that\ngiven a slightly worse pretraining performance,\nEBTs still outperform the Transformer++ recipe\non downstream tasks. Additionally, correspond-\ning to Facet 2 regarding prediction uncertainty,\nwe find that the energy values learned by EBTs\ncorrelate strongly with more challenging to pre-\ndict data, as shown in Figures 8 and 11. This\nis a promising characteristic of EBTs, as it en-\nables uncertainty estimation within continuous\nstate spaces, which would allow for principled\ninference-time behavior adaptation (Facet 1)\nwhen models determine a problem is more chal-\nFigure 12: Image Denoising Thinking Scala-\nbility. A comparison between EBT and DiT on\nimage denoising given a different number of for-\nward passes. EBTs require only 1% of the forward\npasses used by DiT to achieve comparable or better\nPSNR. Further, the scaling rate of PSNR improve-\nment given more forward passes is much higher\nfor EBTs than it is for DiTs. These results suggest\nEBTs have superior thinking capabilities than DiTs\nLastly, the results from our bidirectional experi-\nments indicate that EBTs hold strong promise in bidirectional modeling. Particularly, we find that\nbidirectional EBTs outperform the DiT baseline in image denoising on both In and Out-of-Distribution\nperformance, while using significantly fewer forward passes. This suggests that EBTs offer promise\nin tasks such as image generation or bidirectional text generation. Further, when evaluating the\nrepresentations from DiTs and EBTs, we find that EBTs perform significantly better, achieving up to\na 10\u00d7 improvement in accuracy, suggesting EBTs offer promise in developing a better understanding\nof what is being generated when performing generative modeling.\n6.1 Traditional Transformers\nThe Transformer architecture [83] has become ubiquitous across various domains [79, 82, 87, 125].\nThe most commonly used transformer variant of today makes predictions directly in the output space\nwith a single forward pass, demonstrated in Figure 1a. Because these models have a finite depth\nand width, and make predictions in a single forward pass, they are unable to dynamically allocate\nmore computation to each prediction being made. Furthermore, they cannot model uncertainty in\ncontinuous state spaces in the same way they can in discrete state spaces, because the normalization\nprocess for continuous state spaces is not as well-defined as it is for discrete spaces using softmax [42].\nRather, training these models to express uncertainty relies on tricks such as Vector Quantization [34]\nor pseudo losses\/objectives (e.g., ELBO [35]). Finally, because these models are not trained to\nexplicitly verify samples, improving inference-time performance at a per-prediction level often\nrequires external models [50].\nRecently, several RNN variants (Figure 1b) have emerged to alleviate memory bottlenecks and achieve\nfaster inference [22, 23]. These approaches have scaled similarly to Transformers in autoregressive\nsequence modeling and achieve better memory efficiency and reduced latency. However, traditional\nRNNs that update their internal state based solely on new information\/data [22, 23] are not capable of\nallocating additional computation during inference, and thus suffer from the same flaws as traditional\ntransformers in achieving human-like System 2 Thinking.\nTo resolve these issues, people have equipped RNNs with the ability to allocate computation dy-\nnamically, with architectures such as the Universal Transformer [126]. Recently, this type of RNN\nhas also been applied to LLMs [25, 127], allowing LLMs to reason using additional computation in\na continuous latent space through the depth of an unrolled RNN. However, like Diffusion models,\nthese models learn to amortize gradient prediction of the energy function [25], meaning they cannot\nmodel uncertainty or explicitly verify predictions. Consequently, EBMs generalize these RNN-based\narchitectures by offering explicit prediction verification capabilities [19]. Further discussion on this\nrelationship is provided in Section E.\n6.3 Dynamic Computation with LLMs\nThe ability to leverage a dynamic amount of computation in LLMs has been emulated using chain-of-\nthought prompting [128] and continuous latent space reasoning [29]. While these approaches can\nimprove performance, they don\u2019t seamlessly transfer to continuous modalities, and LLM chain-of-\nthought has been shown to be unreliable for reasoning [129\u2013131]. More recently, models have been\nexplicitly trained to perform reasoning using Reinforcement Learning [11\u201314]. These approaches\nallow LLMs to simulate additional computational depth based on the number of tokens decoded\nbefore making a prediction, and as a result, significantly improve performance [11, 12]. The main\nlimitations of these approaches are that they currently apply only to discrete domains (i.e., LLMs),\nare effective on a narrow set of problems that are easily verifiable (e.g., math and coding), and require\nadditional supervision, typically in the form of reward signals, making them incompatible with purely\nunsupervised pretraining [12].\n6.4 Dynamic Computation with Diffusion\nThe most common instance of a model architecture specifically created to leverage dynamic computa-\ntion is diffusion models (Figure 1c), where using multiple forward passes to generate a prediction\nis a core aspect of both training and inference [114, 132]. Although diffusion models implicitly\ndefine a likelihood through the reverse process [75], which could theoretically be used to verify\npredictions, in practice an external verifier is necessary to improve performance at inference time\nbeyond increasing denoising steps [19\u201321]. This requirement limits the generalizability and scal-\nability of diffusion models as an approach for System 2 Thinking, as they do not have two of the\ncognitive facets discussed in Table 1: the ability to model uncertainty in continuous state spaces and\nthe ability explicitly verify predictions without additional models [19\u201321]. Furthermore, diffusion\nmodels rely on a fixed denoising schedule, which restricts their ability to adaptively halt or extend\ncomputation\u2014unlike EBMs. Additionally, diffusion models can be seen as predicting the gradient of\nthe data density\/energy function [133], and therefore EBMs are a generalization of diffusion models\nthat learn to explicitly verify predictions. More on this connection is in Section E, and a side-by-side\ncomparison of diffusion models and EBMs is in Figure E.1.\n6.5 Energy-Based Models (EBMs)\nThe perspective of energy minimization as thinking\/reasoning has been known for some time [134].\nTherefore, the most similar approaches to EBTs also train EBMs to do reasoning\/thinking [48,\n67]. While these works achieved impressive generalization results, they only focus on small-scale\nproblems, and did not scale EBMs to high-dimensional real-world problems such as language or\nvideo. Additionally, these works did not perform an in-depth analysis on the types of System 2\nThinking that emerge with EBMs, more complex inference time procedures beyond just increasing\nthe number of gradient descent steps, approaches towards improving EBM scalability, and required\ntechniques for enhancing System 2 Thinking in EBMs.\n7 Limitations and Conclusion\nIn this work, we proposed Energy-Based Transformers (EBTs), a new paradigm that frames System 2\nThinking as an optimization procedure with respect to a learned verifier (an Energy-Based Model),\nenabling System 2 Thinking to emerge across any problem or modality entirely from unsupervised\nLimitations. Despite demonstrating strong performance, EBTs have several current limitations.\nFirst, because EBTs generate predictions through an optimization process, they introduce additional\nhyperparameters, such as the optimization step size and the number of optimization steps. Tuning\nthese hyperparameters is crucial for training stability, as we found poorly chosen values often lead to\nunstable training. Second, training and inference are more computationally expensive than standard\nfeed-forward models, requiring additional gradient computations. Third, while EBTs scale well\nup to 800M parameters, we have not explored larger models due to resource constraints. However,\nexperimental scaling trends demonstrate that EBTs scale faster than existing paradigms during\npretraining, suggesting that EBTs would perform better at foundation-model scale. Finally, EBTs\ncurrently struggle with data distributions that have many modes, such as class conditional image\ngeneration, likely due to the convex energy landscape assumption made during training.\nConclusion. EBTs are the first instance of an approach that scales at a faster rate than the Trans-\nformer++ during pretraining across both continuous and discrete modalities. Additionally, our results\nsuggest that EBTs scale better than existing approaches during inference at System 2 Thinking\nby dynamically allocating computational resources and self-verifying their own predictions; these\nSystem 2 Thinking capabilities enable improved generalization to out-of-distribution data. Ultimately,\nthe superior scaling in both training and inference, coupled with improved generalization, positions\nEBTs as a promising new paradigm shift for advancing the capabilities of future foundation models.\n8 Acknowledgements\nWe extend special thanks to Jeonghwan Kim and Cheng Qian for their helpful discussions. This\nresearch used the Delta and DeltaAI advanced computing and data resources, which are supported by\nthe National Science Foundation (award OAC 2320345 and award OAC 2005572) and the State of\nIllinois. Delta and DeltaAI are joint efforts of the University of Illinois Urbana-Champaign and its\nNational Center for Supercomputing Applications.\n[1] Daniel Kahneman. Thinking, fast and slow. macmillan, 2011. 1, 3, 30\n[2] Jonathan St BT Evans. Dual-process theories of reasoning: Contemporary issues and developmental\napplications. Developmental review, 31(2-3):86\u2013102, 2011.\n[3] Daniel Kahneman, Shane Frederick, et al. Representativeness revisited: Attribute substitution in intuitive\njudgment. Heuristics and biases: The psychology of intuitive judgment, 49(49-81):74, 2002.\n[4] Keith Frankish. Dual-process and dual-system theories of reasoning. Philosophy Compass, 5(10):\n914\u2013926, 2010. 1\n[5] Wim De Neys. Dual processing in reasoning: Two systems but one reasoner. Psychological science, 17\n(5):428\u2013433, 2006. 2\n[6] Vinod Goel, Christian Buchel, Chris Frith, and Raymond J Dolan. Dissociation of mechanisms underlying\nsyllogistic reasoning. Neuroimage, 12(5):504\u2013514, 2000. 2\n[7] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu,\nJunhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: A survey of reasoning large\nlanguage models. arXiv preprint arXiv:2502.17419, 2025. 2\n[8] Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Fara-\njtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models.\narXiv preprint arXiv:2410.05229, 2024. 2\n[9] Yang Yan, Yu Lu, Renjun Xu, and Zhenzhong Lan. Do phd-level llms truly grasp elementary addition?\nprobing rule learning vs. memorization in large language models. arXiv preprint arXiv:2504.05262, 2025.\n[10] Cheng Qian, Peixuan Han, Qinyu Luo, Bingxiang He, Xiusi Chen, Yuji Zhang, Hongyi Du, Jiarui Yao,\nXiaocheng Yang, Denghui Zhang, Yunzhu Li, and Heng Ji. Escapebench: Pushing language models to\nthink outside the box. In arxiv, 2025. 2\n[11] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Hel-\nyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint\narXiv:2412.16720, 2024. 2, 16, 30\n[12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2501.12948, 2025. 2, 8, 16\n[13] xAI. Grok 3 Beta \u2014 The Age of Reasoning Agents, 2025. URL https:\/\/x.ai\/blog\/grok-3.\nAccessed: 2025-02-21. 2\n[14] Anthropic. Claude 3.7 sonnet and claude code, 2025. URL https:\/\/www.anthropic.com\/news\/\nclaude-3-7-sonnet. Accessed: 2025-02-21. 2, 16\nLearning to reason with llms, 2024.\nURL https:\/\/openai.com\/index\/\nlearning-to-reason-with-llms\/. Accessed: 2025-02-21. 2\n[16] Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Expanding\nrl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025.\n[17] Parshin Shojaee*\u2020, Iman Mirzadeh*, Keivan Alizadeh, Maxwell Horton, Samy Bengio, and Mehrdad\nFarajtabar. The illusion of thinking: Understanding the strengths and limitations of reasoning mod-\nels via the lens of problem complexity, 2025. URL https:\/\/ml-site.cdn-apple.com\/papers\/\nthe-illusion-of-thinking.pdf. 2\n[18] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does\nreinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv\npreprint arXiv:2504.13837, 2025. 2\n[19] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong\nLi, Tommi Jaakkola, Xuhui Jia, et al. Inference-time scaling for diffusion models beyond scaling denoising\nsteps. arXiv preprint arXiv:2501.09732, 2025. 2, 3, 6, 8, 15, 16, 30, 36, 37\n[20] Fangfu Liu, Hanyang Wang, Yimo Cai, Kaiyan Zhang, Xiaohang Zhan, and Yueqi Duan. Video-t1:\nTest-time scaling for video generation. arXiv preprint arXiv:2503.18942, 2025.\n[21] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh\nRanganath. A general framework for inference-time scaling and steering of diffusion models. arXiv\npreprint arXiv:2501.06848, 2025. 2, 16\n[22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv\npreprint arXiv:2312.00752, 2023. 2, 8, 9, 16, 27, 33, 34\n[23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng,\nMichael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era.\narXiv preprint arXiv:2305.13048, 2023. 2, 16\n[24] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u2013\n[25] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R Bartoldson,\nBhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent\nreasoning: A recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. 2, 3, 12, 16, 41\n[26] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. 2, 4, 8, 13, 33, 34,\n[27] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation\nwithout vector quantization. Advances in Neural Information Processing Systems, 37:56424\u201356445, 2025.\n[28] Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, and Haoqi Fan. Causal diffusion transformers for\ngenerative modeling. arXiv preprint arXiv:2412.12095, 2024. 2, 33\n[29] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian.\nTraining large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769,\n[30] Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh\nNagarajan. Think before you speak: Training language models with pause tokens. arXiv preprint\narXiv:2310.02226, 2023. 3\n[31] Jochen Ditterich. Evidence for time-variant decision making. European Journal of Neuroscience, 24(12):\n3628\u20133641, 2006. 3\n[32] Nicolas P Rougier, David C Noelle, Todd S Braver, Jonathan D Cohen, and Randall C O\u2019Reilly. Prefrontal\ncortex and flexible cognitive control: Rules without symbols. Proceedings of the National Academy of\nSciences, 102(20):7338\u20137343, 2005. 3\n[33] Christian Tomani, Kamalika Chaudhuri, Ivan Evtimov, Daniel Cremers, and Mark Ibrahim. Uncertainty-\nbased abstention in llms improves safety and reduces hallucinations. arXiv preprint arXiv:2404.10960,\n[34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems, 30, 2017. 3, 15\n[35] Diederik P Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. 3, 15\n[36] Karthik Abinav Sankararaman, Sinong Wang, and Han Fang. Bayesformer: Transformer with uncertainty\nestimation. arXiv preprint arXiv:2206.00826, 2022. 3\n[37] Alvin Heng, Harold Soh, et al. Out-of-distribution detection with a single unconditional diffusion model.\nAdvances in Neural Information Processing Systems, 37:43952\u201343974, 2024.\n[38] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deep\ngenerative models know what they don\u2019t know? arXiv preprint arXiv:1810.09136, 2018.\n[39] Joan Serr\u00e0, David \u00c1lvarez, Vicen\u00e7 G\u00f3mez, Olga Slizovskaia, Jos\u00e9 F N\u00fa\u00f1ez, and Jordi Luque. Input\ncomplexity and out-of-distribution detection with likelihood-based generative models. arXiv preprint\narXiv:1909.11480, 2019. 3\n[40] Christopher M Bishop. Mixture density networks. 1994. 3\n[41] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020. 3\n[42] Anna Dawid and Yann LeCun. Introduction to latent variable energy-based models: a path toward\nautonomous machine intelligence. Journal of Statistical Mechanics: Theory and Experiment, 2024(10):\n104011, 2024. 3, 6, 7, 15, 40\n[43] A. Peters, B. McEwen, and Karl J. Friston. Uncertainty and stress: Why it causes diseases and how it is\nmastered by the brain. Progress in Neurobiology, 156:164\u2013188, 2017. doi: 10.1016\/j.pneurobio.2017.05.\n[44] I. Vilares, J. D. Howard, Hugo L. Fernandes, J. Gottfried, and Konrad Paul Kording. Differential\nrepresentations of prior and likelihood uncertainty in the human brain. Current Biology, 22:1641\u20131648,\n2012. doi: 10.1016\/j.cub.2012.07.010.\n[45] Issidoros C. Sarinopoulos, D. Grupe, Kristen L. Mackiewicz, J. Herrington, M. Lor, E. E. Steege, and\nJ. Nitschke. Uncertainty during anticipation modulates neural responses to aversion in human insula and\namygdala. Cerebral cortex, 20 4:929\u201340, 2010. doi: 10.1093\/cercor\/bhp155. 3\n[46] Frank Loesche, Jeremy Goslin, and Guido Bugmann. Paving the way to eureka\u2014introducing \u201cdira\u201d as an\nexperimental paradigm to observe the process of creative problem solving. Frontiers in Psychology, 9:\n[47] Zaid Alkouri. Using contents and containers to investigate problem solving strategies among toddlers.\n[48] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Learning iterative reasoning through energy\nminimization. In International Conference on Machine Learning, pages 5570\u20135582. PMLR, 2022. 3, 6,\n[49] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without\nhuman knowledge. nature, 550(7676):354\u2013359, 2017. 3\n[50] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. In The Twelfth International\nConference on Learning Representations, 2023. 3, 15\n[51] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. Advances in\nneural information processing systems, 32, 2019. 4, 5, 6, 7, 8, 27, 42\n[52] Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence training\nof energy based models. arXiv preprint arXiv:2012.01316, 2020.\n[53] Zengyi Li, Yubei Chen, and Friedrich T Sommer. Learning energy-based models in high-dimensional\nspaces with multiscale denoising-score matching. Entropy, 25(10):1367, 2023. 4\n[54] Michael Arbel, Liang Zhou, and Arthur Gretton. Generalized energy based models. arXiv preprint\narXiv:2003.05033, 2020. 4, 8, 42\n[55] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understand-\ning by generative pre-training. 2018. 4, 8\n[56] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2019. 4, 8\n[57] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of\nneural nets. Advances in neural information processing systems, 31, 2018. 5\n[58] Stephen A Cook. The complexity of theorem-proving procedures. In Logic, automata, and computational\ncomplexity: The works of Stephen A. Cook, pages 143\u2013152. 2023. 5\n[59] Shafi Goldwasser, Silvio Micali, and Chales Rackoff. The knowledge complexity of interactive proof-\nsystems. In Providing sound foundations for cryptography: On the work of shafi goldwasser and silvio\nmicali, pages 203\u2013225. 2019. 5\n[60] Kurt G\u00f6del. Letter to john von neumann, 1956. URL https:\/\/ecommons.cornell.edu\/server\/\napi\/core\/bitstreams\/46aef9c4-288b-457d-ab3e-bb6cb1a4b88e\/content. Accessed: 2025-\n[61] Ryan Lavin, Xuekai Liu, Hardhik Mohanty, Logan Norman, Giovanni Zaarour, and Bhaskar Krishna-\nmachari. A survey on the applications of zero-knowledge proofs. arXiv preprint arXiv:2408.00243, 2024.\n[62] Ronald L Rivest, Adi Shamir, and Leonard Adleman. A method for obtaining digital signatures and\npublic-key cryptosystems. Communications of the ACM, 21(2):120\u2013126, 1978. 5\n[63] AlphaCode Team. Alphacode 2 technical report. December 2023. 6\n[64] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. 6\n[65] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in neural information processing systems, 35:27730\u201327744, 2022. 6\n[66] Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and J Andrew Bagnell. All roads lead\nto likelihood: The value of reinforcement learning in fine-tuning. arXiv preprint arXiv:2503.01067, 2025.\n[67] Yilun Du, Jiayuan Mao, and Joshua B Tenenbaum. Learning iterative reasoning through energy diffusion.\narXiv preprint arXiv:2406.11179, 2024. 6, 16, 30, 37\n[68] Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D Hwang, Liwei Jiang, Jillian\nFisher, Abhilasha Ravichander, Khyathi Chandu, et al. The generative ai paradox:\" what it can create, it\nmay not understand\". arXiv preprint arXiv:2311.00059, 2023. 6\n[69] Gala Stojni\u00b4c, Kanishk Gandhi, Shannon Yasuda, Brenden M Lake, and Moira R Dillon. Commonsense\npsychology in human infants and machines. Cognition, 235:105406, 2023. 6\n[70] Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri,\nLucas Paul Saldyt, and Anil B Murthy. Position: Llms can\u2019t plan, but can help planning in llm-modulo\nframeworks. In Forty-first International Conference on Machine Learning, 2024. 6\n[71] Ze Wang, Jiang Wang, Zicheng Liu, and Qiang Qiu. Energy-inspired self-supervised pretraining for\nvision models. arXiv preprint arXiv:2302.01384, 2023. 6, 7, 38, 40, 43\n[72] Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review,\n[73] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing\nsystems, 27, 2014. 7\n[74] Yoshua Bengio, R\u00e9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language\nmodel. Journal of machine learning research, 3(Feb):1137\u20131155, 2003. 7\n[75] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems, 33:6840\u20136851, 2020. 7, 16\n[76] Mathieu Dagr\u00e9ou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. How to compute hessian-vector\nproducts? In ICLR Blogposts 2024, 2024. URL https:\/\/iclr-blogposts.github.io\/2024\/blog\/\nbench-hvp\/. https:\/\/iclr-blogposts.github.io\/2024\/blog\/bench-hvp\/. 7, 35\n[77] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in neural\ninformation processing systems, 33:3008\u20133021, 2020. 8\n[78] OpenAI. Gpt-4 technical report, 2023. 8\n[79] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas,\nWojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu\nSharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr\nBojanowski. Dinov2: Learning robust visual features without supervision, 2023. 14, 15\n[80] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders\nare scalable vision learners, 2021. 14\n[81] Zal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,\nDominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a\nlanguage modeling approach to audio generation, 2023. 8\n[82] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 8, 15, 30\n[83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing\nsystems, 30, 2017. 8, 15, 31\n[84] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models.\narXiv preprint arXiv:2407.21783, 2024. 8, 9, 10, 11, 12, 28\n[85] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv\npreprint arXiv:2001.08361, 2020. 8, 33, 34\n[86] Margaret Li, Sneha Kudugunta, and Luke Zettlemoyer. (mis) fitting: A survey of scaling laws. arXiv\npreprint arXiv:2502.18969, 2025. 8, 9, 33\n[87] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh\nKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan\nSubramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and\nfine-tuned chat models, 2023. 8, 9, 10, 11, 15, 33, 42\n[88] Ibrahim M Alabdulmohsin, Behnam Neyshabur, and Xiaohua Zhai. Revisiting neural scaling laws in\nlanguage and vision. Advances in Neural Information Processing Systems, 35:22300\u201322312, 2022.\n[89] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling.\narXiv preprint arXiv:2010.14701, 2020.\n[90] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022. 8\n[91] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential\nequations. Advances in neural information processing systems, 31, 2018. 8, 14\n[92] Maurice Weber, Dan Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong\nLyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, et al. Redpajama: an open dataset for training large\nlanguage models. Advances in neural information processing systems, 37:116462\u2013116492, 2024. 8\n[93] Together Computer. Redpajama: an open dataset for training large language models, 2023. URL\nhttps:\/\/github.com\/togethercomputer\/RedPajama-Data. 8\n[94] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745, 2022. 8\n[95] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen,\nXiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states.\narXiv preprint arXiv:2407.04620, 2024. 8\n[96] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1, grade-\nschool math and the hidden reasoning process. In The Thirteenth International Conference on Learning\nRepresentations, 2024. 9\n[97] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168, 2021. 9\n[98] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. 9\n[99] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615,\n[100] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a\nmirage? Advances in Neural Information Processing Systems, 36:55565\u201355581, 2023. 9\n[101] Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li,\nChunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, et al. Byte latent transformer: Patches scale\nbetter than tokens. arXiv preprint arXiv:2412.09871, 2024. 9, 10\n[102] Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman,\nRulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models scale reliably with\nover-training and on downstream tasks. arXiv preprint arXiv:2403.08540, 2024. 11\n[103] Suchin Gururangan, Ana Marasovi\u00b4c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and\nNoah A Smith. Don\u2019t stop pretraining: Adapt language models to domains and tasks. arXiv preprint\narXiv:2004.10964, 2020.\n[104] Tristan Thrush, Christopher Potts, and Tatsunori Hashimoto. Improving pretraining data using perplexity\ncorrelations. arXiv preprint arXiv:2409.05816, 2024. 11\n[105] Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, and Heng Ji. Scaling laws\nfor predicting downstream performance in llms. arXiv preprint arXiv:2410.08527, 2024. 11, 12, 34\n[106] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi\nKoyejo. Scaling laws for downstream task performance of large language models. In ICLR 2024 Workshop\non Mathematical and Empirical Understanding of Foundation Models, 2024. 11, 12\n[107] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan,\nYonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv\npreprint arXiv:2412.14169, 2024. 12\n[108] Dirk Weissenborn, Oscar T\u00e4ckstr\u00f6m, and Jakob Uszkoreit. Scaling autoregressive video models. arXiv\npreprint arXiv:1906.02634, 2019.\n[109] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video\ntransformer. arXiv preprint arXiv:2006.10704, 2020.\n[110] Xi Ye and Guillaume-Alexandre Bilodeau. Video prediction by efficient transformers. Image and Vision\nComputing, 130:104612, 2023.\n[111] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with\nnext-frame prediction. arXiv preprint arXiv:2503.19325, 2025. 12\n[112] Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. Will\nwe run out of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint\narXiv:2211.04325, 2022. 12\n[113] URL https:\/\/research.google\/blog\/data-centric-ml-benchmarking-announcing-dataperfs-2023-challenges\/.\n[114] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE\/CVF conference on computer\nvision and pattern recognition, pages 10684\u201310695, 2022. 12, 16, 33, 35, 40\n[115] URL https:\/\/huggingface.co\/stabilityai\/sd-vae-ft-mse. 12, 33, 35\n[116] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal,\nHeuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something\nsomething\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE\ninternational conference on computer vision, pages 5842\u20135850, 2017. 12\n[117] Md Mofijul Islam, Alexi Gladstone, Riashat Islam, and Tariq Iqbal. Eqa-mx: Embodied question answer-\ning using multimodal expression. In The Twelfth International Conference on Learning Representations,\n[118] Xinshi Chen, Hanjun Dai, Yu Li, Xin Gao, and Le Song. Learning to stop while learning to predict. In\nInternational conference on machine learning, pages 1520\u20131530. PMLR, 2020. 13\n[119] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer vision\u2013ECCV 2014:\n13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part v 13, pages\n740\u2013755. Springer, 2014. 13, 34\n[120] URL https:\/\/huggingface.co\/datasets\/AbdoTW\/COCO_2014. 13, 34\n[121] Jiahuan Pei, Cheng Wang, and Gy\u00f6rgy Szarvas. Transformer uncertainty estimation with hierarchical\nstochastic attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages\n11147\u201311155, 2022. 14\n[122] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision, 115:211\u2013252, 2015. 14\n[123] URL https:\/\/www.youtube.com\/watch?v=6nJZopACRuQ&ab_channel=OpenAI. 15\n[124] Audrey Huang, Adam Block, Qinghua Liu, Nan Jiang, Dylan J Foster, and Akshay Krishnamurthy. Is\nbest-of-n the best of them? coverage, scaling, and optimality in inference-time alignment. arXiv preprint\narXiv:2503.21878, 2025. 15\n[125] Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, and Junaid Qadir.\nTransformers in speech processing: A survey. arXiv preprint arXiv:2303.11607, 2023. 15\n[126] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal\ntransformers. arXiv preprint arXiv:1807.03819, 2018. 16\n[127] Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank J Reddi. Reasoning with\nlatent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. 16, 36\n[128] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837, 2022. 16\n[129] Tianhe Lin, Jian Xie, Siyu Yuan, and Deqing Yang. Implicit reasoning in transformers is reasoning\nthrough shortcuts. arXiv preprint arXiv:2503.07604, 2025. 16\n[130] Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman. Language models don\u2019t always say\nwhat they think: Unfaithful explanations in chain-of-thought prompting. Advances in Neural Information\nProcessing Systems, 36:74952\u201374965, 2023.\n[131] Chirag Agarwal, Sree Harsha Tanneru, and Himabindu Lakkaraju. Faithfulness vs. plausibility: On the\n(un) reliability of explanations from large language models. arXiv preprint arXiv:2402.04614, 2024. 16\n[132] Tobias H\u00f6ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for\nvideo prediction and infilling, 2022. 16\n[133] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha\nSohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional\ngeneration with energy-based diffusion models and mcmc. In International conference on machine\nlearning, pages 8489\u20138510. PMLR, 2023. 16, 36, 38\n[134] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, Fujie Huang, et al. A tutorial on energy-based\nlearning. Predicting structured data, 1(0), 2006. 16\n[135] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak,\nand Owain Evans. The reversal curse: Llms trained on\" a is b\" fail to learn\" b is a\". arXiv preprint\narXiv:2309.12288, 2023. 26\n[136] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible\nbehavior synthesis. arXiv preprint arXiv:2205.09991, 2022. 26\n[137] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake,\nand Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International\nJournal of Robotics Research, page 02783649241273668, 2023.\n[138] Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained\nvisual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024. 26\n[139] Anton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc\u2019Aurelio Ranzato, and Arthur Szlam. Residual\nenergy-based models for text. Journal of Machine Learning Research, 22(40):1\u201341, 2021. 26, 38\n[140] Sumanta Bhattacharyya, Amirmohammad Rooshenas, Subhajit Naskar, Simeng Sun, Mohit Iyyer, and\nAndrew McCallum. Energy-based reranking: Improving neural machine translation using energy-based\nmodels. arXiv preprint arXiv:2009.13267, 2020. 26, 38\n[141] Rodney J Douglas and Kevan AC Martin. Recurrent neuronal circuits in the neocortex. Current biology,\n17(13):R496\u2013R500, 2007. 27\n[142] Michael Betancourt. A conceptual introduction to hamiltonian monte carlo.\narXiv:1701.02434, 2017. 27\n[143] Guilherme Penedo, Hynek Kydl\u00ed\u02c7cek, Anton Lozhkov, Margaret Mitchell, Colin A Raffel, Leandro\nVon Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale.\nAdvances in Neural Information Processing Systems, 37:30811\u201330849, 2024. 28\n[144] Rohin Manvi, Anikait Singh, and Stefano Ermon. Adaptive inference-time compute: Llms can predict if\nthey can do better, even mid-generation. arXiv preprint arXiv:2410.02725, 2024. 30\n[145] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can\nbe more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 30\n[146] Thomas Parr, Giovanni Pezzulo, and Karl J Friston. Active inference: the free energy principle in mind,\nbrain, and behavior. MIT Press, 2022. 30\n[147] William A Falcon. Pytorch lightning. GitHub, 3, 2019. 33\n[148] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source\nautoregressive language model, 2022. 33, 35, 36\n[149] Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding,\nZebin Ou, Guoyang Zeng, et al. Predicting emergent abilities with infinite resolution evaluation. arXiv\npreprint arXiv:2310.03262, 2023. 34\n[150] Adam Casson.\nTransformer flops.\nURL https:\/\/adamcasson.com\/posts\/\ntransformer-flops. 35\n[151] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual\ngeneration with composable diffusion models. In European Conference on Computer Vision, pages\n423\u2013439. Springer, 2022. 36\n[152] Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Horng Chau,\nMohammed Zaki, and Dmitry Krotov. Energy transformer. Advances in Neural Information Processing\nSystems, 36, 2024. 38\n[153] Yezhen Wang, Tong Che, Bo Li, Kaitao Song, Hengzhi Pei, Yoshua Bengio, and Dongsheng Li. Your\nautoregressive generative model can be better if you treat it as an energy-based one. arXiv preprint\narXiv:2206.12840, 2022. 38\n[154] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation,\n23(7):1661\u20131674, 2011. 38, 40\n[155] Angela D Friederici and J\u00fcrgen Weissenborn. Mapping sentence form onto meaning: The syntax\u2013semantic\ninterface. Brain research, 1146:50\u201358, 2007. 38\n[156] Seijin Kobayashi, Simon Schug, Yassir Akram, Florian Redhardt, Johannes von Oswald, Razvan Pascanu,\nGuillaume Lajoie, and Jo\u00e3o Sacramento. When can transformers compositionally generalize in-context?\narXiv preprint arXiv:2407.12275, 2024. 38\n[157] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive\nbenchmark for open-world compositional text-to-image generation. Advances in Neural Information\nProcessing Systems, 36:78723\u201378747, 2023. 38\n[158] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities.\nProceedings of the national academy of sciences, 79(8):2554\u20132558, 1982. 41\n[159] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics,\npages 249\u2013256. JMLR Workshop and Conference Proceedings, 2010. 42\n[160] Noam Shazeer. Glu variants improve transformer, 2020. 42\n[161] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 42\nIn this appendix, we provide additional insight and details on EBTs. First, we provide more insight\non the broader impact\/future work of EBTs in Section A. Next, in Section B, we include additional\nexperiments. Then, we include additional approach details in Section C, as well as additional\nexperimental details in Section D. After that, we include a comprehensive related work in Section E,\nadditional facets of cognition in Section F, and a discussion of counterarguments in Section G. Finally,\nin the hopes of making EBTs more accessible to general audiences, in Section H we contain a general\neasier-to-understand intro to EBMs, and in Section I we describe a tutorial for getting started with\nA Future Works and Broader Impact\nEBTs, being qualitatively different from existing approaches, open several future research directions.\nA.1 Reversal Curse\nRecently, a phenomenon known as \u201cThe Reversal Curse\u201d has been observed where LLMs fail to learn\na symmetric mapping [135] \u201cB is A\u201d despite learning \u201cA is B\u201d. For example, LLMs trained on an\nexample such as \u201cQ: Who is Tom Cruise\u2019s mother? A: Mary Lee Pfeiffer\u201d often fail to generalize\nto know the answer to the reverse question of \u201cWho is Mary Lee Pfeiffer\u2019s son?\u201d Remarkably,\nthe Reversal Curse has manifested itself in LLMs regardless of the size or scale [135]\u2014probing\nresearchers to investigate whether there are fundamental limitations to traditional feed-forward LLMs.\nOne predicted cause of The Reversal Curse is the nature of gradient updates, where backpropagation\nonly updates tokens within context. That is, while learning the mapping \u201cA is B\u201d, none of B\u2019s tokens\nare within context, meaning they do not receive gradient updates when updating A\u2019s tokens. We\nhypothesize that LLMs trained with EBTs rather than standard feed-forward Transformers could help\nreduce this phenomenon, as with EBTs the tokens of A and B are within context during gradient\nupdates due to predictions being made in the input space. Therefore, an exciting research direction\nwould be investigating whether this hypothesis is correct in LLMs trained with EBTs, allowing\nimproved generalization.\nImproved Stability\nIn this work, we primarily trained EBTs with either two or three optimization steps. While these\nparameters worked well, we suspect that increasing the number of steps would improve the System\n2 Thinking capabilities and the scaling during pretraining of EBTs, as more steps enable a longer\n\u201cthinking process\u201d before needing to converge. However, because of challenges in stability when\ntraining with more steps, due to a larger gradient computation graph, we were unable to successfully\nincrease past two or three steps. Future work could focus more on extending the number of steps by\nstudying ways to improve the training stability of EBMs.\nA.3 World Models\nIn this work, we focus on autoregressive and bidirectional models over just state information (no\nactions). EBTs offer high promise in modeling states and actions due to the nature of EBMs learning\na distribution over all possible inputs. Particularly, given a model trained to estimate the unnormalized\njoint distribution of the current context, future, as well as future actions, such world models could\nimplicitly be used as policies to generate actions to achieve a specific state, similar to [136\u2013138].\nThis would involve holding the current context (past states) constant, and minimizing the energy by\npropagating the gradient back to the action inputs and future state predictions. Thus, world models\ntrained in this manner become capable of more than just predicting the future, but also in decision\nmaking to achieve a specific goal state.\nA.4 EBTs as Complementary Models\nAs demonstrated in [139, 140], EBMs can be used to improve the quality of generated text from\nlanguage models. It\u2019s possible EBTs could be used in a similar manner for a broad variety of tasks,\nserving as the verifier of predictions initialized by standard feed-forward models. Therefore, although\nwe do a side-by-side comparison to existing models in this work, EBTs could be complementary to\nexisting model paradigms\u2014being used as the System 2 Backbone for helping lighter models that\nperform System 1 thinking.\nThere exist several current real-world use cases, such as low-latency LLM serving, where doing a\nsingle forward pass is sufficient, and where the added inference overhead of gradients with EBTs\nwould not be worth the extra computation. However, we also envision a world in which people use\nEBTs for long-term System 2 Thinking to solve challenging problems. How much computation\nwould it be worth dedicating to prove a long-standing mathematical conjecture, or figuring out a cure\nA.5 Recurrent Energy-Based Models\nWhile EBTs scale well, for latency-driven use cases, Transformers require significantly more memory\nthan Recurrent Neural Networks. Additionally, there is strong evidence for recurrence in the human\nbrain [141]. Therefore, we anticipate that recurrent Energy-Based Models, possibly leveraging the\nMamba architecture [22], will eventually become common.\nImproved Thinking Algorithms\nThe EBM thinking algorithms described have strong connections to or are derived from Markov Chain\nMonte Carlo (MCMC) sampling. Therefore, we broadly expect known MCMC samplers with more\nadvanced techniques for traversing the energy landscape to be successful, such as Hamiltonian Monte\nCarlo [142] or annealed Langevin dynamics [51]. Additionally, we did not explore more advanced\nsearch algorithms such as Monte Carlo Tree Search, which we suspect could offer performance\nimprovements and leave for future work.\nA.7 Multimodal Energy-Based Models\nWe did not experiment with multimodal EBMs, however, EBMs offer several advantages for learning\nover multiple modalities. For example, multimodal EBMs would enable a single energy scalar to\nrepresent the alignment between modalities, and would simplify joint training across modalities by\nproviding a unified objective that naturally captures inter-modal dependencies.\nA.8 Thinking Scalability\nDue to a lack of computational resources, we were unable to train models with more than 1021 FLOPs\n(\u2248 1300 A100 GPU Hours). Therefore, training and thinking with EBTs remains untested at larger\nfoundation model scale. We leave it to future work to scale with more GPUs and investigate the\nqualitative differences in training and thinking with EBTs.\nA.9 Learning Multimodal Distributions\nWe found that EBTs, with the current training approach, struggle to capture distributions with many\nmodes (e.g., unconditional image generation). Therefore, future work could explore approaches to\nimprove the learning of distributions with many modes. More info is in Section B.2.\nA.10 Understanding Predictions\nWe believe that there exists a fundamental distinction between the internal representations associated\nwith model inputs and outputs. Specifically, models generate internal representations of inputs, as\nthese serve as the foundation that dictate the model\u2019s behavior at any given point in time. Conversely,\nas outputs do not affect a model\u2019s behavior at a given point in time, we contend that models construct\nrepresentations for predicting (and not necessarily understanding) outputs. This distinction leads us\nto an interesting insight: models may not achieve a genuine understanding of outputs in the same way\nthey understand inputs. This implies that while models may develop an intricate understanding of\ninput data, such understanding does not naturally extend to predictions that are made in the output\nspace. Therefore, existing feed-forward models primarily making predictions in the output space\nmay not understand their predictions in the same way they understand their inputs. This intuition\nfurther supports the principles behind EBT, where predictions are made in the input space, enabling\n(a) Self-verification with BoN-10 versus BoN-2.\n(b) Results in Fig. 6b projected to Llama3 scale [84].\nFigure B.1: EBT Thinking Analysis for Data Scaling. (a) Self-verification of BoN-2 compared to\nBoN-10. EBTs become less adversarial and thus benefit more from verifying an increasing number\nof samples during training. (b) A projection of the results from Figure 6b to the data scale of\nLlama3 [84], demonstrating that as data scale increases, improvements from self-verification can lead\nto potentially massive performance increases from System 2 Thinking.\nrepresentations of predictions to be developed. We leave the investigation of this hypothesis to future\nB Additional Experimentation\nB.1 Additional Natural Language Processing Experiments\nWe conduct experiments to confirm hypotheses on thinking results obtained in the main paper. First,\nwe confirm that EBTs become less adversarial with scale by comparing the performance of using\nBoN with 2 versus 10 samples. The results in Figure B.1a demonstrate that when models are trained\non less tokens, there is little performance improvement by verifying 10 samples instead of just 2.\nIn fact, verifying 10 samples occasionally leads to worse performance than verifying 2 samples,\nlikely because the EBT found an adversarial sample (a sample with low energy that is in fact not a\ngood prediction). However, as data scale increases we observe that performance improvements from\nBoN-10 versus BoN-2 increase, and that these adversarial dynamics decrease. Together, these results\nsuggest that with scale EBTs become less adversarial due to an improved energy landscape. In an\neffort to understand the impacts of thinking at the scale of modern foundation models, we project\nresults from Figure 6b to the scale of modern foundation models [84] to extrapolate a projected\nperformance gain from self-verification based thinking. The results are visualized in Figure B.1b,\nwhere they demonstrate that, because of the 1000\u00d7 data scale of modern foundation models, the\nperformance improvement from self-verification increases drastically.\nAdditionally, in an effort to understand whether EBTs can capture epistemic uncertainty (uncertainty\nrelated to knowledge), in addition to aleatoric uncertainty (uncertainty related to inherent data\nrandomness), we visualize the energies of different token sequences that are in versus Out-Of-\nDistribution (OOD) in Figure B.2. The results demonstrate that, for a more in-distribution sequence,\nEBTs have lower energy (less uncertainty), than for an OOD sequence. This suggests that EBTs learn\nto know what they don\u2019t know, as they learn to have higher energy for OOD sequences signifying\nharder predictability.\nTo confirm some of the scaling trends in the paper, as well as experiment with additional datasets,\nwe conduct larger-scale experiments on the FineWeb dataset [143]. Particularly, for Figure 4a, we\nconfirm that the generally better data scaling of EBTs compared to the Transformer++ holds at\nincreased scale. We confirm this by training small-sized models with a batch size of 256, a context\nlength of 1024, and for 500, 000 training steps. These results are visualized in Figure B.3, where we\nobserve that EBTs still continue to outscale the Transformer++ by as much as 35% in scaling rate.\nThese results further reinforce that EBTs are more data-efficient than the Transformer++.\nFigure B.2: Epistemic Uncertainty Comparison. EBTs learn to express epistemic uncertainty\n(uncertainty related to lack of knowledge) on unseen data. Particularly, the sequence on the left,\nwhich is a text sequence likely seen during training, has consistently lower energy (uncertainty)\nfor tokens than the sequence on the right, which is a random text sequence not from the training\ndistribution. This demonstrates that EBTs learn to \u201cknow what they don\u2019t know.\u201d\n(a) Data Scaling Higher Scale.\n(b) Data Scaling Zoomed In.\nFigure B.3: EBT Larger Scale Data Scaling. (a) Data scaling results from Figure 4a scaled up to a\nlarger model size, higher context length, increased batch size, and a higher number of training steps.\n(b) Results from (a) zoomed in.\nB.2 EBT Failure Cases\nDuring experimentation, along with image denoising experiments, we also conducted small-scale\nexperiments with text-to-image generation. We found that, in datasets such as COCO with many\ndifferent modes for a single condition (e.g., hundreds of images with a caption similar to \u201cgiraffe with\na long neck\u201d), that EBTs did not learn to generate high-quality novel images. Instead, EBTs often\ngenerated blurred images similar to the training distribution. We believe this is caused by the training\napproach pushing the energy landscape to be convex surrounding the training examples (modes).\nTherefore, when there are many different modes within the same region (same condition), this\nconvex energy landscape \u201cmerges\u201d to one landscape averaged around the different modes, resulting\nin blurriness. We believe that this is not a fundamental limitation of EBTs, and that future work could\naddress this issue, such as by adding more conditioning.\nC Additional EBT Details\nC.1 Formalizing Thinking\nDue to the recent surge of interest in scaling the performance of models during inference\/test time,\nthere are several common terms used to refer to these ideas. These terms include scaling the thinking\ncapabilities of models [11], inference-time scaling [19], inference-time compute [144], and test-time\ncompute [11, 145]. Therefore, to reduce confusion stemming from a wide variety of terminology and\nunite the community, in this work we broadly define these concepts as System Two Thinking or\nmore concisely Thinking. We formalize improvements made by Thinking as the following:\nDefinition C.1 (System 2 Thinking). Given a problem with data x, a model \u03b8, and additional\ncomputational resources in the form of function evaluations F greater than the minimum number\nof function evaluations to get a valid prediction from the model F0, System Two Thinking STT(\u00b7)\nquantifies the expected percentage improvement in performance as F increases. Let P (x, \u03b8, F ) be\nthe performance on input x when the model \u03b8 uses F function evaluations:\nSTT(x, \u03b8, F ) = Ex\n(cid:20) P (x, \u03b8, F )\nThis formalization is compatible with any type of metric (e.g., Accuracy, Perplexity, FID, etc),\nand uses more psychology-aligned terminology [1]. Further, avoiding terms such as \u201cinference\u201d\nor \u201ctest-time\u201d makes the idea of Thinking more compatible with domains where the line between\ninference and training is blurry, such as real-world continual learning, domain adaptation, or actual\nhuman learning\/thinking processes [146]. Just as learning has become a flexible term across machine\nlearning representing several different ideas, we intend for thinking to similarly unify many diverse\nideas under a common framework. For a greater justification on this perspective, please see Section G\nC.2 Energy-Based Transformer (EBT) Thinking Types\nAll experiments in the paper are conducted with two main variants of EBTs, which we call System 1\n(S1) and System 2 (S2) EBTs. S1-EBTs have hyperparameters specifically optimized for stability\nand learning convergence, whereas S2-EBTs have hyperparameters optimized for System 2 Thinking\ncapabilities. Many of the pretraining scaling experiments conducted in Section 4 are with S1 models\nas to reduce the computational resources required for experimentation. To confirm that these results\nhold for S2 models, we plot the scaling trends of S1 and S2 models side by side; in Figure C.1, we find\nthat S2 models scale at the same or a higher rate than S1 models during training, but have a higher\nY-intercept. This Y-intercept offset does not affect asymptotic scaling behavior (as asymptotically\nthe scaling rate dominates), and hence, scaling trends that hold for S1 models should generally\nhold for S2 models. In fact, S2 models may even perform better than S1 models during pretraining\nasymptotically because of the higher scaling rate. Intuitively, switching from S1 to S2 models allows\nfor a compute trade-off between the model\u2019s pretraining performance and the model\u2019s System 2\nThinking capabilities.\nS1 models have the gradient of predictions detached between optimization steps to increase training\nstability. Conversely, following [67], the S2 models truncate backpropagation and avoid detaching\nprediction tensors between optimization steps. Additionally, the S2 models have all the energy\nlandscape regularization techniques described in Section 3.3, whereas the S1 models have none. We\nalso find that S2 models require a different value for the optimization step size, that the optimization\nstep size not be learned, and to perform a minimum number of optimization steps greater than one.\nC.3 Autoregressive Causal Energy-Based Transformers Efficient Implementation\nGPT-style decoder-only autoregressive Transformers are parallelizable due to making all next-state\npredictions simultaneously [82]. In this section, we detail the implementation of decoder-only\nautoregressive EBTs, which also parallelize all next-state predictions simultaneously, but involve\nmore complexity due to EBMs making predictions in the input space rather than the output space.10\n10For simplicity we often use the term matrix to refer to slices of tensors.\nFigure C.1: EBT S1 and S2 Scaling Comparison. Scaling rate of System 1 (S1) compared to System\n2 (S2) models. System 2 models have a higher Y-intercept but scale slightly faster than System 1\nmodels. Therefore, as the scaling rate ultimately dominates asymptotic scaling behavior, and not the\nY-intercept, scaling results in the main paper that hold for S1 models should generally hold for S2\nTo demonstrate why this poses a challenge, for the decoder-only autoregressive Transformer, consider\nthe case of the N \u00d7 N attention scores matrix after the causal mask has been applied:\nwhere \u03b1zi,zj represents the attention score (probability mass) from observed state zi to observed\nstate zj. Now, in the case of an EBM, where predictions are made in the input space, the desired\nN \u00d7 (N + 1) attention scores matrix would look like the following:\n\u03b1z2,z1 \u03b1z2,z2 \u03b1z2,\u02c6z3\n\u03b1zn,z1 \u03b1zn,z2 \u03b1zn,z3\n. . . \u03b1zn,\u02c6zn+1\nwhere \u02c6zj denotes a predicted state (as opposed to an observed state). This is challenging to compute\nbecause each \u02c6zj along the superdiagonal is unique for its row, as we chose for each prediction to\nbe made independently from each other prediction.11 Consequently, unlike standard attention in\nfeed-forward Transformers, this attention matrix cannot be computed with a matrix multiplication\n), as every element of the superdiagonal is a prediction and not an observed state.\nAdditionally, in traditional transformers, if the context length is N , the size of manipulated tensors\nwill generally be B \u00d7 N \u00d7 D where B is the batch size and D is the embedding dimension. However,\nbecause EBMs make predictions in the input space, the input tensor needs to be different to allow\nfor inputting predictions. Therefore, to distinguish these tensors, for a context length of N tokens\n(meaning we are given N tokens), we define a sequence of the first N elements as zo, or the observed\nsequence representations, and the final N elements as zp, or the predicted representations (for each\nnext element in the sequence). This means that the manipulated tensors within the EBT are of size\nB \u00d7 2N \u00d7 D for a context length of size N .\nThe values for zo, or the observed states, are computed identically as in the original transformer [83],\nas the attention scores of the observed states do not depend on the predicted states. This can be\nformalized as the following:\nAttention(Qo, Ko, Vo)zo = softmax\n11We note that it\u2019s possible to make each prediction not be independent, however, not making every prediction\nindependent would mean that there is stochasticity for each prediction caused not only by its initial value but\nalso by all initial values of all previous predictions.\nwhere Qo, Ko, and Vo are the Query, Key, and Value matrices of the observed states zo. In every\nblock of the transformer, the representations of all observed states are updated in this manner,\nindependent of the representations of the predictions.\nFor the computation of prediction representations, three matrices are computed, which we notate as\nQp, Kp, and Vp.12 First, we compute the self-attention scores of all predicted representations to all\nobserved representations:\nwhere \u02dcSzo\u2190zp denotes the raw unnormalized attention scores of the predicted states to the observed\nstates. Note, however, that the self-attention scores of each prediction with itself have not yet been\ncalculated\u2014due to the key matrix being from the observed states. Therefore, the superdiagonal of the\n\u02dcSzo\u2190zp matrix needs to be replaced with the self-attention scores of each prediction with itself to\nachieve the attention score matrix shown in Equation 3.\nTo achieve this matrix, we first need to append a column to the right side of the \u02dcSzo\u2190zp matrix,\nas the size of the matrix is currently N \u00d7 N , but the matrix needs to be N \u00d7 (N + 1) (N for the\nobserved states and then one for the predicted states along the second dimension). After doing this,\nwe first mask out the superdiagonal with zeros to ensure that the probabilities in the superdiagonal of\nthe score matrix only correspond to the values of predicted states with themselves. To make this\noperation differentiable, this masking operation is done through elementwise multiplication of a\nmatrix with ones everywhere except the superdiagonal, which has zeros. Then, we compute the\nself-attention scores of each predicted state with itself, using the following equation:\nwhere the \u2217 indicates the Hadamard product and the sum is across the feature dimension. Using\na superdiagonal mask again, we set the diagonal of the \u02dcSzo\u2190zp to these values. We denote the\nupdated matrix as \u02dcSzp , representing that this matrix is still unnormalized scores but takes into ac-\ncount interaction between zp and zo as well as from zp to themselves. Now, after applying the softmax:\nwe have the intended scores matrix shown in Equation 3. Note that for this softmax operation we\nadjust the standard causal attention mask to ensure all future information is masked out and the\nsuperdiagonal we added is not masked out.\nOne more barrier towards finally extracting all updated zp representations is the fact that we cannot\nsimply multiply this resulting scores matrix by the values matrix, as each element of the superdiagonal\ncorresponds to a different predicted state. Thus, using similar techniques to before, we first clone and\nthen extract the superdiagonal from this scores matrix using a diagonal mask.\nAfter extracting out the superdiagonal, zeroing it out after extraction, and removing the earlier\nappended right-most column, we can multiply the resulting scores matrix by the Vo matrix to get\nall of the representations summed together of each predicted state with all observed states. This is\ncomputed using the following matrix multiplication:\nAs we also need to add the representation of each predicted state weighted with its own attention\nscore (what was extracted on the superdiagonal), we perform another Hadamard product of the Vp\nmatrix with the cloned superdiagonal to get these values, and then add these element wise to the zp\nrepresentations. Now, we have computed the intended representations involving the scores matrix\nshown in Equation 3. Now, zo and zp are updated by multiplying these tensors by the shared output\nweight matrix Wo.\n12In practice, we shared the weights for the Q\/K\/V matrices for both observations and predictions to enable\na one-to-one comparison to existing feed-forward transformers. It would be interesting to experiment with\nusing different weight matrices to determine if that improves convergence and generalization at the cost of more\nTable D.1: Model sizes and hyperparameters for scaling experiments. For applicable model sizes\nwe follow Mamba [22].\nNon-Embedding Params\nembed. dim # heads\nTable D.2: Hyperparameters for Transformer++.\nOptimizer Momentum\nMinimum LR Scale\nGradient Clip Value\nImage Dimension\n\u03b21, \u03b22 = 0.9, 0.999\nLinear warm up cosine decay\nSD-XL VAE [114, 115]\nEleutherAI\/gpt-neox-20b [148]\nC.4 Autoregressive Energy-Based Transformers Simplified Implementation\nA more simplified implementation involves the entire attention matrices and a generalized causal\nmask, as described in [28]. However, because this implementation involves a matrix multiplication\nwith 2 times the sequence length, this results in 4 times the number of FLOPs as normal attention,\nwhich is around double the number of FLOPs of our more efficient implementation.\nD Experimentation Details\nTables D.2 and D.3 specify general model information and hyperparameters. We utilized the Llama 2\ntransformer implementation [87] for the Transformer++ and used this implementation as the backbone\nupon which we built causal decoder-only EBTs. We seed all libraries using PyTorch Lightning [147]\nfor all experiments with a seed of 33. For the Diffusion Transformer, we use the implementation\nfrom [26]\u2014for the bidirectional EBT we build upon this implementation.\nUnless otherwise stated, for all scaling experiments we copy the popular Mamba paper pretraining set-\ntings for the model configuration (we use different batch sizes\/data due to computational constraints),\nshown in Table D.1. Because we are compute-constrained, we also include two extra model sizes in\nthis table, extra extra small (xxs) and extra small (xs), which allow us to collect more data points\nfor the extrapolation of scaling trends. Because the hyperparameters used in these experiments were\ntuned for feed-forward Transformers in [85], we broadly expect that hyperparameters tuned for EBTs\nwill further increase the performance gap between EBTs and the Transformer++ in autoregressive\nmodeling. Future research should investigate the optimal width\/depth\/batch sizes for EBTs.\nD.1 Autoregressive Language Modeling Experimental Details\nD.1.1 Autoregressive Language Modeling Learning Scalability Details\nConducting thorough scaling experiments is extremely challenging\u2014a recent survey on \u201cscaling\nlaws\u201d [86] showed just how fragile many of these \u201cscaling laws\u201d are to hyperparameters, data, and\nother parameters, and how changing these variables slightly can lead to vastly different conclusions.\nBeing bottlenecked by a limited set of computing resources further exacerbates the issue of comparing\ntwo models. Therefore, we sought out to conduct controlled experiments that revealed the most\ninformation possible regarding the scaling of EBTs compared to different models.\nMost existing works study scaling by varying several factors simultaneously, including depth (number\nof transformer blocks) [85], width (embedding dimension) [85], possibly batch size [105, 149], and\nthe amount of data [85]. Therefore, to be more comprehensive in determining when EBTs scale\ndifferently than the Transformer++, we decided to conduct normal scaling experiments over all of\nthese factors at the exact same time (as is standard), as well as ablating over changing just one of\nthese factors at a time. Notably, conducting experiments in this manner allows for controlling a single\nindependent variable at a time (i.e., just changing the number of Transformer Blocks), which allows\nfor stronger conclusions regarding what aspects of model scaling different models perform better over\n(i.e., EBTs scale better then the Transformer++ when increasing the number of Transformer Blocks).\nScaling all factors at once does not allow for such insight, as there are many independent variables\nchanging at once. We believe experiments that involve changing a single independent variable are in\nbroader alignment with traditional scientific principles.\nFor the parameter and FLOP scalability experiments, all models are pretrained for 105k steps. For\neach model size, following [105, 149], we scale the batch size. For NLP experiments we use the\nfollowing batch sizes for the xxs, xs, small, medium, and large models respectively: 32, 46, 90, 170,\nand 256. These batch sizes were determined by scaling the batch size with the square root of the\nnumber of parameters and then rounding to an even number, similar to batch size trends in [149]. All\nmodel sizes use the same learning rate as in the Mamba paper [22] (0.0006, 0.0003, 0.00025, and\n0.0002 for small, medium, large, and xl models respectively), where for the xxs and xs models we\nuse a learning rate of 0.0012 and 0.0009 respectively. For the data scaling and batch size scaling\nexperiments conducted in Figure 4, we use xxs models (we also report small models in Section B).\nThe data scaling experiment used a batch size of 128 and the batch size experiments ran for 105k\nsteps. All models were trained with a context length of 256 and no FFN multiplier (FFN dimension\nbeing equal to the embedding dimension) due to limited resources.\nD.1.2 Autoregressive Language Modeling Thinking Scalability Details\nWe train xxs S2-EBT and Transformer++ models with the same setup as above, with the exception\nthat models are trained with a batch size of 128 for 1M training steps (Table D.4 contains more\nhyperparameter details). Increasing the data scale enables us to better understand how thinking scales\nduring pretraining. It\u2019s worth noting that since we are training small language models, they could\nnot benefit from modern techniques such as Chain of Thought (CoT) in improving performance.\nFigures 6, B.1, 7 and Table 3 use the best checkpoints of these two models. Figures 6a, 7 both use all\nfour downstream datasets shown in Table 3. Figure 7 also uses the pretraining dataset in addition to\nthese downstream datasets, where every dataset is represented as a separate OOD point.\nFigure 6b was solely from the Dyck Languages benchmark. We did not observe this trend in other\nbenchmarks, possibly due to perplexity not being a completely linear metric. Figure B.1a was on the\nRedPajamaV2 validation dataset.\nD.2 Autoregressive Video Experimental Details\nWe use the same model parameter scaling, shown in Table D.1, as the NLP experiments. We also\nused a batch size of 256 for all models, as we found that it did not affect the scaling performance due\nto models training for many epochs. We also processed videos with 0.25 seconds between frames.\nFor the Transformer++ baseline, we use the same learning rates as the NLP experiments. For EBTs,\nwe found that it was necessary to use a lower learning rate by a factor of 3 for proper training stability.\nWe use the standard SSV2 train and validation split for experiments. Other hyperparameters are\nshown in Figure D.2 and Figure D.3.\nD.3 Bidirectional Image Denoising Experimental Details\nWe use the COCO 2014 dataset [119, 120] with 128 by 128 images, its train\/validation split, a patch\nsize of 16, and the Diffusion Transformer implementation from [26]. All models were trained using\nthe large model size described in Table D.1, with a learning rate of 1e \u2212 4 for 100, 000 steps. For\nthe DiT baseline, we used the same hyperparameters from [26], changing only the batch size to\nTable D.3: Hyperparameters for EBT scaling experiments.\nOptimizer Momentum\nMinimum LR Scale\nGradient Clip Value\nImage Dimension\nNormalize Input Distribution\nOptimization Steps\nOptimization Step Size\nOptimization Step Size LR Multiplier\nLearnable Optimization Step Size\n\u03b21, \u03b22 = 0.9, 0.999\nLinear warm up cosine decay\nSD-XL VAE [114, 115]\nEleutherAI\/gpt-neox-20b [148]\n128 from 256. We based our bidirectional EBT implementation on the code from this repository.\nWe experimented with several different diffusion inference strategies to ensure a fair comparison,\nincluding DDPM, DDIM, increasing the number of diffusion steps at inference, and recursively\napplying the diffusion model on its own denoised output.13 Ultimately, we found that the combination\nof DDIM recursively applied on its own output performed best, hence we used this as the baseline in\nall experiments. We used the default denoising schedule from the DiT codebase [26].\nFor both DiTs and EBTs, we found that models performed best on OOD noise levels when denoising\ntheir own outputs twice, that is applying the model to denoise the image three times recursively. This\nis how we are able to achieve the results in Figure 12 demonstrating the performance for 300 forward\npasses from DiTs and 3 forward passes from EBTs. We found that for image denoising, it was not\nnecessary to train EBTs with the S2 hyperparameters for System 2 Capabilities to emerge, although\nits possible these would further improve performance. Additionally, for image classification, for both\nmodels, we take the average of all the final patch tokens, and for DiTs we feed in T = 0.\nD.4 Computational Resources\nAll experiments were conducted on either Nvidia A100s, H100s or GH200s, with the largest scale\nexperiment requiring approximately \u2248 1300 A100 GPU Hours. The runtime for each experiment\nwas dependent on the model sizes used as well as the amount of data trained on.\nD.5 FLOP Calculations\nWe adopt the standard estimate of 6N FLOPs per token for the AR Transformer++ [150], where N\ndenotes the number of non-embedding parameters. For AR EBTs, however, the per-token cost varies\nwith the number of training optimization steps and chosen hyperparameters.\nTo derive the FLOPs for EBT training, we follow [76] for the Hessian-vector product (HVP), which\nEBTs require to backpropagate through a first-order derivative. Since an HVP has the same theoretical\ncomplexity as a gradient computation, we express the per-step FLOPs as\nFLOPs = F + B + B.\nBased on [150], the forward and backward passes require approximately 2N and 4N FLOPs per\ntoken, respectively, where N denotes the count of non-embedding parameters in the Transformer.\nIn the autoregressive EBT implementation, the effective sequence length becomes twice that of the\noriginal Transformer (formally 2S \u2212 2 for an original sequence length S). Owing to the efficient\nscheme of Section C.3, this doubling of sequence length translates roughly into a two-fold increase\n13This recursive application was only performed during testing due to the distribution shift.\nTable D.4: Hyperparameters for EBT System 2 Thinking experiments.\nOptimizer Momentum (\u03b21, \u03b22)\nLearning Rate (LR)\nLearning Rate (LR) Schedule\nMinimum LR Scale\nGradient Clip Value\nNormalize Input Distribution\nOptimization Steps\nOptimization Step Size\nOptimization Step Size Multiplicative Random Factor\nLangevin Dynamics Noise\nLearnable Optimization Step Size\nNo Detach Between Optimization Steps\nTruncate Optimization\nLinear warm-up + cosine decay\nEleutherAI\/gpt-neox-20b [148]\n2-3 (randomized)\nin FLOPs, rather than a four-fold increase in FLOPs. Hence, each second-order optimization step\ndemands roughly\n(F + B + B) \u00d7 2 = (2N + 4N + 4N ) \u00d7 2 = 10N \u00d7 2,\nmaking it \u2248 3.33\u00d7 more expensive than a standard feed-forward Transformer step.\nThe overall FLOP count also depends on one\u2019s choice of hyperparameters. For S1 models, where the\nloss is evaluated at every iteration without gradient truncation, the total FLOPs simply multiply by\nthe number of steps. Therefore, for our pretraining experiments using two optimization steps, we get\nthat EBTs used 6.66\u00d7 the FLOPs of a comparable Transformer++ during training. In contrast, for S2\nmodels, a random number of optimization steps is used, the gradient is truncated, the loss is only\ncalculated at the last step following [48], and a Replay Buffer is used. Therefore, the FLOP count\nvaries and can both decrease (as truncating uses less FLOPs for earlier steps) as well as increase\n(as using more steps and a replay buffer both use more FLOPs). These numbers also vary during\ninference, where the full EBT implementation parallelizing all predictions at once is not necessary.\nGiven the scarcity of published methods for computing higher-order derivative FLOPs and our\ninability to leverage existing libraries for Hessian-vector products, these estimates remain approximate.\nWe welcome corrections or additional insights from readers familiar with FLOP calculations for\nsecond-order methods.\nE Additional Related Works\nE.1 Energy-Based Models (EBMs) as a Generalization of Diffusion Models and Recurrent\nBoth diffusion models [133] and RNNs [127] can be seen as predicting the score, or the gradient\nof the energy function\/data density, \u2207xE\u03b8(x), where diffusion models do this with an additional\ntime condition [151]. Thus, the largest benefit of explicit EBMs over these approaches, which can be\nseen as implicit EBMs (due to only implicitly defining an energy function), is that using an explicit\nEBM allows for explicit verification\/likelihood modeling. We show that this enables the use of\nself-verification to improve predictions, whereas with diffusion models and RNNs an additional\nverifier model is necessary to achieve this capability [19].\n(a) Diffusion Model\n(b) Energy-Based Model (EBM)\nFigure E.1: EBM and Diffusion Comparison. Diffusion models receive supervision at each step\nof the denoising process (e.g., for one thousand steps), whereas EBMs only receive supervision\nat the end of the optimization process. This training procedure allows EBMs to learn an entire\nEnergy Landscape over predictions, associating a scalar energy for every prediction according to\nits likelihood. Learning landscapes in this manner can reduce \u201cerror\u201d accumulation throughout\nthe denoising process [67] and makes EBMs more flexible by allowing unnormalized likelihood\nestimation at each step of the denoising process. Additionally, diffusion models update predictions\nby predicting the noise at each timestep, meaning they must follow a set denoising schedule. On\nthe other hand, EBMs update predictions by performing gradient descent with respect to the energy\nscalar, allowing for flexible inference where this optimization process can be performed for any\nnumber of steps. x here refers to some condition (e.g., a class or text) whereas y is the generated\nIt\u2019s also worth noting that RNN, diffusion models, and EBMs need not be incompatible with one\nanother. For example, [67] combines EBMs and diffusion to reason over challenging problems. This\ncan increase stability of the learned energy landscape by adding explicit score supervision.\nE.2 In Depth EBM and Diffusion Model Comparison\nBecause of the similarity of diffusion models and EBMs, we present a side-by-side comparison of\nthe training and inference approach for both in Figure E.1, where the primary difference lie in the\nsupervision they receive during training and the update rule for predictions. Additionally, we provide\nmore information to compare these approaches.\nUnder the assumption that the energy landscape is well formed and that optimization is well behaved,\nEBMs offer several distinct advantages over diffusion models. In EBMs, the energy function is\ntrained to represent a meaningful landscape where the energy value of a sample directly corresponds\nto its relative unnormalized likelihood. Consequently, two samples can be directly compared to\ndetermine which is more likely, in a single forward pass. On the other hand, diffusion models require\nrunning samples through the entire reverse diffusion process to get likelihoods, which often requires\nhundreds to thousands of steps, and rely on likelihood approximations such as ELBOs or numerical\nsolvers for SDEs\/ODEs. In practice, these result in incomparable likelihoods, as ELBOs only give\nlikelihood lower bounds and numerical solvers result in high approximation error [19].\nFurthermore, the learning of an energy landscape over predictions means that any approximation\nerrors at each individual step of the Markov Chain (optimization process) do not result in cumulative\nerror, as the minimum of the energy landscape can still be reached. This differs from diffusion models,\nwhere any approximation error at each step will result in increasing accumulated error across the\nentire Markov Chain [67] (demonstrated in Figure E.1).\nLastly, EBMs, giving an unnormalized likelihood estimate at each step, are in practice much more\nflexible for generation than diffusion models. While diffusion models require running the entire\nreverse diffusion process with a specific denoising schedule to generate a sample, EBMs can be\ntrained to directly predict the next sample in a single step, and give an unnormalized likelihood at\neach step indicating how likely they think this sample is. This better approximates human-like System\n2 thinking, where humans naturally evaluate the strength of current predictions, and on the basis of\nknowing how good their predictions are, decide to dynamically allocate more or less computational\nE.3 Additional Energy-Based Models Related Works\nOne contribution of this work was the design of a custom architecture for EBMs called the Energy-\nBased Transformer (EBT). Roughly similar in naming is the work of the Energy Transformer [152].\nHowever, despite similarity in the names of these architectures, they are very different\u2014with the\nprimary similarity in architectures being the usage of attention mechanisms as well as a global\nenergy function. The existing work integrated ideas from Modern Hopfield Networks, including\nRNNs, whereas in our work the architecture is non-recurrent and does not use associative memories.\nAdditionally, in this work with EBTs we focused on System 2 Thinking and scalability to high-\ndimensional problems, which the previous work did not experiment with.\nOther loosely related approaches to EBTs involve autoregressive Energy-Based Models, including\nE-ARM [153], EBR [140], and Residual EBMs [139]. E-ARM involves adding an objective to the\nlearning process to turn a traditional autoregressive model into an EBM, and as such does not achieve\ntwo of the cognitive facets discussed in Section 1. EBR and Residual EBMs involve the training of an\nEBM on top of an already pretrained autoregressive language model. Both works, however, leverage\na contrastive objective, which suffers from the curse of dimensionality.\nThe optimization procedure used to train EBTs can be seen as a form of denoising score matching [71,\n154]. Particularly, EBTs having predictions initialized at some Gaussian, and then optimizing\npredictions using the gradient of the energy function, can be seen as training EBMs to denoise by\nlearning the score of the data starting from a Gaussian prior. However, we find the optimization\nperspective is more intuitive, and this denoising score matching perspective is more similar to the\ndiffusion model training procedure than it is EBMs, involving multiple levels of noise rather than just\nF Additional Cognitive Facets\nIn this section, we describe in more detail the different facets of cognition, in addition to introducing\nadditional facets.\nFacet 1, Dynamic Allocation of Computation, can be formalized as Turing completeness (assuming\nan infinite length external memory). Because standard feed-forward Transformers have finite length\nprograms (i.e., they have finite depth), they are unable to be Turing complete at the granularity of\neach prediction being made. This also holds for common RNN variants that are only updated with\nnew sequence information. Diffusion models and EBMs, being able to iteratively compute functions,\nhave potentially infinite length programs, and therefore when augmented with an infinite external\nmemory can be Turing complete.\nFacet 4: Compositional Reasoning and Systematicity. Humans routinely solve novel tasks by\nrecombining familiar primitives (e.g., verbs with new arguments or visual parts into unseen objects),\na hallmark of compositional generalization well-documented in neuroscience [155]. In contrast,\nstate-of-the-art Transformers and diffusion models often fall short when evaluated on compositional\ngeneralization [156, 157]. Energy-Based Models (EBMs) seamlessly address these limitations:\nenergies for individual factors are composable in several different manners [133], enabling zero-\nshot generation of novel combinations without retraining, where gradient-based sampling provides\nan intrinsic mechanism to verify and iteratively correct compositions [133]. Thus, EBMs offer a\npromising path toward human-like systematicity that remains elusive for existing paradigms and\nG Counterarguments\nG.1 System 2 Thinking\nIn this paper, strong claims were made regarding the capabilities of current models and their ability\nto perform System 2 Thinking. However, there are common counterarguments to our claims, which\nwe address here.\nG.1.1 System 2 Thinking and Inference-Time Compute Term Usage\nWhether the computational effort spent at inference time fully captures what psychologists term\nSystem 2 Thinking is still actively debated. In Section C.1 we outline three reasons for preferring\nthe broader terminology of System 2 Thinking: (i) it naturally extends to settings such as contin-\nual learning where terms such as \u201cinference-time compute\u201d becomes ambiguous, (ii) it connects\nour discussion to a substantial body of cognitive-science work, and (iii) it offers a conceptually\nstraightforward entry point for readers beyond the machine-learning community.\nWe believe that Human System 2 Thinking encompasses a far greater depth and complexity than the\nspecific approaches explored in this paper, such as \u201cThinking Longer\u201d and \u201cSelf-Verification.\u201d We\nwish to emphasize that our work does not claim current models replicate the full spectrum of human\nSystem 2 Thinking. Rather, we view the methods presented here as foundational steps toward that\nmore ambitious long-term goal.\nWe propose that \u201cSystem 2 Thinking\u201d offers a useful umbrella term that can effectively encompass\nand generalize other existing terminologies, including \u201cinference-time compute,\u201d \u201ctest-time compute,\u201d\nor \u201creasoning.\" A parallel can be drawn with the term \u201clearning\u201d in our field. \u201cLearning\u201d itself\nhas evolved to describe a wide array of processes, some of which, such as k-Nearest Neighbors\n(KNNs) or the specific mechanisms of weight matrix updates in Artificial Neural Networks (ANNs),\nrepresent distinct facets rather than the entirety of human-like learning, meaning these approaches\nmay not encompass the complexity of true human-like learning. However, despite this breadth and\nthese simplicities, \u201clearning\u201d has become a cornerstone term, upon which our entire field of machine\nlearning is named upon.\nIn a similar vein, while the \u201cthinking\u201d exhibited by the models discussed in this paper may not yet\ncapture the full nuance and intricacy of human cognition, we believe the term \u201cSystem 2 Thinking\u201d\ncan still serve a valuable role. It offers a generalizing framework for existing vocabulary and can\ncontribute to making complex concepts within the field more accessible and understandable to\nnewcomers or experts from other fields. Our intention is to contribute to a constructive and unifying\ndialogue of intelligent systems.\nIs Chain-of-Thought (CoT) Sufficient for System 2 Thinking?\nCoT is commonly believed to be sufficient for advanced reasoning to emerge in LLMs. However,\nwe argue that there are several flaws with CoT preventing advanced thinking capabilities. First,\nChain-of-Thought (CoT) involves reasoning over a discrete state space, which limits the granularity\nof \u201cthoughts.\u201d Second, CoT is not an intrinsic architectural capability but an external procedure\napplied to token sequences. Ideally, such reasoning should be embedded within the model and\nlearned during training (unsupervised training, particularly). Third, each token is produced with a\nfixed computational budget, restricting the depth of reasoning per step. In contrast, humans allocate\nvariable effort across steps when reasoning \u201cstep by step\u201d. Similarly, models should be able to spend\na variable amount of computation per token, as enabled by EBTs. This aligns with the intuition\nbehind the saying: \u201ca chain is only as strong as its weakest link\u201d\u2014each step in a chain of thought\nshould receive sufficient computation to avoid failure points that result in bad reasoning.\nH Energy-Based Models (EBMs) Introduction\nH.1 Simplified Energy-Based Model (EBM) Introduction\nFeed-forward neural networks generally take the form of: given an x predict y (Fig. H.1a). Energy-\nBased Models (EBMs) are a family of models that learn the compatibility (unnormalized probability)\nover all possible combinations of x and y (Fig. H.1b). Intuitively, this can be seen as learning to\nverify the strength of y as a prediction with x as the input. Training models in this manner allows for\nrepresenting multiple plausible versions of y compatible with a given x. The differences between\nthese models is visualized in Fig. H.1.\nFormulating models in this manner ultimately brings about two primary questions:\nFirst question: Assuming we still care about ultimately predicting \u02c6y how do we use such an EBM to\npredict \u02c6y? With feed forward models, generally we can just input x and get the output of the model\nas \u02c6y, but we can\u2019t do this with EBMs?\nWith EBMs, what happens is conceptually similar to diffusion models [114], where we (commonly)\ninitialize \u02c6y as random noise. Then, we input x and \u02c6y into the model, and get a single scalar energy\noutput (our initial energy output) from the model. Now, because our entire model is differentiable,\nwe can get the gradient from this energy scalar to \u02c6y and perform gradient descent along the energy\nlandscape (energy landscapes are surfaces resulting from mapping all possible predictions to scalar\nvalues, visualized in Figure 3) using this gradient (this is the key)! This process is visualized in\nFigures 3, 2. This gradient can be seen as the opposite of the noise (e.g., denoising) and therefore\nEBMs have strong relations with Diffusion models predicting the noise. EBMs can be seen as a\ngeneralization of diffusion models, where diffusion models are predicting the gradient of the energy\nfunction\/scalar (more on this in Section E).\nSecond question: How do we train an EBM? Generally, models are trained over a dataset of x and y\npairs, but now there are several different possible y values that can be associated with any given x\nvalue\u2014so how does that work?\nIt turns out that all the training techniques for EBMs boil down to two main categories: contrastive\nand regularizing approaches [42].\nContrastive approaches are more common for EBMs and are easier to rationalize about due to their\nsimilarity to GAN discriminators. The idea behind contrastive approaches is to push down on the\nenergy of positive samples (i.e., the true data), and to push up on the energy of negative samples.\nWhile these positive samples are easy to rationalize about, as they are just the true data, the difficulty\nof contrastive EBMs is finding negative samples. Several approach exist, such as GANs, which use a\ngenerator to amortize negative sample generation, or running MCMC (similar to optimization) for\nsome time. However, as discussed in Section 3.1, such approaches don\u2019t scale well due to the curse\nof dimensionality.\nTherefore, to achieve a scalable EBM approach, we train EBMs through an optimization procedure\n(which has strong resemblance to Langevin Dynamics). That is, EBMs are trained to, starting from\nan initial prediction, optimize predictions to the ground truth solution (shown in Figures 3, 2). This\npushes the energy landscape to be locally convex surrounding the ground truth solution, thereby\nregularizing the energy landscape to have low energy only on the true data. As mentioned in [71],\nthis can be seen as being similar to denoising score matching [154].\nCommonly, when people learn about EBMs and the iterative denoising\/optimization procedure\nperformed during training, they think of diffusion models, so we include a more in depth comparison\nbetween the two in Section E.2.\n(a) Feed Forward Model\n(b) Energy-Based Model\nFigure H.1: Feed-Forward and Energy-Based Model Comparison. Feed-forward models (a),\ngiven an input x, directly try to predict \u02c6y. Instead of just getting x as an input, EBMs receive both x\nand \u02c6y as an input and learn the compatibility of all possible values of \u02c6y with x by outputting a scalar\nenergy value for each combination. Low energy corresponds to high probability, and high energy to\nlow probability. In practice, \u02c6y is often initialized as random.\nH.2 Energy-Based Model Types\nBecause EBMs are a broad modeling framework, and can generalize many existing approaches, we\naim to provide precise language to distinguish EBM types. We broadly classify the EBMs described\nthroughout this paper as explicit EBMs, meaning they explicitly define an energy function over\ninputs as the entire function being learned. In other words, explicit EBMs directly map all variables\n(inputs) to a single scalar energy as the output of the neural network. We define these in contrast\nto implicit EBMs, or EBMs where the energy function is not the learned model but rather some\nimplicit definition of the learned model. For example, with diffusion models, the energy function is\nimplicitly defined by the learned score network (s\u03b8(x, t)) as the following:\n\u2207xE(x, t) = \u2212 s\u03b8(x, t)\ns\u03b8(u, t) du + C(t) .\nOther notable examples of implicit EBMs include Hopfield Networks [158], RNNs [25], and Boltz-\nH.3 Energy-Based Model Frequently Asked Questions (FAQ)\n\u2022 What is energy\/compatibility, what does it represent, and how is it learned? What\nenergy corresponds to what probability?\nEnergy is just a learned compatibility score between x and y (lower means more likely).\nThe EBMs described in this paper learn it implicitly as described in Section 3.1 such that\nthe true data (good pairs) have low energy and bad pairs (non-data) have higher energy.\nProbabilities follow:\np\u03b8(x, y) \u221d e\u2212E\u03b8(x,y)\nso the energy E is essentially the (unnormalized) negative log-likelihood up to an additive\nconstant. The term compatibility is just a term used for intuition.\n\u2022 Does training EBMs require a full Hessian calculation?\nNo\u2014the approach described in the paper only requires Hessian-vector products. That makes\ntraining only about a constant 1.66\u00d7 as expensive as a vanilla feed-forward model given\neverything else remains constant and you use a single step.\n\u2022 Why is low energy good (and high energy bad)? Why not just use probability?\nLow energy is good because of the negative exponential. The reason we don\u2019t use prob-\nabilities is avoiding normalized probabilities makes the problem much more tractable in\nreal-world high-dimensional continuous state spaces by removing the focus on explicit\nnormalization via regularizing the partition function. EBMs come from a long line of work\nin statistical physics.\n\u2022 Is it okay that energies are unnormalized probabilities?\nYes, for most real-world applications, you only ever need sample relative likelihood\ncomparison; it\u2019s significantly less common to need the exact likelihood of samples. An\nexample of this is reward models, which can be seen as EBMs (just multiplying the reward\nby \u22121), where all that really matters is the relative reward for choosing which sample to use\nor which behavior to perform.\n\u2022 Is it fine to not do Maximum Likelihood Training?\nContrary to what your intuition may say, the answer is yes! For most real-world distributions,\ndata lies completely concentrated on a very thin manifold with no defined distribution\noutside of this manifold. Thus, directly doing Maximum Likelihood Estimation (MLE)\ntraining would push EBMs to have low energy on the true data manifold and then infinite\nenergy off that manifold (as the probability of such samples is 0). We don\u2019t want this as it\nwould make the score (gradient of the energy function) undefined and the energy landscape\nuntraversable\u2014so not doing MLE makes the problem tractable.\nI Energy-Based Transformers (EBTs) Tutorial\nI.1 Improving Stability and Scalability\nEnergy-Based Models are notorious for instability during training [51\u201354]. Therefore, we experiment\nwith several different hyperparameters to increase the stability and scalability of EBTs and EBMs in\nI.1.1 Optimization Step Size and Stability\nWe found that the step size for gradient descent updates of predictions (\u03b1) was one of the primary\nfactors affecting the stability of EBTs. Thus, for S1 models, we make the step size a learnable\nparameter (this is not the case for S2 models). We calculate its learning rate by multiplying the\nmodel\u2019s learning rate by the step size learning rate multiplier. We found that the values for the step\nsize have a large effect on the magnitude of gradients generated for the optimization of predictions.\nThis is because the step size is directly multiplied by the prediction gradients. Particularly, a smaller\nstep size results in larger generated gradients, whereas a larger step size results in smaller gradients.\nTherefore, the step size needs to be tuned per modality, as the update required for predictions depends\non data. It\u2019s also worth noting that we do not weight decay the step size in any of the models.\nWe also found that a relatively high optimization step size was necessary (30, 000 for video and\nbetween 5 and 500 for text). Without a high optimization step size, gradient magnitudes continued to\nincrease throughout training, resulting in unstable training dynamics.\nI.1.2 Architecture Stability\nFor autoregressive models, we found that simply prepending a learnable \u201cstep\u201d embedding to\nsequences significantly improved scalability and stability, especially for S1 models. This step\nembedding mapped a discrete step index (i.e., step 0, 1, etc.) of the current optimization step to an\nembedding the same dimension as the model\u2019s embedding. We believe this helped improve stability\nby enabling the accumulation of attention mass, as well as enabling less steep energy landscapes\nconditioned on the optimization step.\nAdditionally, we experimented with adaptive layer normalization from the DiT architecture, but\nwe found that the timestep embedding worked better. We also experimented with several different\nnormalization and initialization approaches, where we found that the standard Llama2 [87] archi-\ntecture and initialization worked best. This involves using RMSNorm, Xavier init [159], SwiGLU\nMLP [160], and RoPE [161].\nI.1.3 System 2 Thinking Hyperparameter Stability\nFor S2 models, we found certain hyperparameters to be essential for the stability and scalability of\nmodels. First, we found that using a lower number of optimization steps resulted in more stability, as\nusing more optimization steps necessitates longer gradient chains. Additionally, we found that the\nstrategy used for the randomization of \u03b1 to be very important for stability. Particularly, we found that\nrandomizing \u03b1 with a single value for an entire batch resulted in issues with training convergence. We\nbelieve this is because using the same value for every element in the batch resulted in high-variance\ngradients. Thus, when randomizing \u03b1 differently for every batch and sequence element, we found\ntraining convergence to be more stable. It\u2019s possible that randomizing the number of optimization\nsteps would yield similar results in reducing gradient variance, however, we did not experiment with\nsuch a configuration.\nI.1.4 Clamping Optimization Gradients for Stability\nWe found that clamping gradients of the energy function with respect to predictions (or prediction\nupdate gradients) could help improve training stability at the cost of some slight reductions in\nconvergence speed. We do not conduct any experiments in the paper with clamped prediction\ngradients as we found that the other hyperparameters were sufficient for stable training, however, it\u2019s\na potentially useful trick worth discussing.\nI.1.5 Normalizing Data for Stability\nWe found that normalizing\/standardizing input data was crucial for the stability of EBTs. An example\nof this was in our NLP experiments, where we ran experiments with and without normalizing proba-\nbility distributions. The experiments with unnormalized distributions often had extreme activations\nas well as large loss spikes, whereas the experiments with normalized distributions (by applying\nsoftmax) were stable.\nI.2 How to Train Your EBT\nThe hyperparameters used for training EBTs are extremely important and can often be highly sensitive\ntowards performance, so here we offer a guide toward hyperparameter tuning. First, we recommend\nstarting off with training S1-EBTs, which are the easiest and most stable variant of EBTs not designed\nfor System 2 Thinking. Then, once S1-EBTs can be trained succesfully and scaled, we recommend\nchanging the hyperparameters gradually towards the hyperparameters used for S2 models (we say\ngradually here as occasionally these parameters can cause instability and require additional tuning).\nWhen training S1 EBTs, we recommend tuning hyperparameters in the following order:\n\u2022 First, tune standard hyperparameters such as Learning Rate (LR), batch size, etc. Having a\nhigh batch size helps with stability by making gradients less noisy (because you initialize\npredictions from random noise this makes gradients noisier).\n\u2022 Second, start tuning S1-specific hyperparameters\u2014primarily alpha and its LR multiplier\n(we recommend keeping its LR multiplier around 3x the value of alpha) and then tuning the\nnumber of optimization steps.\n\u2022 Third, potentially tune whether the step size is learnable and try other EBT architectures\n(inspired by DiT [26] we tried a time embedding as well as adaptive layer normalization).\nOnce you have tuned these, the model should be stable and fine for most use cases. At which point, if\nyou are desiring System 2 capabilities, you can proceed to the S2 models I.3. Some potential metrics\nto monitor and look out for include the gradient magnitudes (if these increase too much or spike a lot\nthat\u2019s a bad sign) and the gap between the initial and final energy after optimization (if this is too\nhigh or low it could be a sign your model\u2019s alpha value needs to be adjusted).\nFollowing [71], we give pseudocode for training EBTs in natural language for language modeling\n(Listing 1) as well as in computer vision for autoregressive video modeling (Listing 2). The pseu-\ndocode is primarily for S2 models without any energy landscape regularization techniques. The\nfirst primary design decision in the presented pseudocode is whether or not to detach predictions in\nbetween steps. Not detaching predictions in between steps allows for more \u201cThinking Time\u201d before\nmaking predictions, but makes the gradient computation graph longer and therefore increases the\nlikelihood of stability issues with gradients. Similarly, calculating the loss at every step versus solely\nthe last step enables model to \u201cthink for longer\u201d before needing to make accurate predictions, and\ntherefore affects the convexity and smoothness of the energy landscape. For S2 models, we found\nthat not detaching between steps was best, and similarly that calculating the loss only at the last step\nwas best. For S1 models, we found the opposite to be most stable. Generally, if one is calculating\nthe loss only at the last step, then one should not detach between steps as it\u2019s best if the gradient\npropagates to previous steps in this case. For more details on these techniques, we refer the reader to\nthe source code as well as Section I.3.\n# make sure to enable gradient tracking\nwith torch . set_grad_enabled ( True ) :\nloss_fn = nn . CrossEntropyLoss ( weight = None , ignore_index =\nto k e niz er_ p a d_ t ok e n_ i d )\ncontext_embeddings = self . embeddings ( input_ids [: , : -1]) # B , S , D\nnext_tokens = input_ids [: , 1:]\nnext_embeddings = self . embeddings ( next_tokens ) # B , S , V ; are just\nused for shaping next tensor\np re d i ct e d _ di s t r ib u t i on s = torch . randn_like ( next_embeddings ) # B , S\n, V ; initialize predictions as random\nfor _ in range ( num_steps ) :\n# Can optionally detach predicted distributions so that no\ngradient flows through past steps\n# pr e d i ct e d _ di s t r ib ut io n s = p r ed ic te d _d is tr i bu ti o ns . detach ()\npr edi ct ed_ embe dd in gs = self . vocab_to_embed ( softmax (\np r e di c t e d_ d i s tr i b ut i o n s ) ) # B , S , D ; need to proj . to embed space\nfor transformer to work in , use linear layer , weighted sum , etc\nall_embeddings = torch . cat (( context_embeddings ,\np red ic ted _emb ed di ng s ) , dim =1) # B , 2S , D\npred ic ted _e ne rgi es = self . transformer ( all_embeddings ) # B , S ,\n1; this returns only energies for the predicted_embeddings\n# Compute the gradient of predicted energies w . r . t . predicted\np r e d i c t e d _ d i s t r i b u t i o n s _ g r a d = torch . autograd . grad (\npredicted_energies . sum () ,\npredicted_distributions ,\ncreate_graph = True\n) [0] # B , S , V\n# Perform gradient descent w . r . t . the energy function where\nself . alpha is the optimization step size\np r e di c t e d_ d i s tr i b ut i on s = pr ed ic t ed _d is t ri bu t io ns - self . alpha\n* p r e d i c t e d _ d i s t r i b u t i o n s _ g r a d\n# Calculate cce loss based on predicted and ground truth\ndistributions , optionally at each optimization step or only at the\ncce_loss = loss_fn ( predicted_distributions , next_tokens )\nListing 1: Autoregressive Language Model Training Pseudocode in PyTorch\n# make sure to enable gradient tracking\nwith torch . set_grad_enabled ( True ) :\nloss_fn = torch . nn . SmoothL1Loss ( beta =1.0) # use whichever loss\nfunction desired\ncontext_embeddings = embeddings [: , : -1] # B , S , D\nnext_embeddings = embeddings [: , 1:] # B , S , D\np red ic ted _e mb ed di ng s = torch . randn_like ( next_embeddings ) # B , S , D\n; initialize predictions as random\nfor _ in range ( num_steps ) :\n# Can optionally detach embeddings so that no gradient flows\nthrough past steps\n# pr ed ict ed _em be ddin g s = predicted_embeddings . detach ()\nall_embeddings = torch . cat (( context_embeddings ,\np red ic ted _emb ed di ng s ) , dim =1) # B , 2S , D\npred ic ted _e ne rgi es = self . transformer ( all_embeddings ) # this\nreturns only energies for the predicted_embeddings # B , S , 1\n# Compute the gradient of predicted energies w . r . t . predicted\np r e d i ct e d _ e m b e d d i ng s _ g r a d = torch . autograd . grad (\npredicted_energies . sum () ,\npredicted_embeddings ,\ncreate_graph = True\n) [0] # B , S , D\n# Perform gradient descent w . r . t . the energy function where\nself . alpha is the optimization step size\npr edi ct ed_ embe dd in gs = p redic ted_embeddings - self . alpha *\np r e d i c te d _ e m b e d d i ng s _ g r a d\n# Calculate reconstruction loss based on predicted and ground\ntruth embeddings , optionally at each optimization step or only at\nreco nstr uc tion _lo ss = loss_fn ( predicted_embeddings ,\nnext_embeddings )\nListing 2: Autoregressive Video Model Training Pseudocode in PyTorch\nI.3 How to Think Using Your EBT\nOnce an S1 EBT has been trained, we recommend tuning the System 2 hyperparameters in the\nfollowing manner:\n\u2022 Remove detaching tensors between optimization steps and add loss truncation so the loss is\nonly calculated at the final step.\n\u2022 Then, tune alpha, as it\u2019s by far the most important EBT-specific hyperparameter. But, do not\nmake it learnable.\n\u2022 Next, tune the number of optimization steps, including potentially a minimum and maximum\nnumber when randomizing the number of steps.\n\u2022 Then add a replay buffer, Langevin Dynamics, and eventually a randomized alpha (step\nsize). Tune all of these in tandem while tweaking the earlier parameters (particularly alpha).\nIt\u2019s possible that randomizing the number of steps for each sample within a batch would work better\n(similar to randomizing the alpha value within a batch). Additionally, it\u2019s worth mentioning that all\noptimization steps are performed along the same energy landscape (same time embedding condition),\nunlike with S1-EBTs using multiple time steps.",
    "embedding":[
      -0.1047344357,
      0.0243072473,
      0.0097531769,
      0.0399812981,
      0.02002565,
      -0.0507065766,
      -0.0105926329,
      0.0573632829,
      -0.0108970283,
      -0.0149909835,
      -0.0598066039,
      -0.049285356,
      0.0734628215,
      0.0397185944,
      0.0760412216,
      -0.0045506833,
      0.0479154661,
      0.0749422088,
      -0.0855926201,
      -0.119714275,
      0.0964526907,
      0.0044947248,
      0.0257782731,
      -0.0465435423,
      0.0764269754,
      0.0198883917,
      0.0231693238,
      -0.0233864915,
      0.0384834893,
      -0.0177163985,
      -0.0046060099,
      0.0007810831,
      -0.0829021186,
      0.0021347106,
      -0.0350010283,
      0.1045888588,
      -0.0356466286,
      0.0464544185,
      -0.009586378,
      -0.0098598171,
      -0.0347113088,
      -0.0093860524,
      0.0322356448,
      -0.0194492117,
      0.0503266864,
      -0.0627788752,
      0.0445634276,
      -0.0668681115,
      -0.0926048756,
      -0.02843724,
      -0.0442386419,
      -0.0956865549,
      0.0165889617,
      0.0640900508,
      0.0126579478,
      0.0074405437,
      0.0653918684,
      -0.0036898009,
      -0.0357395224,
      -0.0609057583,
      -0.1046135798,
      -0.085312888,
      -0.0647150576,
      -0.0818489864,
      -0.0392959565,
      -0.0139359068,
      0.0672363117,
      0.0515422262,
      -0.0131792584,
      -0.0116314329,
      -0.0296928771,
      0.0235476382,
      -0.0407430381,
      -0.0149707329,
      0.0494926237,
      -0.0265067369,
      0.0569435991,
      -0.0117571549,
      0.0745465234,
      -0.0460547693,
      0.0464718975,
      -0.018415954,
      -0.0202212669,
      -0.0071615493,
      0.0098864455,
      0.0306313653,
      -0.0513817258,
      0.0596333779,
      -0.0015016268,
      -0.0119562019,
      0.0074179028,
      -0.0598380454,
      0.0287354216,
      0.0353827886,
      0.0945190266,
      0.0107784476,
      -0.0214739554,
      -0.0289811566,
      -0.0098171514,
      0.0787418708,
      -0.0155882938,
      0.0923158377,
      0.0281184651,
      -0.0379644372,
      -0.0084968023,
      -0.0535611436,
      0.0082447398,
      0.1185147464,
      0.0301482398,
      -0.0753122717,
      0.0628491268,
      0.0176875517,
      -0.0264594331,
      -0.0159001928,
      0.0403379016,
      -0.0086594559,
      0.0272955205,
      -0.0244736671,
      -0.0524069704,
      0.0340061747,
      0.0123642096,
      0.0700372905,
      0.0244944338,
      0.0264117252,
      0.0730340034,
      -0.0768692493,
      -0.0456821956,
      6.645793964e-33,
      -0.0443824716,
      0.0790378824,
      0.0927766114,
      0.1123465672,
      -0.0272939075,
      0.005543449,
      -0.0291757379,
      0.0143337501,
      0.0469564013,
      -0.0080560409,
      -0.026756132,
      0.0756498575,
      -0.0430212095,
      0.0618343726,
      0.0873189718,
      -0.0492915139,
      -0.0166408364,
      -0.0076187253,
      0.0386859402,
      -0.0031090945,
      0.1098318547,
      -0.0003183717,
      0.0016240062,
      -0.0871697366,
      -0.0214710087,
      0.0024597822,
      0.0565985329,
      -0.0393079259,
      0.0219419636,
      0.0015479387,
      -0.0457688794,
      0.0451028869,
      -0.1165879071,
      0.0503334366,
      -0.024648387,
      -0.0696016476,
      0.0636398941,
      -0.0629943982,
      0.0149197467,
      -0.0749201402,
      -0.0029798204,
      0.0436834581,
      0.0216386896,
      -0.0332839079,
      -0.0293050259,
      -0.0499376208,
      0.1035806537,
      -0.044182878,
      -0.0425675362,
      0.0161173195,
      0.0240459535,
      0.0120764636,
      -0.0546029545,
      -0.0809755251,
      0.0721789747,
      0.0270778127,
      0.0456028469,
      0.0605763383,
      0.1213320047,
      0.0728372708,
      -0.0755997971,
      0.0138567854,
      -0.0006391569,
      -0.0108229546,
      0.0141359288,
      0.0050717597,
      -0.0609879531,
      -0.0006056796,
      0.0505309105,
      0.0012558531,
      0.0000880585,
      -0.0245952383,
      -0.0263376739,
      -0.056258671,
      0.1136013046,
      -0.0663199499,
      0.0784666613,
      -0.0014074177,
      -0.0682980865,
      0.0209643058,
      -0.0168516524,
      -0.0074555161,
      0.0236033928,
      -0.0272552948,
      0.0130090704,
      -0.0282179136,
      0.0036303548,
      -0.0421702936,
      -0.0686525628,
      -0.0305856932,
      -0.027599778,
      0.0507047437,
      -0.0340571143,
      0.0228109658,
      -0.003385379,
      -6.900175349e-33,
      -0.0232363623,
      0.0098187113,
      -0.0573405921,
      0.0653587952,
      0.0945413038,
      -0.0402507931,
      -0.0171999931,
      -0.0334608406,
      -0.0163114853,
      -0.0515584275,
      -0.0487835035,
      -0.084277533,
      0.0133206006,
      -0.0473629795,
      0.0559863225,
      -0.0437492095,
      -0.0288831517,
      -0.046175167,
      0.0779261664,
      0.0734592006,
      -0.0222951677,
      0.1211284474,
      -0.1430010945,
      -0.0627545938,
      -0.0307529327,
      0.0586043,
      -0.0548912324,
      0.0855074301,
      0.0220432375,
      0.0167210326,
      -0.0699456111,
      -0.0768145919,
      -0.0458611362,
      -0.006148112,
      -0.0589907393,
      0.0302964784,
      0.0473709591,
      0.0347226337,
      -0.0476636402,
      0.0988323241,
      0.0122095039,
      -0.0504663587,
      -0.0968301445,
      -0.0239170026,
      -0.0511429496,
      -0.0034547022,
      -0.0501586795,
      -0.0494716354,
      0.0390320085,
      -0.0103289755,
      0.0698074251,
      0.0349834338,
      -0.1038467884,
      -0.0582394823,
      -0.0233015027,
      -0.0896533951,
      0.0550566614,
      -0.0497817248,
      0.0777490512,
      -0.0322003141,
      -0.0636077598,
      -0.0372559577,
      0.0958720744,
      -0.0228189696,
      -0.0135229118,
      -0.0363828763,
      -0.0260877106,
      -0.0236826632,
      0.0539967008,
      0.0504128262,
      -0.0451141633,
      -0.0093848547,
      0.0041710646,
      -0.0489510708,
      0.044627808,
      -0.0039379285,
      0.037518274,
      -0.0061138486,
      -0.0299509075,
      -0.116095297,
      0.0030136185,
      -0.0049293153,
      -0.0054913787,
      -0.0206923243,
      0.0139647666,
      -0.0027360143,
      0.0540280864,
      0.0200474691,
      0.0192384589,
      -0.0147597995,
      -0.0766237304,
      0.0428393222,
      0.026717674,
      0.1005956084,
      -0.0697413981,
      -0.000000062,
      -0.0240499973,
      0.0274152122,
      0.0511934683,
      0.0409550853,
      0.0258113053,
      -0.0051112412,
      -0.0516906306,
      0.0493640378,
      -0.0017631755,
      0.0371268503,
      0.0911538452,
      -0.0309021156,
      0.0527953766,
      0.0052872617,
      0.0410674661,
      0.0909942091,
      0.0710225776,
      -0.0057450305,
      -0.0349517055,
      -0.0395489708,
      0.0231524315,
      0.0378058478,
      -0.0557605699,
      -0.0094182026,
      0.0480397604,
      -0.0551049076,
      -0.0124519346,
      0.0114106918,
      0.0478474982,
      0.0591753088,
      -0.0827183798,
      0.0293891914,
      0.0132690398,
      -0.0444364399,
      0.055325821,
      -0.0470321402,
      0.0455525331,
      -0.0332678147,
      -0.0197028536,
      -0.0054492815,
      0.0084633436,
      0.0578792989,
      -0.1023917422,
      0.0492734797,
      0.067198962,
      -0.0435201302,
      0.0123954527,
      -0.1176115125,
      -0.0056224903,
      0.0817034096,
      0.0496878251,
      0.0054944069,
      -0.04651298,
      -0.0010393556,
      0.0596723892,
      0.070828259,
      0.0287659075,
      -0.1027605906,
      -0.0382949449,
      0.036798235,
      0.0248758011,
      -0.0040776506,
      -0.076411128,
      0.0349981934
    ],
    "cluster":4,
    "time":"00:01:40"
  },
  {
    "id":7,
    "url":"https:\/\/arxiv.org\/pdf\/1706.03762",
    "content":"Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\navaswani@google.com\nnoam@google.com\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nScaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\ndk, and apply a softmax function to obtain the weights on the\nquery with all keys, divide each by\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\n. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1\u221a\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\ni=1 qiki, has mean 0 and variance dk.\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = (cid:80)dk\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\nWhere the projections are parameter matrices W Q\nand W O \u2208 Rhdv\u00d7dmodel.\ni \u2208 Rdmodel\u00d7dk , W K\ni \u2208 Rdmodel\u00d7dk , W V\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel\/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nComplexity per Layer\nSelf-Attention (restricted)\nSequential Maximum Path Length\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nP E(pos,2i) = sin(pos\/100002i\/dmodel)\nP E(pos,2i+1) = cos(pos\/100002i\/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208 Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n\/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n\/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4 Regularization\nWe employ three types of regularization during training:\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nDeep-Att + PosUnk [39]\nDeep-Att + PosUnk Ensemble [39]\nGNMT + RL Ensemble [38]\nConvS2S Ensemble [9]\nTransformer (base model)\nTransformer (big)\nTraining Cost (FLOPs)\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1\/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\npositional embedding instead of sinusoids\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative\nWSJ only, discriminative\nWSJ only, discriminative\nWSJ only, discriminative\nWSJ only, discriminative\nsemi-supervised\nsemi-supervised\nsemi-supervised\nsemi-supervised\nsemi-supervised\nPetrov et al. (2006) [29]\nZhu et al. (2013) [40]\nDyer et al. (2016) [8]\nTransformer (4 layers)\nZhu et al. (2013) [40]\nHuang & Harper (2009) [14]\nMcClosky et al. (2006) [26]\nVinyals & Kaiser el al. (2014) [37]\nTransformer (4 layers)\nLuong et al. (2015) [23]\nDyer et al. (2016) [8]\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https:\/\/github.com\/\ntensorflow\/tensor2tensor.\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs\/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs\/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs\/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs\/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks.\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs\/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs\/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\nAttention Visualizations\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\nInput-Input Layer5ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>ItisinthisspiritthatamajorityofAmericangovernmentshavepassednewlawssince2009makingtheregistrationorvotingprocessmoredifficult.<EOS><pad><pad><pad><pad><pad><pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\nInput-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\nInput-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>Input-Input Layer5TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>TheLawwillneverbeperfect,butitsapplicationshouldbejust-thisiswhatwearemissing,inmyopinion.<EOS><pad>",
    "embedding":[
      -0.0548286438,
      -0.108554922,
      0.0106587997,
      0.0021563941,
      0.0550471433,
      0.0289627649,
      -0.0608086996,
      0.0239283871,
      0.1092782989,
      -0.0356810577,
      0.020663755,
      -0.0252872799,
      -0.0247263927,
      0.0355646275,
      -0.0962012634,
      -0.0073005315,
      0.05264768,
      0.0735860541,
      -0.0855796412,
      -0.0918256864,
      0.0360924453,
      0.0316304825,
      0.058891654,
      0.0176442433,
      0.0245393775,
      -0.0120451758,
      -0.0849028826,
      -0.0671260953,
      -0.0207866915,
      0.0042731073,
      0.0053154207,
      -0.0347093828,
      -0.0070306906,
      0.102350682,
      -0.0698995069,
      0.0842456371,
      -0.0451148786,
      0.0004448921,
      0.0286938529,
      -0.0481086634,
      0.0238583498,
      -0.0300814882,
      -0.0091145793,
      0.0245828219,
      0.1338485181,
      0.0061660255,
      0.0347820111,
      0.0045981435,
      0.0011152108,
      0.0334384702,
      -0.1026834324,
      0.0218634624,
      0.0195783433,
      0.0469693355,
      -0.0020902171,
      -0.0142527325,
      -0.0365625806,
      0.0151031921,
      -0.0231279321,
      -0.0642412305,
      -0.0766064301,
      -0.0420038775,
      -0.0378492475,
      -0.0829584748,
      -0.0042166035,
      -0.0379327796,
      0.0362309925,
      0.0235204939,
      -0.0806744397,
      0.0220745038,
      0.027841419,
      0.0928834304,
      -0.0060623176,
      0.0446426943,
      -0.0006920352,
      0.0247906055,
      0.0894884542,
      0.0114232507,
      0.0225734413,
      -0.0446835794,
      0.0712943375,
      -0.0204779245,
      0.1220319793,
      -0.0050311815,
      0.0393744484,
      -0.0716206431,
      0.000256677,
      0.0436756052,
      -0.0447263531,
      0.0088213766,
      -0.0307070613,
      -0.0738065541,
      0.1095125899,
      -0.0418576114,
      0.0331688933,
      0.0552034639,
      -0.0069863992,
      -0.0154274385,
      -0.0331256203,
      0.0569675975,
      0.0316714048,
      0.0297343563,
      0.053709209,
      -0.0834098756,
      -0.0199846961,
      -0.0208120514,
      0.0524211004,
      0.0398655236,
      -0.0111401109,
      -0.0697889924,
      0.0034754891,
      0.0572075322,
      0.0068218238,
      -0.0558419563,
      0.0681544542,
      -0.0279507246,
      -0.0691918656,
      -0.0341160446,
      0.0295946728,
      0.0194606949,
      -0.0298513845,
      0.0477074832,
      -0.0566134118,
      0.0436382145,
      -0.0040157842,
      -0.0448504686,
      0.0162353273,
      8.833624049e-33,
      0.0163028799,
      0.0935815722,
      0.0111612575,
      -0.0006493727,
      0.0359682925,
      0.0195914693,
      -0.0458527096,
      0.0223640222,
      -0.0567599535,
      -0.0474068634,
      -0.0799563006,
      -0.0284732748,
      -0.0670935139,
      0.0733022466,
      -0.0269139055,
      -0.0191202927,
      0.0096340608,
      0.0561683066,
      -0.0141391037,
      -0.0036885322,
      -0.0295931641,
      -0.0224049278,
      0.0346245393,
      -0.0093651405,
      0.0025438797,
      0.0001379375,
      0.0325893126,
      -0.0990770683,
      -0.0685644299,
      -0.0101148533,
      -0.1138348505,
      0.0504097715,
      -0.0081828861,
      -0.0170594938,
      0.0387577079,
      -0.1252942681,
      0.0101367077,
      -0.0517666712,
      0.0525784083,
      -0.0309225414,
      -0.0416037254,
      0.1244573891,
      -0.0334552415,
      0.0316325985,
      -0.0285580177,
      -0.0019362968,
      0.0121599548,
      0.0055901567,
      0.0286455806,
      -0.0215357542,
      0.0695878267,
      0.0040001865,
      -0.0999837518,
      -0.1069890782,
      0.0803556517,
      0.0780822933,
      0.091239512,
      0.0716208741,
      0.0472411029,
      0.0674073771,
      0.0381327085,
      0.0530107692,
      -0.0060139992,
      0.111953482,
      0.0770012215,
      0.007858485,
      -0.086827673,
      0.0745004043,
      0.044727914,
      -0.0211628228,
      -0.0177112855,
      -0.0070643593,
      -0.0182144642,
      -0.0176314022,
      0.0470342599,
      0.0481452048,
      -0.0046033366,
      -0.1241641939,
      -0.0608570613,
      0.0488692597,
      -0.03079658,
      0.0166028515,
      -0.0410989262,
      -0.0405705571,
      -0.0832754225,
      0.0238584559,
      0.0788793042,
      -0.031115897,
      0.0320198573,
      0.0203601588,
      0.020581536,
      0.017821094,
      -0.0241170544,
      0.0034688141,
      0.0158839598,
      -7.165283855e-33,
      -0.0082371412,
      0.0431868546,
      -0.0416341871,
      0.0574519262,
      -0.0695336461,
      -0.0234076325,
      -0.0267385058,
      0.0304161645,
      -0.0024803607,
      0.0168741979,
      -0.0070861825,
      -0.1265240461,
      0.0688359737,
      -0.0169686656,
      0.0136971734,
      -0.0532812588,
      0.0571108535,
      0.0090261009,
      -0.0604662485,
      0.0479038469,
      0.0358503386,
      0.1054521427,
      -0.0730025098,
      -0.01615097,
      0.0131126447,
      0.0077264416,
      -0.065685451,
      0.0874498487,
      -0.0128194634,
      -0.0629349649,
      -0.0580184534,
      0.0163885374,
      0.0383173078,
      -0.01837999,
      -0.0517314672,
      0.1002151668,
      0.0454202369,
      -0.0349217728,
      -0.0509838201,
      0.0565031804,
      0.0896248445,
      -0.01448947,
      -0.0263331514,
      0.0174248926,
      -0.077301383,
      -0.0138454121,
      -0.1410406977,
      0.0028771718,
      -0.0293467268,
      0.0476162285,
      -0.0231285319,
      -0.0096728066,
      -0.0969157889,
      0.0026756306,
      0.006655483,
      -0.0400645956,
      -0.0521548688,
      -0.037205562,
      0.0259376746,
      -0.0613735281,
      -0.0744532123,
      -0.0044754227,
      0.0025808017,
      -0.0493951179,
      0.0272878315,
      -0.0316868164,
      0.0412803814,
      -0.0101627838,
      0.073745884,
      -0.0012433052,
      0.0560874157,
      -0.0621627904,
      0.0995313153,
      0.0476939641,
      0.0346493013,
      -0.0321078487,
      -0.0082046688,
      -0.0299717877,
      -0.0197831187,
      -0.0714099556,
      -0.0560670756,
      -0.0186042078,
      -0.0076258965,
      0.0006797698,
      0.0466452949,
      0.0531132817,
      0.0969075337,
      0.0239695366,
      0.0495075509,
      0.0414266326,
      0.0104440069,
      0.0078512346,
      0.0064196563,
      0.0622394271,
      -0.0279348493,
      -0.0000000585,
      -0.0911355466,
      -0.0281916279,
      -0.0472318716,
      0.0088644763,
      0.009931989,
      -0.1292621791,
      -0.0004866759,
      0.0436861999,
      -0.0251368154,
      0.0084342221,
      0.0024866732,
      0.0093522873,
      -0.0631116778,
      -0.0141438898,
      -0.0120220995,
      0.078525193,
      0.1015029922,
      -0.0261697732,
      0.0334196724,
      -0.005345712,
      0.0498026684,
      0.065738894,
      0.0052463338,
      0.0348694697,
      0.0330605954,
      -0.0437874906,
      -0.0999922529,
      0.0146916704,
      0.0332994126,
      -0.0620851964,
      0.0237451065,
      0.0164087899,
      -0.0211455487,
      -0.0436177216,
      -0.0012069338,
      0.0430595726,
      0.0500537083,
      0.0309065357,
      -0.0131356884,
      -0.0078568198,
      0.0845454037,
      0.0284279883,
      -0.05622527,
      0.0558121875,
      -0.0128190583,
      -0.0501881428,
      -0.0194592793,
      -0.07070636,
      0.0514218584,
      -0.0660809278,
      0.0791582912,
      -0.0896909609,
      0.06390661,
      0.0245114379,
      -0.0231395755,
      0.0272004325,
      0.0682352185,
      -0.027717771,
      0.0329992361,
      0.0697355643,
      0.1035022512,
      0.0346625671,
      -0.0775191635,
      -0.0453133583
    ],
    "cluster":0,
    "time":"00:01:40"
  },
  {
    "id":8,
    "url":"https:\/\/arxiv.org\/pdf\/2509.14360",
    "content":"Embodied sensorimotor\ncontrol: computational\nmodeling of the neural\ncontrol of movement\nMuhammad Noman Almani,1,2,4\u2217 John\nLazzari,1,3,5\u2217 Jeff Walker,1,3,6\u2217 and Shreya\n1Center for Neurocomputation and Machine Intelligence, Wu Tsai Institute, Yale\nUniversity, New Haven, USA, 06511\n2Department of Electrical and Computer Engineering, Yale University, New\nHaven, USA, 06511\n3Department of Biomedical Engineering, Yale University, New Haven, USA,\n4email address: muhammadnoman.almani@yale.edu\n5email address: john.lazzari@yale.edu\n6email address: jeffrey.walker@yale.edu\n7email address: shreya.saxena@yale.edu\n\u2217These authors contributed equally.\nXxxx. Xxx. Xxx. Xxx. YYYY. AA:1\u201330\nhttps:\/\/doi.org\/10.1146\/((please add\nCopyright \u00a9 YYYY by the author(s).\nAll rights reserved\nsensorimotor loops, neural dynamics, optimal control, deep\nreinforcement learning, musculoskeletal models\nWe review how sensorimotor control is dictated by interacting neural\npopulations, optimal feedback mechanisms, and the biomechanics of\nbodies. First, we outline the distributed anatomical loops that shut-\ntle sensorimotor signals between cortex, subcortical regions, and spinal\ncord. We then summarize evidence that neural population activity\noccupies low-dimensional, dynamically evolving manifolds during plan-\nning and execution of movements. Next, we summarize literature ex-\nplaining motor behavior through the lens of optimal control theory,\nwhich clarifies the role of internal models and feedback during motor\ncontrol. Finally, recent studies on embodied sensorimotor control ad-\ndress gaps within each framework by aiming to elucidate neural popula-\ntion activity through the explicit control of musculoskeletal dynamics.\nWe close by discussing open problems and opportunities: multi-tasking\nand cognitively rich behavior, multi-regional circuit models, and the\nlevel of anatomical detail needed in body and network models. To-\ngether, this review and recent advances point towards reaching an in-\ntegrative account of the neural control of movement.\n1. INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2. RELEVANT ANATOMY OF THE SENSORIMOTOR CONTROL LOOP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.1. Sensory Regions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.2. Motor Regions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2.3. Musculoskeletal System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3. NEURAL POPULATION DYNAMICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.1. Computation Through Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.2. Manifolds during Motor Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.3. Dynamics during Motor Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.4. Role of Multiple Regions in Motor Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.5. Emulating Motor Control using Dynamical Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4. OPTIMAL CONTROL THEORY FOR UNDERSTANDING THE CONTROL OF MOVEMENTS . . . . . . . .\n4.1. Internal Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2. Motor Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3. State Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.4. Motor Planning and Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.5. Algorithms for Optimal Control. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5. SIMULATING EMBODIED CONTROL FOR ELUCIDATING NEURAL CONTROL OF MOVEMENT. . .\n5.1. Musculoskeletal Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.2. Deep Reinforcement Learning to simulate Embodied Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.3. Towards embodied control of complex behavior. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.4. Neural representations and dynamics in embodied systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5.5. Open questions and opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6. CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1. INTRODUCTION\nHow do distributed neural circuits drive purposeful movements from the complex muscu-\nloskeletal system? This characterization is critical towards not just furthering our under-\nstanding of the generation of movement, but, importantly, guiding us towards therapeutic\ntargets for diseases affecting motor control. The neural processes leading to movements have\nbeen relatively well posited and understood due to the quantitative nature of the behav-\nioral outputs involved. Classic approaches have largely focused on optimization principles,\nincluding limb control, to achieve human-like behavioral trajectories. These largely theo-\nretical models of sensorimotor control can recapitulate observed movement trajectories by\nhypothesizing the presence of a controller guiding the complex movements. However, these\nmodels cannot predict how neuronal populations in each brain region affects the resulting\nmovement and vice-versa. On the other hand, breakneck advances in hardware techniques\nhave led to vast improvements in our ability to record large-scale multi-regional neural\ndata. These recordings have enabled dimensionality reduction and modeling techniques\nto elucidate the structure in high-dimensional neural activity during different conditions,\nand relate the neural activity directly to kinematic outcomes. However, these data-driven\nmodels typically do not consider the biophysical underpinnings of the musculoskeletal sys-\ntem, and thus fail to elucidate the computational role of neural activity in driving the\nmusculoskeletal system such that the body reaches a desired state. The emerging field of\n2 Almani et al.\nembodied control incorporating detailed musculoskeletal models and integrating them with\nneural computations aim to address these gaps.\nIn this review, we (1) outline the anatomical substrates of sensorimotor control; (2)\nexamine how neural dynamics are quantified in motor regions; (3) map the theoretical\nframework of optimal control onto sensorimotor processing; and (4) survey recent and on-\ngoing efforts to explicitly incorporate embodiment into models of sensorimotor control. We\nend with directions of future work and open questions in the field.\n2. RELEVANT ANATOMY OF THE SENSORIMOTOR CONTROL LOOP\nThe abstraction of a sensorimotor loop is often used to model the parallel and distributed\nneuromechanical systems that come together to implement sensorimotor control in humans\nand other animals. Although the concept of a single sensorimotor loop does not naturally\nsimplify the complexities of hierarchical and nested biological neural circuits, it does provide\na useful framework for modeling goal-directed movements of bodies in the world. Below we\nwill briefly describe components of primate motor systems to illustrate the neural structures\ninvolved in biological sensorimotor control and the ubiquity of sensorimotor loops connecting\nthem (Figure 1). The central loop comprises ascending pathways of the spinal cord that\ncarry somatosensory signals from the periphery and descending pathways that carry motor\nsignals to control the musculature. These pathways are embedded in hierarchically nested\ncortical, subcortical, and spinal structures that modulate and use those signals for various\nlearning processes to support flexible behavior. Sensorimotor delays resulting from relatively\nslow muscle activation and long-range signal transduction create the need for predictive\nmechanisms to compensate. We find the ingredients needed to build such mechanisms\nacross both cortical and subcortical structures.\n2.1. Sensory Regions\n2.1.1. Ascending projections: sources and targets. Sensory feedback is an essential part\nof naturalistic motor control. Afferent feedback resulting from movement of the body or\nworld propagates through a sequence of circuits originating in peripheral receptors, traveling\nthrough the dorsal root ganglion and ascending the spinal cord and brainstem toward\nsomatotopically structured thalamocortical loops for visual and somatosensory processing.\nThe most immediately relevant areas for discussions of sensorimotor control are the cortical\nareas, 3b, 3a and area 2, spanning anterior and posterior parietal cortex. These areas\nare involved in processing tactile input from receptors in glabrous skin (mainly primary\nsensory area 3b) and proprioceptive muscle spindle and golgi tendon organ inputs from\nwithin muscles and tendons (primarily 3a and area 2) ascending the dorsal column of the\nspinal cord, entering the brain through the cuneate nucleus before passing through thalamic\nnuclei on their way to the cerebral cortex (7, 8). Population analysis of responses in these\ncortical areas has revealed that these neurons encode detailed information about kinematic\nlimb state (e.g. (9, 10)). Although the exact nature of the connection between S1 and\nthe motor regions remains a topic of ongoing study, anatomical work demonstrates strong\nreciprocal connections, both intracortical and transthalamic (11).\n2.1.2. Vision and posterior parietal cortices. Various brain regions involved in visual pro-\ncessing and visuomotor coordinate transformations are critical for visually guided motor\nwww.annualreviews.org \u2022 Embodied sensorimotor control\nSchema of structures and pathways involved in sensorimotor control of upper limb\nbehavior in the macaque monkey. Descending and ascending tracts comprise central conduit\nfor sensorimotor signal flow. Long range projections and slow muscles induce sensorimotor delays.\nNote multiple examples of anatomical features thought to create copies of motor commands that\ncould provide elements needed to learn predictive internal models for sensorimotor control:\ncorticospinal collaterals to subcortical structures (1, 2), V2b interneurons and spinocerebellar\nloops (3). Also note nested loops through distributed circuits: intracortical frontoparietal,\ncorticothalamic, cortico-basal ganglia-thalamo-cortical, spinocerebellar, basal ganglia-cerebellar\ninter thalamic (4). PMd: dorsal premotor cortex, M1: primary motor cortex, S1: primary sensory\ncortex, PPC: posterior parietal cortex, GPi: globus paladus internus, STN: subthalamic nucleus,\nVis.: visual cortical areas. Adapted from (5, 3, 1, 6, 4).\ncontrol involving complex interactions with the external environment. For example, areas\nwithin the posterior parietal cortex (PPC) receive a significant amount of visual informa-\ntion, share extensive connections with premotor areas, and have been shown to be crucial\nfor reaching to visual targets in humans and nonhuman primates (6, 12).\n2.2. Motor Regions\n2.2.1. Descending projections: sources and targets. Parallel and distributed circuits span-\nning cortical and subcortical structures underpin the sophisticated control of movement\n4 Almani et al.\nobserved in both humans and other primates. We can make sense of these circuits in terms\nof the composition of descending projections of the spinal cord (5). Reticulospinal, vestibu-\nlospinal, and tectospinal fibers originate from the brain stem and terminate largely in the\nintermediate zone of the spinal cord. Rubrospinal fibers originate in the red nucleus and\nterminate in the ventromedial intermediate zone as well as in the motor pools of the spinal\ncord. These pathways are strongly conserved, provide an evolutionary foundation for the\ncontrol of voluntary movement, provide redundancy in spinal access that can compensate\nfor injury, and are critical for any comprehensive understanding of vertebrate motor con-\ntrol (13). We know that in mice, these circuits are spatially organized to allow access to\ncomponents of movement, for example, a reach or a grasp (14). However, while the spatial\norganization and function of many brain stem circuits have been mapped in exquisite detail\nin mice (2, 3), their population dynamics in primates has been largely unexplored. In con-\ntrast, there has been an explosion of interest in dynamics of neural populations from which\nthe corticospinal tract originates, specifically primary motor cortex, premotor cortex, but\nalso primary sensory areas just posterior to the central sulcus, as well as cingulate motor\nareas and the supplementary motor area. Neural populations in these cortical areas will be\nthe focus of much of the discussion about population dynamics in Section 3. Corticospinal\nprojections from these areas terminate in both the intermediate zone and the motor pools\nof the spinal cord. These populations are potentially 1-2 synapse from neuromuscular junc-\ntions and are known to be critical for dexterous and visually guided voluntary movement\n2.2.2. Cerebellum, Basal ganglia and Thalamus. Although they do not project directly\nonto the spinal cord, the basal ganglia and cerebellum are critical to sensorimotor function.\nThe cortical-basal ganglia (BG)-thalamocortical (CBGTC) loop contains multiple direct\nand indirect feedback loops implicated in a wide array of motor functions (15). The frontal\ncortex projects densely to the striatum, and striatal efferents converge at the BG output\nnuclei such as the Substantia Nigra reticulata or Globus Pallidus internus. Output nuclei\ninhibition (excitation) then disinhibits (inhibits) the thalamus which subsequently excites\n(inhibits) the cortex.\nIn addition to classical corticostriatal projections known to be in-\nvolved in task selection, motor control, and learning, recent work using the rabies virus to\ntrace multisynaptic pathways from the motor cortex, cerebellum, and BG has revealed a\nreciprocal basal ganglia cerebellar loop of projections primarily through the ventrolateral\nthalamic nucleus and the pontine nucleus of the pons (4). These subcortical structures\ncontain many anatomical features that suggest elements of feedback control of the sort\ndiscussed in Section 4. Corticospinal projections are known to provide collaterals to sub-\ncortical structures on their way to their spinal targets, such as the pontine nucleus of the\npons; these are thought to carry copies of motor commands (2). Similarly, V2b interneurons\nin ventromedial intermediate zone of the spinal cord bifurcate on their way to the motor\npools and ascend the spinal cord to provide a copy of this motor command to the lateral\nreticular nucleus (3). In both of these cases, the structures that receive the motor command\ncopy subsequently project to different components of the cerebellum and deep cerebellar\nwww.annualreviews.org \u2022 Embodied sensorimotor control\n2.3. Musculoskeletal System\nAll signals bound to activate muscles must go through the motor pools of the spinal cord,\nthe final common path. These motor pools are somatotopically organized and engage al-\npha motor neurons to activate motor units and muscle fibers (16). The muscle fibers are\norganized into three-dimensional joint-spanning muscle bodies in series with tendons such\nthat force production for a given set of muscle activations will be a function of the bony\norigins and attachments of the muscles, the arrangement and composition of muscle fibers\nin the muscle bodies, and the current state of the joint (17). Muscle spindles are embedded\nin parallel with muscle fibers to sense passive and active joint stretch. The sensitivity of\nthese muscle spindles is itself regulated by a system of gamma motor neurons (7). Golgi\ntendon organs are embedded in tendinous musculoskeletal attachments and work in concert\nwith inhibitory interneurons in the spinal cord to sense and protect against forceful muscle\ncontraction. While the description of muscle physiology is necessarily brief given the scope\nof this review, for a more comprehensive review of muscle physiology especially modeling\nat the neuromuscular interface see (18).\nCircuits and structures underlying sensorimotor control in primates and other verte-\nbrates comprise a parallel and distributed neuromechanical system where each component\ncontains sensorimotor loops and anatomical features consistent with feedback control pro-\ncesses being distributed throughout. This presentation of relevant anatomy drew primarily\nfrom work with primates, but also from work with rodents for cellular and circuit level char-\nacterizations. We now turn our attention to an understanding of the population dynamics\nthat has largely emerged from studies of the activity of motor cortical populations.\n3. NEURAL POPULATION DYNAMICS\nHere we discuss the hypothesis that movement is generated by an underlying high-\ndimensional, non-linear dynamical system implemented by populations of neurons. This\ntheory of neural computation now permeates systems neuroscience, guiding theories of mo-\ntor and associated cognitive processes such as memory, decision-making, and preparation.\n3.1. Computation Through Dynamics\nThe computations performed by the motor cortex used to produce volitional movements\nhave traditionally been studied using a bottom-up approach. In this framework, the tuning\nof single-unit responses to external variables such as muscle activity or movement kinematics\nhave provided a basis for interpreting descending commands from the motor cortex to\nmuscles. More formally, we can define a resulting population response r(t) as a non-linear\nfunction f of external parameters m1, m2, ..., mn as such\nr(t) = f (m1(t), m2(t), ..., mn(t))\nIndividual unit responses have been shown to encompass tuning to both types of movement\nparameters as well as time in a heterogeneous manner (24, 25, 26), making it increasingly\ndifficult to reliably define the computations performed by the population - or even individual\nunits - before or during movement. Recent progress in understanding the neural control of\nmovement has instead been driven by a top-down approach: interpreting population-level\nmechanisms to further understand single unit responses, and ultimately, the overarching\ncomputation. This has led to a novel framework that interprets neural activity through\n6 Almani et al.\nComputation through neural population dynamics. (A) The firing rate of each neuron in\nthe population defines a (high dimensional) neural state space. Adapted from (19). (B) As neural\nfiring changes over time, an N dimensional curve is traced through the state space. Adapted from\n(19). (C) The neural trajectory generated during behavior may lie on a lower dimensional\nmanifold within the N dimensional space. In this case, the manifold is linear and two-dimensional,\ndefined by the modes u1 and u2. Adapted from (20). (D) Neural trajectories during complex\nbehavior may shift between subspaces. The latent activity along the first preparatory mode\n(dotted line) is nearly orthogonal to the latent activity defining the movement space (filled line).\nAdapted from (21). (E) Optogenetic perturbations of mouse ALM activity during motor\npreparation either recovers, or switches its position along the choice axis after stimulation. (left)\nSchematic of externally driven discrete attractor guided by a ramping signal. (middle) Recovery of\nperturbed activity along a choice axis during correct lick trials. (right) Perturbation trials\nresulting in incorrect choices show activity flipping along the choice axis. Adapted from (22). (F)\nDuring cyclic movements of the primate limb, activity in M1 displays elliptical dynamics stacked\naccording to the speed of movement. Adapted from (23).\nthe lens of a dynamical system; here, the activity or firing rates of individual neurons is\nhypothesized to form the states of a high-dimensional dynamical system (Figure 2 (A,\nB)). We denote this as the dynamical systems perspective (19). Mathematically, we define\na population trajectory h(t) and its derivative \u02d9h, scaled by a time constant \u03c4 , with inputs\ns(t) and non-linear activation f using the following form.\n\u03c4 \u02d9h(t) = f (h(t)) + s(t)\nEquation 2 describes the time varying activity of a population of recurrently connected\nnon-linear units performing a computation leading to a kinematic trajectory through space.\nThis framework clears uncertainties surrounding response complexity in individual units,\ngiven that some units may represent external parameters while others are responsible for\nshaping internal computations. However, as opposed to the model proposed in Equation 1,\nthe nature of computations performed by a complex, high-dimensional dynamical system\nand their relation to movement generation are less obvious, thus requiring techniques to\nsimplify and interpret the underlying signals.\nwww.annualreviews.org \u2022 Embodied sensorimotor control\n3.2. Manifolds during Motor Control\nHow can we make sense of the heterogeneous, time-varying signals of a non-linear dynamical\nsystem and relate them to movement? It is possible that out of the many dimensions that\nmake up the recorded neural space, only a small fraction are needed to sufficiently describe\nthe system (20). Given that the activity of neurons is likely constrained by the underlying\ncircuitry connecting them, the effective dimensionality of the system is likely lower than that\nof the entire population. This gives rise to the notion that population activity may reside\non a low dimensional surface embedded in a high dimensional population space, known as\nthe manifold hypothesis (Figure 2 (C)).\nIn order to identify these manifolds, or subspaces, linear dimensionality reduction meth-\nods such as Principal Component Analysis (PCA), Factor Analysis, and Gaussian Process\nFactor Analysis have been key (27). These methods generally find the neural modes, or\ndirections in neural state space, that define the latent variables of the population activ-\nity. The latent variables are the time dependent activity of the neural modes that can\nmost sufficiently describe the signal. If only a few latent variables are needed to capture\nthe most important features of the data (e.g., the variance), then we can view the neural\nmodes as defining a hyperplane (or linear manifold) on which the activity resides. Indeed,\nsuch subspaces have been identified in motor and pre-motor cortices of primates perform-\ning center-out reaching tasks, with a three-dimensional manifold capturing target specific\nclusters of latent activity (28). The existence of such manifolds have been shown to impact\nthe learning speed of primates controlling a cursor using a brain-computer interface (BCI),\nwith task perturbations inside an intrinsic manifold being learned on a faster time-scale as\ncompared to otherwise (29). The use of shared manifolds across distinct tasks with similar\nelements has also been discovered in primate M1 (30). Low-dimensional manifolds are not\nlimited to motor cortices during limb movements, and have additionally been discovered\nin a wide range of brain regions such as the pre-frontal cortex (31, 32), V1 (33), olfactory\ncortex (32), and parietal cortex (34) in various species such as monkeys and rats.\nIn traditional task structures involving a preparatory (or delay) epoch followed by move-\nment, such as center-out reaching in primates and instructed directional licking in mice,\nactivity predicting the upcoming movement has been shown to persist during the delay in\nthe absence of external stimuli (35, 36, 37). Activity during preparation and movement are\nshown to be nearly orthogonal (21) (Figure 2 (D)), thus occupying separate subspaces.\nThis suggests the brain utilizes orthogonal manifolds to isolate distinct computations. Ad-\nditionally, the preparatory subspace was found to be output-null (38), now known as the\nnull-space hypothesis (39), explaining how such activity does not directly produce move-\nment. This demonstrates the use of low-dimensional activity as a strategy employed by the\nbrain to compartmentalize computations.\n3.3. Dynamics during Motor Control\nThe time evolution (or dynamics) of neural activity, like the low-dimensional spaces they\ncomprise, are constrained by network connectivity and shape neural computation, as re-\ncently demonstrated in BCI studies (40). Such dynamics have been studied during prepara-\ntion, movement execution, and the transition between epochs in order to fully characterize\nthe computations performed during delay-instructed tasks. To better observe preparatory\ndynamics, optogenetic stimulation of the mouse anterior lateral motor cortex (ALM) was\nperformed during delay-instructed directional licking (22) (Figure 2 (E)). Preparatory ac-\n8 Almani et al.\ntivity in this setting resembles a population ramp to threshold (41). However, ramping\nmay invoke various dynamical solutions, such as the state shifting along a continuous at-\ntractor, decaying to a discrete attractor, or moving in accordance with an externally driven\ndiscrete attractor. The results of the perturbations in (22) demonstrate that activity either\nrapidly recovers to its choice along a decision axis, or switches sides. The discrete nature of\nthis shift, along with the rapid recovery of the state to pre-stimulation levels, suggests an\nexternally-driven discrete attractor guiding the population state during preparation. Neural\ndynamics have also been studied in adjacent cognitive motor settings such as motor timing,\nwhich may utilize sequential activity or more complex population codes (42, 43).\nThe next step in the preparation-to-execution pipeline involves the transfer of activity\nfrom one subspace to another. Large multi-phasic shifts in activity occur as preparation\ntransfers to execution. In (44), it was found that primates performing center out reaches\ndisplayed transient oscillatory dynamics after movement preparation. Large non-selective\nchanges in activity have been shown to occur in response to the go-cue in primate M1\/PMd,\nsignaling movement onset itself (45). Such transient responses to the go-cue have also been\nshown in the mouse ALM (46), with the neural mode defining the go-cue response being\nthe most prominent during memory-guided movement tasks. Such activity is thought to\nrepresent the shift of population activity from the null to output-potent movement subspace.\nDuring movement, it is believed that the motor cortex is tasked with generating coherent\npatterns of activity necessary to drive muscles. For example, primates performing cycling\nmovements display elliptical neural trajectories in M1, likely generated by limit cycles (23)\n(Figure 2 (F)). Such trajectories are stacked according to the speed at which the cycle\nis performed, with low trajectory tangling in comparison to muscle activity (47). The\ndynamical features of the subspaces that define movement preparation and execution have\nled to great strides in understanding how the brain prepares and executes movement.\n3.4. Role of Multiple Regions in Motor Control\nThe motor cortex does not work in isolation to produce movement, but rather works in\nconcert with other areas of the brain and body, including the basal ganglia, thalamus, cere-\nbellum, and spinal cord. This results in a multi-regional circuit with distinct computational\nroles for each region likely depending on their underlying connectivity, cell-types, and func-\ntional specialization. Within the CBGTC loop, the role of the different pathways defined by\nstriatal cell types has been extensively studied in relation to action selection (15). There are\nalso direct reciprocal connections between the thalamus and cortex, studied in settings such\nas planning (48, 49) and sequencing (50), and between the thalamus and striatum, which\nmay implement gating mechanisms (51). The STN also receives direct excitation from the\ncortex through the hyperdirect pathway, largely studied in relation to stopping signals (52).\nThe cerebellum has been shown to play a role in shaping the attractor landscape of the\nmouse ALM during preparation (53). While the cerebellum has primarily been studied in\nrelation to internal models, evidence has shown that population level mechanisms such as\na null-spaces are implemented as well (54).\n3.5. Emulating Motor Control using Dynamical Systems\nNeural network models of non-linear dynamical systems, known as recurrent neural net-\nworks (RNNs), paired with gradient descent optimization of specified loss functions, have\nrecently proved to be invaluable tools for testing hypothesis in both motor and cognitive\nwww.annualreviews.org \u2022 Embodied sensorimotor control\nsettings. RNNs are a special class of artificial neural networks where each unit is recurrently\nconnected with each other unit in the network. The units in RNNs directly model neuronal\nfunction: they integrate information from many inputs through weighted connections, and\ntheir outputs are governed by nonlinearities. This deep learning based modeling framework\nfinds a set of weights, and consequentially a dynamical solution, to a specified objective\nfunction. When this objective is modeled after a laboratory experiment performed by a live\nanimal, such as center-out reaching, the model provides a particular optimal solution to the\ntask that can be compared with recorded neural data. This form of modeling is known as\ntask-driven or goal-driven modeling (55, 23). This is in contrast to data-driven modeling,\nwhere the RNN activity is directly constrained to the recorded neural data (56).\nFormally, a commonly used form of RNNs is given by the equations below\n\u03c4 \u02d9x = f (h, s) = \u2212x + Whh + Wss + bh +\nHere, \u03c4 represents the network time constant, Wh, bh, and Ws represent the hidden weights,\nhidden bias, and input weights respectively, \u03f5 \u223c N (0, 1), \u03b4 is a noise scalar, \u03c3 is the non-\nlinear network activation, and h and x represent the non-linear and linear RNN states\nrespectively. RNNs in neuroscience typically include a linear readout a, with Wo and\nbo representing the output weights and bias respectively. Once trained in a goal-driven\nsetting, researchers typically \u201creverse-engineer\u201d the RNN to derive insight regarding how\nthe task is solved (57). Given the non-linear nature of the model, reverse-engineering RNNs\ntypically involves linearizing about certain states in order to analyze the local dynamics.\nTypically, RNNs are linearized about fixed points, or the states h\u2217 paired with input s\u2217\nsuch that f (h\u2217, s\u2217) = 0, where f denotes equation 3. The dynamics around fixed points\nare approximately linear and can be examined by performing a Taylor expansion of the\nnetwork about the desired point. To do so, it is first necessary to identify fixed points by\n2, ...} that minimize f (h\u2217, s\u2217), where s\u2217 is a fixed\noptimizing for a set of hidden states {h\u2217\ninput (57). Once fixed points are captured, the network can be linearized about the fixed\npoint h\u2217 as such\nf (h\u2217 + \u03b4h\u2217, s\u2217 + \u03b4s\u2217) \u2248 f (h\u2217, s\u2217) +\nBy definition, f (h\u2217, s\u2217) = 0, and second order terms are approximately zero given that\n||\u03b4h||2 \u2248 0. Additionally, assuming the input s\u2217 is held constant, we can ignore any changes\nfrom \u03b4s. Thus, our desired system simplifies to\nf (h\u2217 + \u03b4h\u2217, s\u2217 + \u03b4s\u2217) \u2248\nThe eigenvalues, phase portraits, and other tools common to analysis of linear systems can\nthen be applied to the resulting Jacobian to interpret network computation.\nThe above methods have been predominantly used for cognitive tasks given their depen-\ndence on fixed point computations. The first use of reverse engineering in RNNs to guide\nexperimental analysis was shown in (31), where RNNs trained to mimic the prefrontal\ncortex performing a context based color-motion discrimination task utilized a continuous\nattractor to integrate evidence while changing the direction of its velocity field based on\ncontext. In (58), reverse engineering of network dynamics was utilized to determine optimal\n10 Almani et al.\ninput directions for performing a working memory task. Network dynamics have addition-\nally been explored in multitasking frameworks, where shifts in dynamics across tasks were\nobserved (59, 60). Dynamical solutions found by networks have been shown to be consistent\nacross architectures as well (61).\nFor motor control, the focus has been primarily on modeling the motor cortex generating\npatterns of activity necessary to drive muscles. RNNs that reproduce EMG data have\nbeen shown to closely match recorded M1 activity when the network is incentivized to\nfind simple solutions using specialized regularizations (62). Networks trained to produce\nmuscle activity for monkeys performing cycling movements at different speeds have been\nshown to incorporate elliptical trajectories stacked along a particular network mode, in line\nwith experimental findings (23). In (63), it was demonstrated that an RNN can utilize an\nerror-based feedback signal to adapt to perturbations such as visuomotor rotation. RNNs\nwith biologically inspired constraints such as Dale\u2019s law and excitatory-inhibitory balance\nhave been used to test hypotheses regarding the underlying dynamics controlling primate\nreaches (64). These tools continue to guide our theories of cortical control of movement in\nan experimentally verifiable manner.\n4. OPTIMAL CONTROL THEORY FOR UNDERSTANDING THE CONTROL\nWhile dynamical models of recorded neural activity have been extensively explored (see\nSection 3), there has been relatively little effort to understand the computational goal of\nthese dynamics, such as the optimal control of limb dynamics. In fact, sensorimotor control\nhas vastly benefited from being cast in an optimal control framework (65, 66, 67, 68, 69).\nThe theoretical framework of optimal control formalizes the concept of the brain\u2019s reliance\non sensory feedback while achieving a desired goal (Figure 3). Here, usual formulations\nposit that behavioral dynamics operate according to linear gaussian models. While the\ncontrol of behavior is thought to be implemented by different regions of the brain, this\nassumption is not explicitly reflected in the formulation. The optimal control solution\ncomprises of state estimation and a control policy, which can be determined using a user-\ndefined cost function.\n4.1. Internal Models\nThe transformation from motor commands to sensory feedback is governed by the dynamics\nof the musculoskeletal system and the physical world. Although such processes are executed\nin the external physical world, the brain is hypothesized to construct an internal model to\nrepresent this transformation (72, 73). The internal processes of the brain that model\nthis aspect of the transformation are known as internal models, or forward models. These\nmodels are thought to predict the next state of the environment given the current state and\na copy of the motor command. Evidence suggests that such internal models are primarily\nimplemented in the cerebellum (74). An integrative theoretic account (75, 76) suggests\nthat lack of motor coordination and stability can result from absence of internal predictive\nfeedback and that cerebellum contains internal models that are crucial to overcome such\nbehavioral deficits (77, 78, 79, 80).\nIn the probabilistic framework, the forward models pf encode the probability distribu-\ntion over the possible future states st+1 given the current state st and the motor command\nwww.annualreviews.org \u2022 Embodied sensorimotor control\nThe framework of optimal control applied to sensorimotor control. The arm and\nenvironment dynamics are approximated to be linear with state s, and a forward model and state\nestimator are hypothesized to compute an estimated s. The feedback controller receives the\nestimated state and a goal in order to compute a feedback control policy \u03c0 given a cost function\nJ. The hypothesized brain regions where these specific computations take place are included in\ndifferent colors (BG: basal ganglia; PFC: prefrontal cortex; PPC: posterior parietal cortex; PMd:\ndorsal premotor cortex; PMv: ventral premotor cortex; SMA: supplemental motor area; A5: area\n5; M1: primary motor cortex; S1: primary somatosensory cortex). Adapted from (70, 71).\nA prediction of the future state trajectory, s = {s1, ...., sT }, given the action trajectory,\na = {a0, ..., aT \u22121}, can be obtained by the repetitive application of the forward models:\npf (st+1|st, at)\npf (sk|sk\u22121, ak\u22121)\nThis prediction of future state trajectory is particularly useful in motor planning. It is\nhypothesized that the brain also encodes priors over the sensory signals p(y) and the motor\nsignals p(a), reflecting its belief about these variables before any actual sensory feedback\nis received. Such internal models are also known as prior models and several studies point\ntowards their existence (81). Prior models in combination with the forward models can be\nused to formulate inverse models.\nThe internal processes that compute the optimal motor commands given a desired en-\nvironmental state are known as inverse models. As the output of the inverse models are\nmuscle excitations that produce the desired consequences in the external environment by\ncontrolling the musculoskeletal model, we use the terms \u2018inverse models\u2019 and \u2018controller\u2019\ninterchangeably. Evidence suggests that the implementation of the inverse models may be\ndistributed among several brain regions, such as in cerebellum (74) and in motor cortex\nConsider, for example, the problem of computing an optimal action trajectory a\u2217 that\ngenerates a movement towards a goal state g. Combining forward models with Bayesian\n12 Almani et al.\ninference can be used to determine the joint probability distribution of the state and action\ntrajectory given the observation of goal state g:\np(s, a|g) = pf (s|s0, a)p(g|s)p(a)\nAn inverse model is then a mapping from the desired goal state to the action that can\nbe obtained from 6. by integrating out s:\npf (s|s0, a)p(g|s)p(a)ds\nThe optimal action a\u2217 maximizes the distribution specified by the inverse model:\nMuch of the complexity of associated neural processes and ensuing behavior arises from\nthe interactions between inverse and forward models. Next, we will show how internal\nmodels play a crucial role in all aspects of sensorimotor integration and control: a complex\nprocess through which the brain uses sensory feedback from the external environment and\ninternally generated task-relevant signals for motor learning, planning, and control.\n4.2. Motor Learning\nProperties of the sensorimotor system change at different timescales, for example, on a\nshort timescale, involving interactive processes with the external environment, and on a\nlonger timescale, due to evolutionary processes such as growth. Internal models must adapt\ncontinuously to account for these changes. The learning of forward models is relatively\nstraightforward using the error between the predicted and the actual sensory feedback.\nThe neural mechanisms underlying such predictive learning have been studied in several\nmodel systems, such as the cerebellum-like structure of the electric fish (83).\nAcquiring inverse models is generally more involved, primarily due to the sensory-to-\nmotor coordinate transformation required in the computation of appropriate gradients.\nWhen a movement is made, the sensorimotor system can sense the directional error be-\ntween the resulting and the desired sensory outcome. However, the sensorimotor system\nneeds to convert this sensory prediction error from sensory coordinates into appropriate\ngradients required to update each element of the motor command. Evidence suggests that\nthe sensorimotor system is highly efficient in learning the gradient of the sensory prediction\nerror with respect to the changes in motor commands even when the mapping from sensory\nto motor coordinates is perturbed (84, 85, 86). There is extensive evidence that error-based\nlearning characterized by fast adaptation depends on the cerebellum (87, 88). In addition to\nerror-based learning, reward-based reinforcement learning (RL) is particularly useful when\na sequence of actions is needed to solve a motor task and the outcome is far removed from\na particular action (89, 90).\n4.3. State Estimation\nTo construct inverse models, the sensorimotor system needs information about the current\nstate of the environment. However, it faces three main challenges. First, biological senso-\nrimotor loops are slow and involve significant sensory delays. Second, noise contaminates\nwww.annualreviews.org \u2022 Embodied sensorimotor control\nvarious stages of the sensorimotor loop, i.e., motor outputs and sensory inputs from the\nenvironment. Third, the sensory inputs from the environment may provide only partial in-\nformation about its state. All these factors make online control impractical while carrying\nout most complex and fast movements. To overcome these challenges, the sensorimotor\ncontrol system is hypothesized to use a combination of forward models and actual sensory\nfeedback from the environment to estimate its state in an observer framework. In the case\nof linear systems, the Kalman filter is an optimal observer as it estimates the state with\nthe least squared error (91). The Kalman filter model is a combination of two processes. In\nthe first process, this model uses the efference copy of the motor command and the current\nstate estimate to generate the next state estimate using the internal forward model.\nthe second process, the difference between actual and expected sensory feedback is used\nto refine the next state estimate. The relative weighting between these two processes is\nmodulated optimally by the Kalman gain, Lt.\n\u02c6st+1 = \u02c6A\u02c6st + \u02c6Bat + Lt(yt \u2212 \u02c6C \u02c6st)\nwhere \u02c6A, \u02c6B, and \u02c6C consist of the forward model: they are the brain\u2019s estimates of\nthe arm and environment\u2019s dynamics (Figure 3). When the environmental dynamics are\nnon-linear or the sensory noise is non-Gaussian, linear approximation approaches such as\nExtended Kalman Filters, unscented filters, or particle filter can also be used (91, 92).\nThe observer framework serves a variety of roles in biological motor control, such as\nsensory reafference cancellation, forward state estimation or mental simulation of intended\nmovements, prediction for learning and planning novel behaviors, to name a few. Several\nempirical studies have investigated the existence of such estimates (93, 94, 95).\n4.4. Motor Planning and Control\nMotor tasks are usually specified at a high-level, such as reaching for a cup of coffee.\nHowever, the sensorimotor system must work at a detailed level, specifying the activations\nfor each of the relevant muscle, that are in turn converted into the excitations, joint torques\nand finally to the path of the hand in space. A given motor task can be achieved in infinitely\ndifferent ways. Consider, for example, all the possible hand paths with which to reach for\nthe cup of coffee. Given all the redundant ways to achieve a motor task, it is surprising\nthat the sensorimotor system generates remarkably stereotypical behaviors: both within\nthe repetition of the same task and between individuals on the same task. Optimal control\nprovides an elegant framework to deal with such selection problems. Cost functions provide\na criterion with which to evaluate all the different possible movements, including successful\nmovement execution to the goal state g, as well as enforcing constraints such as minimizing\nmuscle effort. Cost functions are usually specified as functions of the state (movement),\nmotor command (actions), and the goal or task.\nOptimal control models have been proposed based on maximizing the smoothness of the\njoint torques (minimum torque-change) (96) and hand trajectory (minimum jerk) (97) for\narm movements. Optimal control models based on signal-dependent noise have provided a\nunifying cost function for goal-directed eye and arm movements (98). Todorov and Jordan\n(65) deployed stochastic optimal control with energy-minimization constraints to show that\nthe nervous system may correct movements in task-relevant dimensions while allowing for\nhigh variability in task-irrelevant dimensions, known as the minimum intervention princi-\n14 Almani et al.\nple. However, one of the challenges of the field has been to design a unified cost function\nthat can explain a large repertoire of movements in dynamic settings, while being based\non quantities that are plausibly important to the nervous system and can be directly or\nindirectly measured.\n4.5. Algorithms for Optimal Control\nHere, we provide two algorithmic solutions for optimal control that are commonly used in\nthe frameworks described above.\n4.5.1. Dynamic Programming. Cost functions c(st, at) as a function of the state s \u2208 S\nand action a \u2208 A are usually specified at each timestep, t. The goal of the controller\nis to minimize the cumulative cost, J(s(\u00b7), a(\u00b7)) = (cid:80)T\nt=0 c(st, at) incurred over the entire\nmovement trajectory. However, it is not possible to compute the current optimal action a\u2217\nwithout knowing its future consequences. Dynamic Programming (DP) is used to solve such\nsequence-based optimal control problems. DP is based on Bellman\u2019s optimality principle,\nwhich states that any part of the optimal state-action sequence is also optimal. This allows\nfor solving the optimal control law or policy, \u03c0 : S \u2192 A, recursively by starting from the\nfinal state and working backwards to the initial state. For notational clarity, the estimated\nstate \u02c6s evolution dynamics in this section are given by: st+1 = f (st, at).\nThe key to DP is the optimal value function which captures the long term consequences\nof an action, by calculating the minimum cost-to-go for a given state. The optimal value\nfunction is defined as:\nv(s) = mina\u2208A(s){c(s, a) + v(f (s, a))}\nFor a given state s, the value function v represents the minimum cost that will be\nincurred to reach the target state sT starting from s. Although the optimal value function\ncaptures long-term consequences of an action, 10. enables its computation in a greedy\nmanner (using only the local information): we need to consider only the immediate cost\nof every possible action and add to it the optimal value of the resulting\/next state. The\noptimal control law \u03c0 is computed as follows:\n\u03c0(s) = argmina\u2208A(s){c(s, a) + v(f (s, a))}\nEquations 10. and 11. are also known as Bellman equations.\nIf we know the optimal values of all the resulting states possible from a given state s,\nwe can use 11. to compute the optimal control law \u03c0. DP thus provides a useful approach\nto compute \u03c0(s) and v(s). The key is to start from the target or absorbing states for which\nthe optimal values or the final costs are given. Then, using equations 10. and 11., perform a\nbackward pass in which every state is visited after all its successor states have been visited.\nValue iteration and policy iteration are similarly based on iteratively improving the initial\nguesses of the value functions and are guaranteed to converge to optimal solution. RL is\nanother method to solve such discretized optimal control problems and relies on exploration\nfor state visitation, for example, using a stochastic policy.\n4.5.2. Linear Quadratic Gaussian. Now, we turn to continuous time stochastic optimal\ncontrol problems that yield closed form solutions for the optimal feedback control policy.\nConsider the following environmental dynamics:\nwww.annualreviews.org \u2022 Embodied sensorimotor control\nds = (As + Ba)dt + F dw\nwhere, w represents the Brownian motion.\nLet the associated quadratic instantaneous c and final costs h be:\nwhere Q and Qf are symmetric matrices, and R is a symmetric positive-definite matrix.\nThe optimal action a\u2217 is given by the following control policy \u03c0:\na\u2217 = \u2212Ks = \u2212R\u22121BT Vts\ndt Vt = \u02d9Vt given as:\n\u2212 \u02d9Vt = Q + AT Vt + VtA \u2212 VtBR\u22121BT Vt\nWith the boundary conditions V (T ) = Qf . The ODE 16. can thus be solved backward\nin time to compute the function V . In the case of deterministic environmental dynamics\n(F = 0), the optimal policy remains the same - known as the linear quadratic regulator\nOptimal control algorithms are typically limited to generating simple movements and,\nimportantly, lack an explicit neural representation of the feedback control policy, relying\ninstead on optimization methods. Even in nonlinear settings such as in (99), locally optimal\nactions are typically computed using time-varying linear functions of the estimated state.\nConsequently, this framework has generated limited neural predictions with some exceptions\n(71, 100). Moreover, biological evidence suggests that exploration plays a crucial role in\nmotor learning and generalization (101). However, optimal control algorithms compute\noptimal actions analytically and do not rely on exploration. The next section instead turns\nto RL for neural predictions using embodied control.\n5. SIMULATING EMBODIED CONTROL FOR ELUCIDATING NEURAL\nCONTROL OF MOVEMENT\nDynamical models of the brain in the form of RNNs, as well as optimal control formulations,\nhave both enhanced our knowledge of motor control, albeit from parallel perspectives.\nIn reality, both processes are necessary in order to fully characterize the computations\nunderlying the neural control of movement, however the synergy between both frameworks\nis relatively unexplored. Progress has recently been made on this front through the use of\nRNNs in feedback with skeletal or musculoskeletal models performing a variety of behaviors,\ntrained using deep RL (DRL) (Figure 4A). This framework, which we term embodied\ncontrol, has the potential to bridge the gap between the dynamical systems perspective of\nneural computations and the role that feedback plays from a control theoretic perspective.\n16 Almani et al.\n5.1. Musculoskeletal Models\nBodies - specifically, musculoskeletal systems - are critical components of the dynamical\nmachinery that generates complex vertebrate behavior. Any holistic description of motor\ncontrol must therefore include an explicit description of musculoskeletal systems and their\ninteraction with neural circuits. There is a rich history of modeling musculoskeletal control\nin the field of biomechanics, which has produced a variety of biomechanical models and\nphysics simulators for control simulation (e.g., OpenSim, see (102)). There have also been\nefforts toward framework compatibility through tools like myoconverter (103), which allows\none to convert an Opensim model into formats compatible with other engines (e.g., Mujoco\n(104)). Mujoco is a general purpose physics engine that achieves fast simulation of muscle\ndynamics due to its simplified muscle model, and has proved useful in this regard (105, 106).\nRelated ecosystems now include packages for neuromusculoskeletal optimization (e.g., Moco\nin OpenSim (107)) and emerging differentiable or real-time simulators (e.g., Brax (108))\nthat aim to close the loop between control theory and embodied implementation. This\nexpanding ecosystem of tools and approaches provides the context for current efforts to\nboth understand and engineer embodied control of complex behavior.\nIncorporating musculoskeletal models in a comprehensive model for motor control has\ninherent challenges associated with it, since there is significant complexity on multiple\nspatial and temporal scales in musculoskeletal systems. For example, pennation angles\nand muscle architecture shape the 3D structure of force production, in cases such as the\npectoralis major, and this makes modeling muscles with unidirectional contractile units a\nsignificant simplification. In addition, heterogeneity in muscle fiber type is clearly function-\nally significant in biology (109), resulting in muscles with variable activation dynamics and\nresistance to fatigue, often completely ignored in musculoskeletal modeling efforts. Inaccu-\nrate body models can distort estimates of neural control signals and bias conclusions about\nthe principles of coordination.\n5.2. Deep Reinforcement Learning to simulate Embodied Control\nDuring the last decade, researchers have begun to look to DRL to model locomotion and\ndexterous manipulation using musculoskeletal models (117, 106). DRL is a subfield of\nmachine learning that deals with discretized optimal control problems and may address\nthe shortcomings of optimal control theory as mentioned at the end of Section 4. Here,\nthe sensorimotor loop is modeled as a controller, parameterized using a neural network \u03b8\u03c0,\ninteracting with the environment (Figure 4A).\nMany fundamental concepts in RL have their analog in optimal control theory. Instead\nIn RL, the long-term con-\nof cost functions, RL is based on reward functions, r(st, at).\nsequences of an action are usually captured by the action-value function Q (much like its\ncounterpart value function in optimal control):\n[Q\u03c0(st+1, at+1)]]\nwhere, \u03b3 \u2208 [0, 1] is known as the discounting factor, and the rest of the notation follows\nfrom Section 4.5.\nWhile modeling sensorimotor control, an additional neural network parameterized by\n\u03b8Q is typically included for learning the action-value function. Dopaminergic projections to\nthe motor cortex can constitute a possible neural correlate of reward functions (121). The\nwww.annualreviews.org \u2022 Embodied sensorimotor control\nDeep reinforcement learning with simulated bodies for embodied control. A) Actor\ncritic deep reinforcement learning setup for modeling musculoskeletal control. Adapted from\n(110). B) Menagerie of physically simulated models: i) rat (111, 112), ii) mouse (113), iii)\nmacaque arm (114, 110), iv) marmoset (115), v) fly (116), vi) human lower body (117), vii)\nhuman full body (118), viii) ostrich (119). Adapted from (120, 113, 110, 115, 117, 118, 119)\ngoal is to find a feedback control policy that maximizes the cumulative return (analogous\nto minimizing the cumulative cost in optimal control):\n[\u2207\u03b8\u03c0 Q(s, a|\u03b8Q)|s=st,a=\u03c0(st|\u03b8\u03c0 )]\n[\u2207aQ(s, a|\u03b8Q)|s=st,a=\u03c0(st)\u2207\u03b8\u03c0 \u03c0(s|\u03b8\u03c0)|s=st ]\nThis is also known as policy gradient as discussed in Silver et al. (122). \u03c1\u03b2 reflects the\nstate visitation distribution under a different policy \u03b2 and is used to emphasize off-policy\nlearning, such as learning from past experiences in biological motor control.\n5.3. Towards embodied control of complex behavior\nResearchers motivated to engineer general motor intelligence for whole-body humanoid\ncontrol have been developing training strategies and architectures towards such flexibility\nlargely within the general purpose MuJoCo physics engine (104, 123). Although complex\nbehavior can emerge through exploration alone (124), these learned solutions may look\nunnatural. However, in the last few years, a number of studies have applied imitation\nlearning from motion capture data to agents of various embodiments to model diverse\nbehaviors (125, 126). Briefly, imitation learning consists of a family of RL methods where\nan agent learns a policy by copying expert behavior instead of explicitly optimizing a reward\nfunction through trial and error (127). Enabled by recent progress in deep-learning-based\nmarker-less motion capture, which has enabled rich quantification of animal movement for\n18 Almani et al.\nneuroscience applications (128, 129), this strategy has proven useful to neuroscience through\nthe modeling of naturalistic behaviors captured during experiments in a diverse range of\nspecies (e.g. rats (111), mice (113), and flies (116) (Figure 4B)).\nSome of the most impressive examples of this work come from multiple groups indepen-\ndently developing whole-body fruitfly models capable of recapitulating fly behaviors such\nas locomotion, flight, and odor plume tracking (130, 131, 116) (Figure 4B(v)). The con-\ntrollers used across these projects vary, though all incorporate central pattern generators\nmodulated by top-down hierarchical architectures. These hierarchical networks are biologi-\ncally inspired analogs of the fruitfly motor system, where the fly brain operates on the body\nthrough the ventral nerve cord. (132) use this framework to test the effect of sensorimotor\ndelays in locomotor stability in a whole body fly model.\nNot surprisingly, there are a growing number of efforts to build DRL-driven control of\nhuman musculoskeletal models, bolstered by community challenges involving locomotion\nand manual dexterity (117) (Figure 4B(vi)). One such challenge involved rotating Baod-\ning balls within the palm of a human musculoskeletal hand model. The winning solution to\nthis challenge (133) used curriculum learning and DRL to obtain impressive dexterous con-\ntrol, and found that the learned solutions were consistent with the muscle synergies used\nby human subjects. In another example, (134) develop an imitation learning framework\ncapable of controlling a whole body skeletal model with lower limb musculature. The corre-\nsponding muscle activity patterns of the model correlate well with those of humans engaged\nin locomotion. It is worth noting that impressive whole body musculoskeletal control can\nalso be obtained through supervised learning (118) (Figure 4B(vii)), and DRL-driven im-\nitation has been obtained in non-traditional model species (e.g. (119) (Figure 4B(viii))).\n5.4. Neural representations and dynamics in embodied systems\nRecent work has furthered the utility of embodied control models to compare the repre-\n(112) developed a\nsentations of RNNs driving behavior with neural data. Merel et al.\ncomplete rat body model (Figure 4B(i)) and trained it using DRL to perform multiple\ntasks. They then used approaches derived from neuroscience to characterize the learned\nrepresentations within the value and policy networks, such as revealing rotational dynamics\nduring behavior (44). In subsequent work, (111) used whole-body kinematics recorded from\nrats engaged in open field behaviors and trained a virtual rat on the same behaviors using\nimitation learning. Through comparison of neural recordings from rat motor cortical and\ndorsal striatum populations with the policy and value network activity, the authors found\nthat the activity of the inverse dynamics model realized by the policy and value networks\nwas a better fit to experimentally recorded activity than that of alternative representational\nmodels. In addition to a rat, multiple groups have developed whole body mouse models\n(135, 113) (Figure 4B(ii)). In recent work (113) used a whole body mouse skeletal model\nwith upper limb musculature to investigate the coordinate sytems encoding sensorimotor\nprediction errors in recorded populations from M1 and S1 during motor adaptation.\nDespite the relatively early development of a whole body skeletal model of a macaque\nmonkey (136), much of the recent work to develop DRL-driven musculoskeletal modeling\nof macaques has focused on the upper limb (137, 110) (Figure 4B(iii)). In (110, 137),\nAlmani et al. developed a framework called MuSim, which uses an actor-critic framework\n(Figure 4A) to train RNNs in feedback with a macaque upper limb model to reproduce\ncycling behaviors performed by primates during experiments. In addition to reliably re-\nwww.annualreviews.org \u2022 Embodied sensorimotor control\ncapitulating the target kinematics, and demonstrating strong correlations between RNN\nand recorded neural activity on trained behaviors, they show that neural activity in unseen\nconditions could be predicted by the model. Such generalization was likely aided by the\nuse of explicit neural constraints while training the policy, similar to (62) (Box 5.4). The\ninherent reliance on exploration in DRL may also contribute to the model\u2019s generalization\ncapabilities. In subsequent work, the authors present a broader framework for modeling\nboth musculoskeletal dynamics and recorded neural activity by incorporating a semi data-\ndriven approach in addition to network constraints (110). In addition to macaques, common\nmarmosets are growing in prominence as a primate model well-suited for studying complex\nnatural behaviors requiring feedback and prediction (138, 115, 139) (Figure 4B(iv)). Fu-\nture work involves training such embodied models to perform a diversity of complex natural\nIn fruitflies, efforts to align networks with specific details of fruitfly neuroanatomy or\nneural recordings are in early stages. One example is (131), who used a connectome-\nconstrained approach to model the fly visual system(140) performing object detection as\npart of an embodied simulation of courtship behavior. Advances in drosophila connectomics\n(141) offer tremendous potential to build accurate sensorimotor circuit models, combining\nwhole body models with connectome-derived network architectures. Such comparisons be-\ntween experimentally recorded and simulated neural circuits in fruitfly remains a promising\nopportunity for future work.\n5.5. Open questions and opportunities\n5.5.1. Anatomical detail in musculoskeletal models. It is unclear to what extent biologi-\ncal details of the musculoskeletal system are necessary to recapitulate the key features of\nneural dynamics. In fact, much of the recent work building whole-body models of animals\noften used as model systems in neuroscience has focused on joint-based control and left\nincorporation of musculoskeletal dynamics into models for future work (111, 116). While\nthese models have obtained impressive whole-body control, it is possible that incorporating\nmusculoskeletal details will improve the ability of these models to capture properties of\nneural population dynamics.\n5.5.2. Biological fidelity in network architecture. As detailed in Sections 2, motor control\nemerges from macroscale circuits spanning multiple brain regions as well as the spinal cord.\nHierarchical architectures are ubiquitous in vertebrate motor systems (120), and have proven\nto be an especially useful motif for developing flexible control in artificial embodied systems\n(111, 116). Thus, embodied models that implement such circuits, will likely continue to be\ndeveloped. Multi-regional RNNs trained in a goal-driven fashion have previously been used\nin motor (142) and cognitive (143) settings. Such models during embodied control may\nelucidate the distributed mechanisms underlying action selection, movement invigoration,\ntiming, planning, and sequencing. Moving forward, it will be exciting to see work emerging\nat the interface of practical limitations guiding modeling design choices and the potential\nfor increasingly sophisticated cell-type-specific biological fidelity, especially in species with\nknown connectomes.\n5.5.3. Testing hypotheses about biological sensorimotor control. Physically simulated em-\nbodied control models trained with DRL offer a powerful platform to model key features of\n20 Almani et al.\nNEURAL CONSTRAINTS IN EMBODIED CONTROL\nNeural constraints, such as limitations on firing rates due to energetic costs or refractory periods, undoubt-\nedly shape the solution space available to organisms for sensorimotor control. However, such constraints\ncannot be enforced in optimal control models as in Section 4, as they lack an explicit neural implementation\nof policy. In the context of DRL-driven embodied control, biophysically-relevant neural and energetic con-\nstraints can be implemented using regularizations on the policy (and value) networks and reward functions.\nSuch constraints enable DRL-driven models of embodied motor control to explain a broad repertoire of\nbehavioral and neural data. Constrained models can generalize to produce movement trajectories unseen\nduring training, as well as explain the corresponding neural data (110). Specifically, (110) considered the\nfollowing regularizations for an RNN-based policy network:\n1 - Encourage low firing rates\nRF R = (cid:80)C\nwhere C is the number of training conditions with T timesteps per condition. h is the activity or \u2018firing\nrates\u2019 of the RNN units, in a network with N total units.\n2 - L2 penalty on input and output weights\nwhere Ws is the the input weight matrix and Wo is the output weight matrix as in Equation 3, and\n\u2225 \u00b7 \u2225F denotes the Frobenius norm. This is a commonly used loss term for regularizing network weights.\n3 - Encourage simple population dynamics\n0 \u2225 \u2202(Whh(c,t))\nwhere Wh represents recurrent weights and x represents the activity of RNN units before the non-\nlinearity. This loss term was originally used in (62) and has been found to be very helpful for the emergence\nof neural-like solutions.\n4 - Data-Driven Modeling\nHere, a subset of policy network units are constrained to the recorded neural data:\nRD = (cid:80)NREC\n0 (hi(c, t) \u2212 ni(c, t))2dt\nwhere n represents the firing rates of recorded neurons and NREC is the number of recorded neurons.\nbiological sensorimotor control, including sensorimotor delays and predictive mechanisms\n(144, 132). Sensorimotor delays are ubiquitous in biological systems, but difficult to ma-\nnipulate experimentally. They can, however, be incorporated naturally into simulation\nthrough control and physics timestep variation. Observation and action noise can also be\nwww.annualreviews.org \u2022 Embodied sensorimotor control\ndirectly manipulated to study their effects on learning and control (145, 146). Predictive\nmechanisms, evident even near the sensory periphery in muscle spindles and the retina\n(147, 148), are especially critical to mitigating delays during dynamic interactions with the\nenvironment, as in complex natural behaviors such as prey capture (149, 138). Embodied\nDRL agents also allow exploration of prediction as an auxiliary objective, which has been\nshown to induce structured representations, and may ground behavioral observations in\nneuromechanical principles (150). These simulations uniquely offer full access to egocentric\nobservation streams and motor outputs, providing a testbed for modeling the sensorimotor\nloop as a closed system of neural, musculoskeletal, and environmental interactions.\n5.5.4. Achieving flexible behavior. Organisms can learn an impressive range of motor skills.\nAdditionally, animals are able to quickly learn novel tasks that contain already-learned\nmechanisms. Understanding how the brain accomplishes this feat will require animals and\nmodel networks that perform multiple tasks such that the underlying representations can\nIn cognitive settings, such models have been designed with compositional\nrepresentations emerging (59, 60). However, models are lacking in the motor setting, with\nthe exception of (112). Experimentally, a shared manifold has been discovered in primate\nM1 while performing multiple motor tasks (30). Despite such progress, the underlying dy-\nnamical features such as possible compositional representations in biological and artificial\nnetworks performing multiple motor tasks is not well understood. Methods such as curricu-\nlum RL may be employed to further train embodied models on multiple tasks and observe\nnetwork representations.\nSensorimotor control has typically been analyzed from separate viewpoints - the study of the\nanatomy and physiology of distributed loops, neural population dynamics across regions,\nand the optimal control of bodies. Here, we summarize and synthesize these strands to\nbetter situate the emerging field of embodied control for understanding the neural basis of\nsensorimotor control.\nGoing forward, progress will hinge upon models and experiments that span species,\ntasks, timescales, and brain regions. Equally pressing are questions about how much\nanatomical and biophysical detail is needed in body and network models, and how to use\nsimulation-derived hypotheses to design incisive experiments. With computational tools\nand comprehensive datasets now mature, a unified, closed-loop study of embodied sensori-\nmotor control is within reach.\n1. Vertebrate sensorimotor systems are implemented through hierarchical, parallel,\nnested, and distributed loops.\n2. The brain likely performs computations through population level mechanisms, gov-\nerned by non-linear dynamics on low-dimensional manifolds.\n3. Optimal control models provide rational principles for understanding the internal\nprocesses and mechanisms of brain involved in sensorimotor processing.\n4. Recent developments in simulations of embodied control using deep reinforcement\nlearning offer great promise for understanding biological sensorimotor control.\n22 Almani et al.\n1. How important is incorporating anatomical detail in musculoskeletal models in order\nto understand the underlying neural substrate? Are there properties of neural\ndynamics that naturally emerge by recasting them in an optimal control framework\nfor the control of bodies?\n2. How can principles of nervous system design be applied to the design of network\narchitectures in goal-driven frameworks?\n3. In what ways can we probe biological systems with model-generated hypotheses for\nfurthering our understanding of the neural basis of sensorimotor control?\n4. How can embodied control systems be endowed with capacities for flexible control\nfor multiple tasks in complex environments?\nDISCLOSURE STATEMENT\nThe authors are not aware of any affiliations, memberships, funding, or financial holdings\nthat might be perceived as affecting the objectivity of this review.\nACKNOWLEDGMENTS\nWe gratefully acknowledge funding from the National Institute of Health RF1DA056377.\nLITERATURE CITED\n1. Sherman SM. 2016. Thalamus plays a central role in ongoing cortical functioning. Nature\nNeuroscience 19(4):533\u2013541Publisher: Springer Science and Business Media LLC\n2. Arber S, Costa RM. 2018. Connecting neuronal\nfor movement. Science\n360(6396):1403\u20131404Publisher: American Association for the Advancement of Science (AAAS)\n3. Ruder L, Arber S. 2019. Brainstem Circuits Controlling Action Diversification. Annual Review\nof Neuroscience 42(1):485\u2013504Publisher: Annual Reviews\n4. Bostan AC, Strick PL. 2018. The basal ganglia and the cerebellum: nodes in an integrated\nnetwork. Nature Reviews Neuroscience 19(6):338\u2013350Publisher: Springer Science and Business\n5. Lemon RN. 2008. Descending Pathways in Motor Control. Annual Review of Neuroscience\n6. Battaglia-Mayer A, Caminiti R. 2019. Corticocortical Systems Underlying High-Order Motor\nControl. The Journal of Neuroscience 39(23):4404\u20134421Publisher: Society for Neuroscience\n7. 2018. Neural Basis of Touch and Proprioception in Primate Cortex. In Comprehensive Phys-\niology. Wiley, 1st ed.\n8. Versteeg C, Chowdhury RH, Miller LE. 2021. Cuneate nucleus: the somatosensory gateway\nto the brain. Current Opinion in Physiology 20:206\u2013215Publisher: Elsevier BV\n9. Chowdhury RH, Glaser JI, Miller LE. 2020. Area 2 of primary somatosensory cortex encodes\nkinematics of the whole arm. eLife 9Publisher: eLife Sciences Publications, Ltd\n10. Goodman JM, Tabot GA, Lee AS, Suresh AK, Rajan AT, et al. 2019. Postural Representa-\nwww.annualreviews.org \u2022 Embodied sensorimotor control\ntions of the Hand in the Primate Sensorimotor Cortex. Neuron 104(5):1000\u20131009.e7Publisher:\n11. G\u00b4omez LJ, Dooley JC, Sokoloff G, Blumberg MS. 2021. Parallel and Serial Sensory Process-\ning in Developing Primary Somatosensory and Motor Cortex. The Journal of Neuroscience\n41(15):3418\u20133431Publisher: Society for Neuroscience\n12. Bufacchi R, Battaglia-Mayer A, Iannetti G, Caminiti R. 2023. Cortico-spinal modularity in\nthe parieto-frontal system: A new perspective on action control. Progress in Neurobiology\n231:102537Publisher: Elsevier BV\n13. Moreno-L\u00b4opez Y, Olivares-Moreno R, Cordero-Erausquin M, Rojas-Piloni G. 2016. Sensori-\nmotor Integration by Corticospinal System. Frontiers in Neuroanatomy 10\n14. Yang W, Kanodia H, Arber S. 2023. Structural and functional map for forelimb movement\nphases between cortex and medulla. Cell 186(1):162\u2013177.e18Publisher: Elsevier BV\n15. Mink JW. 1996. The basal ganglia: focused selection and inhibition of competing motor pro-\ngrams. Progress in neurobiology 50(4):381\u2013425\n16. Taitano RI, Yakovenko S, Gritsenko V. 2024. Muscle anatomy is reflected in the spatial or-\nganization of the spinal motoneuron pools. Communications Biology 7(1)Publisher: Springer\nScience and Business Media LLC\n17. Zajac F. 1989. Muscle and tendon: properties, models, scaling and application to biomechanics\nand motor control. Critical Reviews in Biomedical Engineering 17(4):359\n18. Rohrle O, Yavuz US, Klotz T, Negro F, Heidlauf T. 2019. Multiscale modeling of the neuro-\nmuscular system: Coupling neurophysiology and skeletal muscle mechanics. WIREs Systems\nBiology and Medicine 11(6)Publisher: Wiley\n19. Vyas S, Golub MD, Sussillo D, Shenoy KV. 2020. Computation Through Neural Population\nDynamics. Annual Review of Neuroscience 43(1):249\u2013275\n20. Gallego JA, Perich MG, Miller LE, Solla SA. 2017. Neural manifolds for the control of move-\nment. Neuron 94(5):978\u2013984\n21. Elsayed GF, Lara AH, Kaufman MT, Churchland MM, Cunningham JP. 2016. Reorganization\nbetween preparatory and movement population responses in motor cortex. Nature communi-\ncations 7(1):13239\n22. Inagaki HK, Fontolan L, Romani S, Svoboda K. 2019. Discrete attractor dynamics underlies\npersistent activity in the frontal cortex. Nature 566(7743):212\u2013217\n23. Saxena S, Russo AA, Cunningham J, Churchland MM. 2022. Motor cortex activity across\nmovement speeds is predicted by network-level strategies for generating muscle activity. eLife\n24. Churchland MM, Shenoy KV. 2007. Temporal complexity and heterogeneity of single-neuron\nactivity in premotor and motor cortex. Journal of neurophysiology 97(6):4235\u20134257\n25. Scott SH. 2008. Inconvenient truths about neural processing in primary motor cortex. The\nJournal of physiology 586(5):1217\u20131224\n26. Fetz EE. 1992. recognizably coded in the activity of single neurons? Behavioral and brain\n27. Cunningham JP, Yu BM. 2014. Dimensionality reduction for large-scale neural recordings.\nNature neuroscience 17(11):1500\u20131509\n28. Santhanam G, Yu BM, Gilja V, Ryu SI, Afshar A, et al. 2009. Factor-analysis methods for\nhigher-performance neural prostheses. Journal of neurophysiology 102(2):1315\u20131330\n29. Sadtler PT, Quick KM, Golub MD, Chase SM, Ryu SI, et al. 2014. Neural constraints on\nlearning. Nature 512(7515):423\u2013426\n30. Gallego JA, Perich MG, Naufel SN, Ethier C, Solla SA, Miller LE. 2018. Cortical popula-\ntion activity within a preserved neural manifold underlies multiple motor behaviors. Nature\ncommunications 9(1):4233\n31. Mante V, Sussillo D, Shenoy KV, Newsome WT. 2013. Context-dependent computation by\nrecurrent dynamics in prefrontal cortex. nature 503(7474):78\u201384\n24 Almani et al.\n32. Kobak D, Brendel W, Constantinidis C, Feierstein CE, Kepecs A, et al. 2016. Demixed prin-\ncipal component analysis of neural population data. elife 5:e10989\n33. Churchland MM, Yu BM, Cunningham JP, Sugrue LP, Cohen MR, et al. 2010. Stimulus\nonset quenches neural variability: a widespread cortical phenomenon. Nature neuroscience\n34. Raposo D, Kaufman MT, Churchland AK. 2014. A category-free neural population supports\nevolving demands during decision-making. Nature neuroscience 17(12):1784\u20131792\n35. Tanji J, Evarts EV. 1976. Anticipatory activity of motor cortex neurons in relation to direction\nof an intended movement. Journal of neurophysiology 39(5):1062\u20131068\n36. Wise SP. 1985. The primate premotor cortex: past, present, and preparatory. Annual review\nof neuroscience 8:1\u201319\n37. Churchland MM, Santhanam G, Shenoy KV. 2006. Preparatory activity in premotor and motor\ncortex reflects the speed of the upcoming reach. Journal of neurophysiology 96(6):3130\u20133146\n38. Kaufman MT, Churchland MM, Ryu SI, Shenoy KV. 2014. Cortical activity in the null space:\npermitting preparation without movement. Nature neuroscience 17(3):440\u2013448\n39. Churchland MM, Shenoy KV. 2024. Preparatory activity and the expansive null-space. Nature\nReviews Neuroscience 25(4):213\u2013236\n40. Oby ER, Degenhart AD, Grigsby EM, Motiwala A, McClain NT, et al. 2025. Dynamical\nconstraints on neural population activity. Nature Neuroscience 28(2):383\u2013393\n41. Inagaki HK, Inagaki M, Romani S, Svoboda K. 2018. Low-dimensional and monotonic prepara-\ntory activity in mouse anterior lateral motor cortex. Journal of Neuroscience 38(17):4163\u20134185\n42. Paton JJ, Buonomano DV. 2018. The neural basis of timing: distributed mechanisms for\ndiverse functions. Neuron 98(4):687\u2013705\n43. Zhou S, Masmanidis SC, Buonomano DV. 2020. Neural sequences as an optimal dynamical\nregime for the readout of time. Neuron 108(4):651\u2013658\n44. Churchland MM, Cunningham JP, Kaufman MT, Foster JD, Nuyujukian P, et al. 2012. Neural\npopulation dynamics during reaching. Nature 487(7405):51\u201356\n45. Kaufman MT, Seely JS, Sussillo D, Ryu SI, Shenoy KV, Churchland MM. 2016. The largest\nresponse component in the motor cortex reflects movement timing but not movement type.\n46. Inagaki HK, Chen S, Ridder MC, Sah P, Li N, et al. 2022. A midbrain-thalamus-cortex circuit\nreorganizes cortical dynamics to initiate movement. Cell 185(6):1065\u20131081\n47. Russo AA, Bittner SR, Perkins SM, Seely JS, London BM, et al. 2018. Motor cortex embeds\nmuscle-like commands in an untangled population response. Neuron 97(4):953\u2013966\n48. Guo ZV, Inagaki HK, Daie K, Druckmann S, Gerfen CR, Svoboda K. 2017. Maintenance of\npersistent activity in a frontal thalamocortical loop. Nature 545(7653):181\u2013186\n49. Kao TC, Sadabadi MS, Hennequin G. 2021. Optimal anticipatory control as a theory of motor\npreparation: A thalamo-cortical circuit model. Neuron 109(9):1567\u20131581\n50. Logiaco L, Abbott L, Escola S. 2021. Thalamic control of cortical dynamics in a model of\nflexible motor sequencing. Cell reports 35(9)\n51. Ding JB, Guzman JN, Peterson JD, Goldberg JA, Surmeier DJ. 2010. Thalamic gating of\ncorticostriatal signaling by cholinergic interneurons. Neuron 67(2):294\u2013307\n52. Koketsu D, Chiken S, Hisatsune T, Miyachi S, Nambu A. 2021. Elimination of the cortico-\nsubthalamic hyperdirect pathway induces motor hyperactivity in mice. Journal of Neuro-\nscience 41(25):5502\u20135510\n53. Li N, Mrsic-Flogel TD. 2020. Cortico-cerebellar interactions during goal-directed behavior.\nCurrent opinion in neurobiology 65:27\u201337\n54. Fakharian MA, Shoup AM, Hage P, Elseweifi HY, Shadmehr R. 2025. A vector calculus for\nneural computation in the cerebellum. Science 388(6749):869\u2013875\n55. Yamins DL, DiCarlo JJ. 2016. Using goal-driven deep learning models to understand sensory\ncortex. Nature neuroscience 19(3):356\u2013365\nwww.annualreviews.org \u2022 Embodied sensorimotor control\n56. Durstewitz D, Koppe G, Thurm MI. 2023. Reconstructing computational system dynamics\nfrom neural data with recurrent neural networks. Nature Reviews Neuroscience 24(11):693\u2013\n57. Sussillo D, Barak O. 2013. Opening the black box:\nlow-dimensional dynamics in high-\ndimensional recurrent neural networks. Neural computation 25(3):626\u2013649\n58. Stroud JP, Watanabe K, Suzuki T, Stokes MG, Lengyel M. 2023. Optimal information loading\ninto working memory explains dynamic coding in the prefrontal cortex. Proceedings of the\nNational Academy of Sciences 120(48):e2307991120\n59. Yang GR, Joglekar MR, Song HF, Newsome WT, Wang XJ. 2019. Task representations in\nneural networks trained to perform many cognitive tasks. Nature neuroscience 22(2):297\u2013306\n60. Driscoll LN, Shenoy K, Sussillo D. 2024. Flexible multitask computation in recurrent networks\nutilizes shared dynamical motifs. Nature Neuroscience 27(7):1349\u20131363\n61. Maheswaranathan N, Williams A, Golub M, Ganguli S, Sussillo D. 2019. Universality and\nindividuality in neural dynamics across large populations of recurrent networks. Advances in\nneural information processing systems 32\n62. Sussillo D, Churchland MM, Kaufman MT, Shenoy KV. 2015. A neural network that finds a\nnaturalistic solution for the production of muscle activity. Nature neuroscience 18(7):1025\u2013\n63. Feulner B, Perich MG, Miller LE, Clopath C, Gallego JA. 2025. A neural implementation\nmodel of feedback-based motor learning. Nature communications 16(1):1805\n64. O\u2019Shea DJ, Duncker L, Goo W, Sun X, Vyas S, et al. 2022. Direct neural perturbations reveal\na dynamical mechanism for robust computation. bioRxiv :2022\u201312\n65. Todorov E, Jordan MI. 2002. Optimal feedback control as a theory of motor coordination.\nNature neuroscience 5(11):1226\u20131235\n66. Scott SH. 2004. Optimal feedback control and the neural basis of volitional motor control.\nNature Reviews Neuroscience 5(7):532\u2013545\n67. Liu D, Todorov E. 2007. Evidence for the flexible sensorimotor strategies predicted by optimal\nfeedback control. Journal of Neuroscience 27(35):9354\u20139368\n68. Franklin DW, Wolpert DM. 2011. Computational mechanisms of sensorimotor control. Neuron\n69. Saxena S, Sarma SV, Dahleh M. 2020. Performance limitations in sensorimotor control: Trade-\noffs between neural computation and accuracy in tracking fast movements. Neural computation\n70. Takei T, Lomber SG, Cook DJ, Scott SH. 2021. Transient deactivation of dorsal premotor\ncortex or parietal area 5 impairs feedback control of the limb in macaques. Current Biology\n31(7):1476\u20131487.e5\n71. Scott SH. 2012. The computational and neural basis of voluntary motor control and planning.\nTrends in Cognitive Sciences 16(11):541\u2013549Publisher: Elsevier BV\n72. Jordan M, Heuer H, Keele S. 1996. Handbook of perception and action: motor skills\n73. Kawato M, Furukawa K, Suzuki R. 1987. A hierarchical neural-network model for control and\nlearning of voluntary movement. Biological cybernetics 57(3):169\u2013185\n74. Wolpert DM, Miall RC, Kawato M. 1998. Internal models in the cerebellum. Trends in cogni-\ntive sciences 2(9):338\u2013347\n75. Miall RC, Wolpert DM. 1996. Forward models for physiological motor control. Neural networks\n76. Therrien AS, Bastian AJ. 2015. Cerebellar damage impairs internal predictions for sensory\nand motor function. Current opinion in neurobiology 33:127\u2013133\n77. Kawato M, Kuroda T, Imamizu H, Nakano E, Miyauchi S, Yoshioka T. 2003. Internal forward\nmodels in the cerebellum: fmri study on grip force and load force coupling. Progress in brain\nresearch 142:171\u2013188\n78. Diedrichsen J, Bastian A. 2014. 38 cerebellar function. The cognitive neurosciences :451\n26 Almani et al.\n79. M\u00a8uller F, Dichgans J. 1994. Dyscoordination of pinch and lift forces during grasp in patients\nwith cerebellar lesions. Experimental brain research 101(3):485\u2013492\n80. Miall RC, Christensen LOD, Cain O, Stanley J. 2007. Disruption of state estimation in the\nhuman lateral cerebellum. PLoS biology 5(11):e316\n81. Fiser J, Berkes P, Orb\u00b4an G, Lengyel M. 2010. Statistically optimal perception and learning:\nfrom behavior to neural representations. Trends in cognitive sciences 14(3):119\u2013130\n82. Arce F, Novick I, Mandelblat-Cerf Y, Israel Z, Ghez C, Vaadia E. 2010. Combined adaptiveness\nof specific motor cortical ensembles underlies learning. Journal of Neuroscience 30(15):5415\u2013\n83. Bell CC, Han VZ, Sugawara Y, Grant K. 1997. Synaptic plasticity in a cerebellum-like structure\ndepends on temporal order. Nature 387(6630):278\u2013281\n84. Mosier KM, Scheidt RA, Acosta S, Mussa-Ivaldi FA. 2005. Remapping hand movements in a\nnovel geometrical environment. Journal of neurophysiology 94(6):4362\u20134372\n85. Johansson RS, Theorin A, Westling G, Andersson M, Ohki Y, Nyberg L. 2006. How a later-\nalized brain supports symmetrical bimanual tasks. PLoS biology 4(6):e158\n86. Liu X, Mosier KM, Mussa-Ivaldi FA, Casadio M, Scheidt RA. 2011. Reorganization of finger\ncoordination patterns during adaptation to rotation and scaling of a newly learned sensorimo-\ntor transformation. Journal of neurophysiology 105(1):454\u2013473\n87. Tseng Yw, Diedrichsen J, Krakauer JW, Shadmehr R, Bastian AJ. 2007. Sensory predic-\ntion errors drive cerebellum-dependent adaptation of reaching. Journal of neurophysiology\n88. Golla H, Tziridis K, Haarmeier T, Catz N, Barash S, Thier P. 2008. Reduced saccadic resilience\nand impaired saccadic adaptation due to cerebellar disease. European Journal of Neuroscience\n89. Izawa J, Shadmehr R. 2011. Learning from sensory and reward prediction errors during motor\nadaptation. PLoS computational biology 7(3):e1002012\n90. Abe M, Schambra H, Wassermann EM, Luckenbaugh D, Schweighofer N, Cohen LG. 2011.\nReward improves long-term retention of a motor memory through induction of offline memory\ngains. Current Biology 21(7):557\u2013562\n91. Stengel RF. 1994. Optimal control and estimation. Courier Corporation\n92. Todorov E, et al. 2006. Optimal control theory. Bayesian brain: probabilistic approaches to\nneural coding :268\u2013298\n93. Van Beers RJ, Sittig AC, Gon JJDvd. 1999. Integration of proprioceptive and visual position-\ninformation: An experimentally supported model. Journal of neurophysiology 81(3):1355\u20131364\n94. Merfeld DM, Zupan L, Peterka RJ. 1999. Humans use internal models to estimate gravity and\nlinear acceleration. Nature 398(6728):615\u2013618\n95. Mulliken GH, Musallam S, Andersen RA. 2008. Forward estimation of movement state in\nposterior parietal cortex. Proceedings of the National Academy of Sciences 105(24):8170\u20138177\n96. Uno Y, Kawato M, Suzuki R. 1989. Formation and control of optimal trajectory in human\nmultijoint arm movement. Biological cybernetics 61(2):89\u2013101\n97. Flash T, Hogan N. 1985. The coordination of arm movements: an experimentally confirmed\nmathematical model. Journal of neuroscience 5(7):1688\u20131703\n98. Harris CM, Wolpert DM. 1998. Signal-dependent noise determines motor planning. Nature\n394(6695):780\u2013784\n99. Todorov E, Li W. 2005. A generalized iterative LQG method for locally-optimal feedback control\nof constrained nonlinear stochastic systems. In Proceedings of the 2005, American Control\nConference, 2005., pp. 300\u2013306. IEEE\n100. Ueyama Y. 2017. Optimal feedback control to describe multiple representations of primary\nmotor cortex neurons. Journal of Computational Neuroscience 43(1):93\u2013106\n101. Sokoloff G, Hickerson MM, Wen RY, Tobias ME, McMurray B, Blumberg MS. 2020. Spa-\ntiotemporal organization of myoclonic twitching in sleeping human infants. Developmental\nwww.annualreviews.org \u2022 Embodied sensorimotor control\npsychobiology 62(6):697\u2013710\n102. Seth A, Sherman M, Reinbolt JA, Delp SL. 2011. OpenSim: a musculoskeletal modeling\nand simulation framework for in silico investigations and exchange. Procedia IUTAM 2:212\u2013\n232Publisher: Elsevier BV\n103. Ikkala A, H\u00a8am\u00a8al\u00a8ainen P. 2022. Converting biomechanical models from opensim to Mujoco. In\nConverging Clinical and Engineering Research on Neurorehabilitation IV: Proceedings of the\n5th International Conference on Neurorehabilitation (ICNR2020), October 13\u201316, 2020, pp.\n277\u2013281. Springer\n104. Todorov E, Erez T, Tassa Y. 2012. MuJoCo: A physics engine for model-based control. In\n2012 IEEE\/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033.\nVilamoura-Algarve, Portugal: IEEE\n105. Caggiano V, Wang H, Durandau G, Sartori M, Kumar V. 2022. MyoSuite \u2013 A contact-rich\nsimulation suite for musculoskeletal motor control. ArXiv:2205.13600 [cs]\n106. Wang H, Caggiano V, Durandau G, Sartori M, Kumar V. 2022. MyoSim: Fast and physiolog-\nically realistic MuJoCo models for musculoskeletal and exoskeletal studies. In 2022 Interna-\ntional Conference on Robotics and Automation (ICRA), pp. 8104\u20138111. IEEE\n107. Dembia CL, Bianco NA, Falisse A, Hicks JL, Delp SL. 2020. OpenSim Moco: Musculoskeletal\noptimal control. PLOS Computational Biology 16(12):e1008493Publisher: Public Library of\n108. Freeman CD, Frey E, Raichuk A, Girgin S, Mordatch I, Bachem O. 2021. Brax \u2013 A Differen-\ntiable Physics Engine for Large Scale Rigid Body Simulation. ArXiv:2106.13281 [cs]\n109. Schiaffino S, Reggiani C. 2011. Fiber Types in Mammalian Skeletal Muscles. Physiological\nReviews 91(4):1447\u20131531Publisher: American Physiological Society\n110. Almani MN, Lazzari J, Chacon A, Saxena S. 2024. \u00b5Sim: A goal-driven framework for\nelucidating the neural control of movement through musculoskeletal modeling. bioRxiv\n:2024.02.02.578628\n111. Aldarondo D, Merel J, Marshall JD, Hasenclever L, Klibaite U, et al. 2024. A virtual rodent\npredicts the structure of neural activity across behaviours. Nature 632(8025):594\u2013602\n112. Merel J, Aldarondo D, Marshall J, Tassa Y, Wayne G, \u00a8Olveczky B. 2019. Deep neuroethology\nof a virtual rodent. arXiv\n113. DeWolf T, Schneider S, Soubiran P, Roggenbach A, Muratore P, Mathis M. 2024. Neuro-\nmusculoskeletal modeling reveals muscle-level neural dynamics of adaptive learning in senso-\n114. Chan SS, Moran DW. 2006. Computational model of a primate arm: from hand position to\njoint angles, joint torques and muscle forces. Journal of neural engineering 3(4):327\n115. Walker J, Hatsopoulos NG. 2023. Building a whole-body marmoset model for deep reinforce-\nment learning driven musculoskeletal simulation to understand sensorimotor control. Janelia\nResearch Campus, Ashburn, Virginia\n116. Vaxenburg R, Siwanowicz I, Merel J, Robie AA, Morrow C, et al. 2025. Whole-body physics\nsimulation of fruit fly locomotion. Nature\n117. Song S, Kidzi\u00b4nski (cid:32)L, Peng XB, Ong C, Hicks J, et al. 2021. Deep reinforcement learning for\nmodeling human locomotion control in neuromechanical simulation. Journal of NeuroEngi-\nneering and Rehabilitation 18(1):126\n118. Nakada M, Zhou T, Chen H, Weiss T, Terzopoulos D. 2018. Deep learning of biomimetic\nsensorimotor control for biomechanical human animation. ACM Transactions on Graphics\n119. La Barbera V, Pardo F, Tassa Y, Daley M, Richards C, et al. 2022. OstrichRL: A Muscu-\nloskeletal Ostrich Simulation to Study Bio-mechanical Locomotion\n120. Merel J, Botvinick M, Wayne G. 2019. Hierarchical motor control in mammals and machines.\nNature Communications 10(1):5489\n121. Luft AR, Schwarz S. 2009. Dopaminergic signals in primary motor cortex. International Jour-\n28 Almani et al.\nnal of Developmental Neuroscience 27(5):415\u2013421\n122. Silver D, Lever G, Heess N, Degris T, Wierstra D, Riedmiller M. 2014. Deterministic policy\ngradient algorithms. In International conference on machine learning, pp. 387\u2013395. Pmlr\n123. Merel J, Tunyasuvunakool S, Ahuja A, Tassa Y, Hasenclever L, et al. 2020. Catch & Carry:\nReusable Neural Controllers for Vision-Guided Whole-Body Tasks. ArXiv:1911.06636 [cs]\n124. Heess N, Wayne G, Tassa Y, Lillicrap T, Riedmiller M, Silver D. 2016. Learning and Transfer\nof Modulated Locomotor Controllers. ArXiv:1610.05182 [cs]\n125. Merel J, Tassa Y, TB D, Srinivasan S, Lemmon J, et al. 2017. Learning human behaviors from\nmotion capture by adversarial imitation. ArXiv:1707.02201 [cs]\n126. Peng XB, Abbeel P, Levine S, van de Panne M. 2018. DeepMimic: example-guided deep rein-\nforcement learning of physics-based character skills. ACM Transactions on Graphics 37(4):1\u2013\n127. Bohez S, Tunyasuvunakool S, Brakel P, Sadeghi F, Hasenclever L, et al. 2022. Imitate and\nRepurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors.\nArXiv:2203.17138 [cs]\n128. Nath T, Mathis A, Chen AC, Patel A, Bethge M, Mathis MW. 2019. Using DeepLabCut for\n3D markerless pose estimation across species and behaviors. Nature Protocols 14(7):2152\u20132176\n129. Pereira TD, Tabris N, Matsliah A, Turner DM, Li J, et al. 2022. SLEAP: A deep learning\nsystem for multi-animal pose tracking. Nature Methods 19(4):486\u2013495\n130. Lobato-Rios V, Ramalingasetty ST, \u00a8Ozdil PG, Arreguit J, Ijspeert AJ, Ramdya P. 2022.\nNeuroMechFly, a neuromechanical model of adult Drosophila melanogaster. Nature Methods\n19(5):620\u2013627Publisher: Springer Science and Business Media LLC\n131. Wang-Chen S, Stimpfling VA, Lam TKC, \u00a8Ozdil PG, Genoud L, et al. 2024. NeuroMechFly v2:\nsimulating embodied sensorimotor control in adult Drosophila. Nature Methods 21(12):2353\u2013\n2362Publisher: Springer Science and Business Media LLC\n132. Karashchuk L, Li JS, Chou GM, Walling-Bell S, Brunton SL, et al. 2025. Sensorimotor delays\nconstrain robust locomotion in a 3D kinematic model of fly walking. eLife 13Publisher: eLife\nSciences Publications, Ltd\n133. Chiappa AS, Tano P, Patel N, Ingster A, Pouget A, Mathis A. 2024. Acquiring musculoskeletal\nskills with curriculum-based reinforcement learning. Neuron 112(23):3969\u20133983.e5Publisher:\n134. Simos M, Chiappa AS, Mathis A. 2025. Reinforcement learning-based motion imitation for\nphysiologically plausible musculoskeletal motor control. ArXiv:2503.14637 [cs]\n135. Tata Ramalingasetty S, Danner SM, Arreguit J, Markin SN, Rodarie D, et al. 2021. A Whole-\nBody Musculoskeletal Model of the Mouse. IEEE Access 9:163861\u2013163881Publisher: Institute\nof Electrical and Electronics Engineers (IEEE)\n136. Putrino D, Wong YT, Weiss A, Pesaran B. 2015. A training platform for many-dimensional\nprosthetic devices using a virtual reality environment. Journal of Neuroscience Methods\n244:68\u201377Publisher: Elsevier BV\n137. Almani MN, Saxena S. 2022. Recurrent Neural Networks Controlling Musculoskeletal Models\nPredict Motor Cortex Activity during Novel Limb Movements. In 2022 44th Annual Inter-\nnational Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp.\n3350\u20133356. IEEE\n138. Shaw L, Wang KH, Mitchell J. 2023. Fast prediction in marmoset reach-to-grasp movements\nfor dynamic prey. Current Biology 33(12):2557\u20132565.e4\n139. Walker JD, Pirschel F, Sundiang M, Niekrasz M, MacLean JN, Hatsopoulos NG. 2021. Chronic\nwireless neural population recordings with common marmosets. Cell Reports 36(2):109379\n140. Lappalainen JK, Tschopp FD, Prakhya S, McGill M, Nern A, et al. 2024. Connectome-\nconstrained networks predict neural activity across\n634(8036):1132\u20131140Publisher: Springer Science and Business Media LLC\n141. Dorkenwald S, McKellar CE, Macrina T, Kemnitz N, Lee K, et al. 2022. FlyWire: online\nwww.annualreviews.org \u2022 Embodied sensorimotor control\ncommunity for whole-brain connectomics. Nature Methods 19(1):119\u2013128Publisher: Springer\nScience and Business Media LLC\n142. Michaels JA, Schaffelhofer S, Agudelo-Toro A, Scherberger H. 2020. A goal-driven modular\nneural network predicts parietofrontal neural dynamics during grasping. Proceedings of the\nNational Academy of Sciences 117(50):32124\u201332135\n143. Kleinman M, Chandrasekaran C, Kao J. 2021. A mechanistic multi-area recurrent network\nmodel of decision-making. Advances in neural information processing systems 34:23152\u201323165\n144. Jiao Y, Ling F, Heydari S, Heess N, Merel J, Kanso E. 2024. Deep Dive into Model-\nfree Reinforcement Learning for Biological and Robotic Systems: Theory and Practice.\nArXiv:2405.11457 [cs]\n145. Osborne LC, Lisberger SG, Bialek W. 2005. A sensory source for motor variation. Nature\n437(7057):412\u2013416\n146. Schumacher P, H\u00a8aufle D, B\u00a8uchler D, Schmitt S, Martius G. 2023. DEP-RL: Embod-\nied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems.\nArXiv:2206.00484 [cs]\n147. Dimitriou M. 2022. Human muscle spindles are wired to function as controllable signal-\nprocessing devices. eLife 11:e78091\n148. Liu B, Hong A, Rieke F, Manookin MB. 2021. Predictive encoding of motion begins in the\nprimate retina. Nature Neuroscience 24(9):1280\u20131291Publisher: Springer Science and Business\n149. Hein AM, Altshuler DL, Cade DE, Liao JC, Martin BT, Taylor GK. 2020. An Algorithmic\nApproach to Natural Behavior. Current Biology 30(11):R663\u2013R675\n150. Fang C, Stachenfeld KL. 2024. Predictive auxiliary objectives in deep RL mimic learning in\nthe brain. ArXiv:2310.06089 [cs]\n30 Almani et al.",
    "embedding":[
      0.0149819329,
      -0.0958452448,
      0.0877460018,
      -0.0171054043,
      -0.0477691144,
      0.0339486934,
      -0.0067477929,
      -0.0297485553,
      0.0194619112,
      0.0464116558,
      0.0548709445,
      0.0098240748,
      0.0079637542,
      -0.0320015103,
      -0.0274310987,
      0.1070352718,
      0.0438952297,
      0.0570265278,
      -0.0931399688,
      0.0456902124,
      0.0786618814,
      0.0320678651,
      0.0897195935,
      -0.0282149389,
      -0.020804856,
      0.0318871923,
      -0.0094350819,
      -0.0451067984,
      -0.0492907576,
      -0.1304491758,
      0.0284325965,
      -0.0582531989,
      -0.0271633994,
      -0.0628862157,
      -0.0604805015,
      0.0464341864,
      -0.0375768468,
      -0.0280558765,
      -0.0300734825,
      0.0152026834,
      0.0043963604,
      0.0019887644,
      0.0446039513,
      -0.013308173,
      0.0325585417,
      0.0961524323,
      -0.0310228635,
      -0.0308358353,
      -0.0460700355,
      -0.0092573864,
      -0.0321903527,
      -0.0792611763,
      0.0983358175,
      -0.007136093,
      0.0399549454,
      0.0121965706,
      0.0622183494,
      0.0323907807,
      -0.041496139,
      -0.0852527544,
      0.0058795572,
      0.0019153338,
      0.0051657292,
      -0.0418372005,
      0.0305827931,
      -0.0479774773,
      -0.0383183993,
      -0.0085627446,
      -0.0151236588,
      0.0341434516,
      -0.0064619468,
      -0.0295145307,
      -0.0206718724,
      -0.0713475794,
      -0.0054527204,
      -0.0637198016,
      0.0334213339,
      0.0213060006,
      0.0147217913,
      -0.0291474592,
      0.056882415,
      -0.024318533,
      -0.0584949218,
      0.0250492282,
      -0.0211645886,
      0.0275570732,
      -0.0419164523,
      0.0503726006,
      0.0371178053,
      0.0298137423,
      -0.0354700759,
      -0.0539191253,
      -0.0370392241,
      -0.0642904192,
      0.0489588343,
      -0.0258447453,
      -0.0045846151,
      -0.0088064075,
      -0.0594462752,
      0.0067322506,
      0.0498041958,
      -0.0053555323,
      0.0086204801,
      0.0766741186,
      0.0687319264,
      -0.032517869,
      0.0903142169,
      0.0111091668,
      0.0324460045,
      -0.0069994717,
      0.0450905897,
      -0.0331955552,
      -0.0090391319,
      0.1324795634,
      -0.0122458925,
      -0.0262376629,
      0.0076140128,
      0.0663838387,
      0.0549539551,
      0.0539463237,
      0.0616128109,
      -0.0806812271,
      -0.0185795464,
      -0.0153367715,
      -0.0421159193,
      -0.0247427542,
      -0.0624073185,
      1.096271321e-32,
      -0.0608503819,
      -0.0258526579,
      0.015021489,
      -0.0393332988,
      0.028962275,
      -0.0388903841,
      -0.0477201268,
      -0.0049164239,
      -0.013253795,
      -0.0283774007,
      -0.0884959772,
      -0.0198201314,
      -0.007759993,
      0.14919357,
      -0.0070027742,
      -0.1251405478,
      -0.0103233755,
      -0.0230607037,
      0.0320612043,
      -0.0656618327,
      0.1327127069,
      0.0064929784,
      -0.0593921281,
      -0.035796281,
      -0.0885413364,
      0.0422707275,
      -0.0667040795,
      0.0458750278,
      -0.038066294,
      0.0130156307,
      -0.0489141308,
      0.0532754734,
      -0.1104317084,
      -0.0260868352,
      0.0641321987,
      0.005679348,
      0.1159503236,
      -0.0539047681,
      0.0381946862,
      -0.0110929189,
      -0.0056079668,
      -0.0245170612,
      0.0301897377,
      0.014968466,
      -0.0379822999,
      -0.0411202908,
      0.0916962922,
      0.0364321768,
      0.0004985387,
      -0.0103676459,
      -0.0275468677,
      -0.0040981043,
      0.0002431819,
      -0.0849553272,
      0.1071436182,
      -0.0747364536,
      -0.0405571125,
      0.0326569341,
      -0.0685716718,
      0.0566764362,
      -0.0365574807,
      0.0337433629,
      0.0756954402,
      -0.0277528949,
      0.0484836139,
      0.1158821285,
      -0.0764863938,
      0.0491089709,
      0.0731383562,
      0.0212261472,
      -0.0243979543,
      0.0243083891,
      0.0164542161,
      -0.0246380884,
      -0.0054688985,
      -0.0202872064,
      0.0290215816,
      -0.0532947183,
      -0.0856208727,
      -0.0494328626,
      -0.0139877098,
      0.0162521377,
      -0.0232269,
      0.0514615029,
      0.0230716337,
      -0.0210073572,
      0.0714688301,
      -0.07797952,
      -0.1388246119,
      0.0044192234,
      -0.0371857733,
      -0.0726171136,
      0.0224916022,
      0.0621024519,
      -0.0778465718,
      -1.274494191e-32,
      -0.0037040194,
      0.065241307,
      0.0568794347,
      0.0372262858,
      -0.0543299615,
      -0.0117383786,
      0.0486785024,
      -0.0043252548,
      -0.0152453212,
      -0.0450266562,
      -0.0427666642,
      0.0071232878,
      -0.0164488889,
      0.0494162217,
      0.1085868925,
      -0.0147377839,
      -0.0079352148,
      -0.0652396902,
      0.0456903391,
      -0.0279316287,
      0.0393823422,
      0.0292449333,
      -0.0560210384,
      0.0381845571,
      0.0040393472,
      0.0587606989,
      -0.0282425936,
      0.0589671619,
      -0.0211134814,
      -0.0025602179,
      -0.0962458849,
      0.0243557878,
      -0.042501159,
      0.0844818279,
      -0.0350302868,
      0.0467070639,
      0.0105914697,
      0.0612037815,
      -0.0325145535,
      -0.0167338848,
      0.0426355563,
      -0.0010356562,
      0.0048874342,
      0.0510907508,
      0.0978238285,
      -0.0331559964,
      -0.0789614543,
      0.0156008275,
      -0.0911866724,
      0.0097565232,
      -0.0506790914,
      0.0151369758,
      -0.0627521798,
      -0.1106147915,
      -0.0039570048,
      0.002390377,
      0.0951480344,
      -0.0178815052,
      0.0789756179,
      -0.0296189338,
      -0.0605206788,
      -0.0420843437,
      -0.036763411,
      0.0220742486,
      0.0436473079,
      0.0905502886,
      -0.0151193412,
      0.0207325928,
      0.0913623571,
      0.0246502236,
      0.0312541425,
      0.0328903869,
      0.0682806075,
      -0.0518776625,
      0.042321384,
      -0.0026254563,
      -0.0182333775,
      -0.0182365477,
      -0.0481365025,
      -0.0137015013,
      -0.0383702442,
      -0.0004427647,
      0.0247091632,
      -0.0287045073,
      0.0017694021,
      0.0482192151,
      -0.157543838,
      0.0457794145,
      -0.0182345957,
      0.0373059697,
      0.0491122268,
      0.0816774443,
      0.0797426179,
      -0.0645978004,
      -0.0375344381,
      -0.0000000714,
      -0.0490480438,
      0.0283616316,
      0.0570543297,
      0.0336726345,
      0.0499435104,
      0.0731236488,
      0.0321820118,
      -0.1344305575,
      -0.0480796881,
      -0.0347452685,
      0.0601234101,
      -0.0143842073,
      0.0513958968,
      0.0146421902,
      0.0011405968,
      0.0667389408,
      -0.0229458511,
      -0.0402255692,
      -0.0122637767,
      -0.0120099504,
      0.0664058775,
      -0.0082676271,
      -0.010792708,
      0.0242597852,
      0.0241888724,
      -0.0430816226,
      -0.1191614568,
      0.0466441251,
      -0.0533263311,
      0.000473033,
      -0.0032342351,
      -0.0097029516,
      0.0485066399,
      0.0670137256,
      -0.0083240932,
      0.0162159428,
      -0.0092339385,
      -0.0476059429,
      -0.0580999181,
      0.0420700498,
      -0.0187098496,
      0.0448463373,
      -0.0479803011,
      -0.0127636092,
      0.0308043733,
      0.004668538,
      0.0270722471,
      -0.130828321,
      0.076991573,
      -0.0009962844,
      0.0102621261,
      0.0245839916,
      -0.0065259496,
      0.0802244917,
      0.0564441793,
      0.0329126008,
      -0.059505675,
      -0.0214336794,
      -0.0524372198,
      0.0514498986,
      0.0016702297,
      0.0281051565,
      -0.1303953528,
      -0.0530643053
    ],
    "cluster":1,
    "time":"00:01:40"
  }
]